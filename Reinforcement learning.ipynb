{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "o-421McOW2Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Refrence- Lab files (Week 7,8,9) provided on Blackboard by Patrick Mannion in the module CT5134 Agents, Multi-Agent Systems and Reinforcement Learning\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# global variables\n",
        "BOARD_ROWS = 5\n",
        "BOARD_COLS = 5\n",
        "WIN_STATE = (4, 4)  # fixed the position of winning state\n",
        "LOSE_STATE = [(1, 0), (1, 3), (3, 1), (4, 2)]  # fixed the name of the losing states variable\n",
        "START = (0, 0)\n",
        "DEBUG = True  # set to true to enable verbose output\n",
        "\n",
        "\n",
        "\n",
        "class State:\n",
        "    def __init__(self, state=START):        \n",
        "        self.state = state\n",
        "        self.isEnd = False        \n",
        "\n",
        "    def get_reward(self):\n",
        "        if self.state == WIN_STATE:\n",
        "            return 10       \n",
        "        elif self.state in LOSE_STATE:\n",
        "            return -5\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    def is_end_func(self):\n",
        "        if (self.state == WIN_STATE) or (self.state in LOSE_STATE):\n",
        "            self.isEnd = True\n",
        "\n",
        "    def nxt_position(self, action):\n",
        "        if action == 0:                \n",
        "            nxt_state = (self.state[0] - 1, self.state[1])\n",
        "        elif action == 1:\n",
        "            nxt_state = (self.state[0] + 1, self.state[1])\n",
        "        elif action == 2:\n",
        "            nxt_state = (self.state[0], self.state[1] - 1)\n",
        "        else:\n",
        "            nxt_state = (self.state[0], self.state[1] + 1)\n",
        "            \n",
        "        if (nxt_state[0] >= 0) and (nxt_state[0] <= 4):\n",
        "            if (nxt_state[1] >= 0) and (nxt_state[1] <= 4):\n",
        "                return nxt_state  # if next state legal\n",
        "        return self.state  # Any move off the grid leaves state unchanged"
      ],
      "metadata": {
        "id": "61nkpPoEWzhv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2a"
      ],
      "metadata": {
        "id": "R_gf9v2qXDAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Refrence- Lab files (Week 7,8,9) provided on Blackboard by Patrick Mannion in the module CT5134 Agents, Multi-Agent Systems and Reinforcement Learning\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self,alpha,gamma,epsilon):\n",
        "        self.states = []\n",
        "        self.actions = [0, 1, 2, 3]  # up, down, left, right\n",
        "        self.State = State()\n",
        "        self.discount = gamma\n",
        "        self.lr = alpha\n",
        "        self.eps = epsilon\n",
        "        self.reward_list=[]\n",
        "        self.min_epsilon=0.01\n",
        "        self.decay_rate=0.001\n",
        "        self.eps_list=[]\n",
        "        \n",
        "        # initialise state values\n",
        "        self.action_values = {}        \n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                for k in range(len(self.actions)):\n",
        "                    self.action_values[(i, j, k)] = 0.0  # set initial value to 0, for Q(s,a)\n",
        "        \n",
        "        self.new_action_values = []\n",
        "\n",
        "    def choose_action(self, current_state):\n",
        "        # choose action according to policy eps-greedy\n",
        "        if np.random.uniform(0, 1) <= self.eps:\n",
        "            action = np.random.choice(self.actions)\n",
        "            if DEBUG:\n",
        "                print(\"selecting random action\")\n",
        "        else:\n",
        "            action = self.best_action(current_state)\n",
        "        return action\n",
        "\n",
        "    def take_action(self, action):\n",
        "        position = self.State.nxt_position(action)\n",
        "        self.State.state = position\n",
        "\n",
        "    def best_action(self, state):\n",
        "        best = -1\n",
        "        max_val = -100000000\n",
        "        for a in self.actions:\n",
        "            q_val = self.action_values[state[0], state[1], a]\n",
        "            if q_val >= max_val:\n",
        "                max_val = q_val\n",
        "                best = a\n",
        "        return best\n",
        "\n",
        "    def q_max(self, state):\n",
        "        best = self.best_action(state)\n",
        "        return self.action_values[state[0], state[1], best]\n",
        "\n",
        "    def q_learning(self, episodes,decay):\n",
        "        # Q-learning implementation\n",
        "        x = 0  # episode counter\n",
        "        while x < episodes:\n",
        "            rewards=0\n",
        "            # Init S\n",
        "            self.State.isEnd = False\n",
        "            self.State.state = START  # Re init S Start state\n",
        "            step = 0\n",
        "\n",
        "            if DEBUG:\n",
        "                print(\"**** Beginning episode\", x, \"****\")\n",
        "                self.show_values()\n",
        "\n",
        "            while True:  # repeat for each step of the episode (until S is terminal)\n",
        "\n",
        "                # Store current state for Q update\n",
        "                current_state = (self.State.state[0], self.State.state[1])\n",
        "\n",
        "                # Choose action A from S using policy derived from Q (e-greedy)\n",
        "                action = self.choose_action(current_state)\n",
        "\n",
        "                # Take action A observe R and next State S'\n",
        "                self.take_action(action)\n",
        "                reward = self.State.get_reward()\n",
        "                rewards+=reward\n",
        "                self.State.is_end_func()\n",
        "                next_state = self.State.state[0], self.State.state[1]\n",
        "\n",
        "                # Update state action values\n",
        "                old_q = self.action_values[current_state[0], current_state[1], action]\n",
        "                max_q = self.q_max(next_state)\n",
        "                new_q = old_q + self.lr * (reward + self.discount * max_q - old_q)\n",
        "                self.action_values[current_state[0], current_state[1], action] = new_q\n",
        "\n",
        "                step += 1\n",
        "                if DEBUG:\n",
        "                    print(\"step\", step, \"state\", current_state, \"action\", action, \"reward\", reward,\n",
        "                          \"next_state\", next_state, \"old_q\", old_q, \"max_q\", max_q, \"new_q\", new_q)\n",
        "\n",
        "                # Check if s is terminal\n",
        "                if self.State.isEnd:\n",
        "                    break\n",
        "\n",
        "                # S <- S' automatically when I took the action                    \n",
        "\n",
        "            x += 1\n",
        "            self.reward_list.append(rewards)\n",
        "\n",
        "            #Refrence for Eps decay- https://towardsdatascience.com/q-learning-algorithm-from-explanation-to-implementation-cdbeda2ea187\n",
        "            if decay==True:\n",
        "              self.eps = max(self.min_epsilon, np.exp(-self.decay_rate*x))\n",
        "\n",
        "            self.eps_list.append(self.eps)\n",
        "\n",
        "        return self.reward_list, self.eps_list\n",
        "\n",
        "    def show_values(self):\n",
        "        for i in range(0, BOARD_ROWS):\n",
        "            print('---------------------------------------------')\n",
        "            out = '| '\n",
        "            for j in range(0, BOARD_COLS):\n",
        "                mx_nxt_value = -1000\n",
        "                for a in self.actions:\n",
        "                    nxt_value = self.action_values[(i, j, a)]\n",
        "                    if nxt_value >= mx_nxt_value:\n",
        "                        mx_nxt_value = nxt_value                \n",
        "                out += str(round(mx_nxt_value, 3)).ljust(6) + ' | '\n",
        "            print(out)\n",
        "\n",
        "        print('---------------------------------------------')\n",
        "\n",
        "    def show_values1(self, f):\n",
        "        for i in range(0, BOARD_ROWS):\n",
        "            print('---------------------------------------------')\n",
        "            f.write('---------------------------------------------\\n')\n",
        "            out = '| '\n",
        "            for j in range(0, BOARD_COLS):\n",
        "                mx_nxt_value = -1000\n",
        "                for a in self.actions:\n",
        "                    nxt_value = self.action_values[(i, j, a)]\n",
        "                    if nxt_value >= mx_nxt_value:\n",
        "                        mx_nxt_value = nxt_value                \n",
        "                out += str(round(mx_nxt_value, 3)).ljust(6) + ' | '\n",
        "            print(out)\n",
        "            f.write(out + '\\n') # modified to include a newline character at the end\n",
        "        print('---------------------------------------------')\n",
        "        f.write('---------------------------------------------\\n')\n"
      ],
      "metadata": {
        "id": "-qq9Nkcpu2DF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Refrence- https://www.geeksforgeeks.org/python-pandas-series-rolling/\n",
        "import pandas as pd\n",
        "def plot(plot1, plot2):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "    axs[0].plot(pd.Series.rolling(pd.DataFrame(plot1), window=100).mean())\n",
        "    axs[0].set_xlabel('Episode')\n",
        "    axs[0].set_ylabel('Total Reward')\n",
        "    axs[0].set_title('Q-Learning Learning Curve')\n",
        "\n",
        "    axs[1].plot(plot2)\n",
        "    axs[1].set_xlabel('Episode')\n",
        "    axs[1].set_ylabel('Epsilon')\n",
        "    axs[1].set_title('Epsilon Decay Graph')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6Ar0O5mhnMtX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2b - i"
      ],
      "metadata": {
        "id": "8WLJ72UjkipJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Refrence- Lab files (Week 7,8,9) provided on Blackboard by Patrick Mannion in the module CT5134 Agents, Multi-Agent Systems and Reinforcement Learning\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    f1 = open(\"2b_i.txt\", \"a\")\n",
        "    ag = Agent(alpha=0.5,gamma=0.9,epsilon=0.10)\n",
        "    reward_list1,eps_list1=ag.q_learning(10000,decay=False)\n",
        "    ag.show_values1(f1)\n",
        "    f1.close()"
      ],
      "metadata": {
        "id": "k2aWe9zVvC4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851fb563-9ed1-4b6c-f2e3-faf0bc294365"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 0 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9769 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9770 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9771 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9772 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 1 reward -5 next_state (1, 0) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9773 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9774 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9775 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9776 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 1 reward -1 next_state (1, 2) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (1, 2) action 2 reward -1 next_state (1, 1) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (1, 1) action 1 reward -1 next_state (2, 1) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (2, 1) action 3 reward -1 next_state (2, 2) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (2, 2) action 3 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9777 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9778 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9779 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 0 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9780 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 7 state (0, 4) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 8 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 9 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 10 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 11 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 12 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 13 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9781 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9782 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9783 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 3 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9784 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9785 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9786 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9787 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 2 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9788 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 1 reward -5 next_state (1, 0) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9789 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9790 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 0 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9791 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 0 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9792 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9793 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9794 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9795 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 0 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 9 state (3, 4) action 0 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 10 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 11 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9796 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 0 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9797 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9798 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9799 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9800 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9801 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9802 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9803 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9804 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9805 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9806 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9807 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9808 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9809 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9810 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9811 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 2 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 9 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 10 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 11 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 12 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9812 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9813 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 2 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9814 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9815 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9816 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 2 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9817 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 2 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9818 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9819 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9820 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9821 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9822 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 2 reward -5 next_state (1, 3) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9823 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9824 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9825 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9826 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9827 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 3 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 7 state (1, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 8 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 9 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 10 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 11 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9828 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9829 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 0 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9830 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9831 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9832 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 0 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 6 state (0, 4) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9833 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9834 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 0 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 7 state (1, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 8 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 9 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 10 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 11 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9835 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 5 state (0, 3) action 0 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9836 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 7 state (0, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 8 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 9 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 10 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 11 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9837 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9838 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9839 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9840 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9841 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9842 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 2 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 9 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 10 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 11 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 12 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9843 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9844 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 3 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 7 state (1, 4) action 2 reward -5 next_state (1, 3) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9845 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 2 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 7 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 8 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 9 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 10 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 11 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 12 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9846 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9847 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9848 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9849 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 6 state (0, 4) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9850 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9851 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9852 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 0 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 5 state (0, 3) action 1 reward -5 next_state (1, 3) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9853 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9854 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9855 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9856 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 0 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9857 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9858 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9859 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9860 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9861 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 0 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 9 state (3, 4) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9862 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 0 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9863 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9864 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9865 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 3 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9866 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9867 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 3 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9868 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (2, 3) action 1 reward -1 next_state (3, 3) old_q 6.199993038177491 max_q 8.0 new_q 6.199996519088746\n",
            "step 7 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9869 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9870 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 0 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 6 state (0, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 9 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 10 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 11 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 12 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9871 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9872 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 6 state (0, 3) action 0 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 7 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 8 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 9 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 10 state (2, 4) action 2 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 11 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 12 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 13 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9873 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 3 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 8 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 9 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 10 state (2, 4) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 11 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 12 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9874 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9875 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9876 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9877 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9878 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9879 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 0 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9880 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9881 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9882 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9883 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 0 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9884 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9885 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9886 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9887 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9888 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 1 reward -5 next_state (1, 3) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9889 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9890 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9891 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9892 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9893 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 3 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9894 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 2 reward -5 next_state (1, 3) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9895 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 2 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9896 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9897 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9898 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9899 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9900 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 3 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9901 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9902 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 1 reward -5 next_state (1, 3) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9903 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9904 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 2 reward -5 next_state (1, 3) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9905 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 0 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9906 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 0 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 4 state (0, 2) action 0 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9907 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9908 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "selecting random action\n",
            "step 3 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9909 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9910 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 8 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 9 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 10 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 11 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9911 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 6 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 7 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 8 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 9 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 10 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 11 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9912 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 9 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 10 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 11 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 12 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9913 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9914 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9915 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9916 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9917 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9918 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 0 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 9 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 10 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 11 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9919 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9920 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9921 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9922 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9923 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9924 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9925 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 10 state (3, 4) action 0 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 11 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 12 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9926 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9927 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9928 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9929 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 2 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9930 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 0 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9931 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 1 reward -1 next_state (1, 2) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (1, 2) action 1 reward -1 next_state (2, 2) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9932 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 2 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 10 state (3, 4) action 0 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 11 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 12 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9933 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9934 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9935 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9936 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 0 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 3 state (0, 1) action 1 reward -1 next_state (1, 1) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (1, 1) action 1 reward -1 next_state (2, 1) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (2, 1) action 3 reward -1 next_state (2, 2) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (2, 2) action 3 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9937 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9938 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9939 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9940 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 0 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 7 state (1, 4) action 3 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 10 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 11 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 12 state (3, 4) action 0 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 13 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 14 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9941 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9942 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9943 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9944 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9945 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9946 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 0 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 6 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 7 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 8 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 9 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 10 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 11 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9947 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9948 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 1 reward -5 next_state (1, 3) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9949 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 2 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9950 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9951 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9952 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 0 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 6 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 7 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 8 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 9 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 10 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 11 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9953 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9954 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9955 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9956 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9957 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9958 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9959 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9960 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 2 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9961 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9962 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 2 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9963 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9964 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9965 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9966 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9967 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 1 reward -1 next_state (1, 2) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (1, 2) action 1 reward -1 next_state (2, 2) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9968 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9969 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9970 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9971 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 2 reward -5 next_state (1, 3) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9972 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9973 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9974 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9975 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 7 state (1, 4) action 3 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9976 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9977 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9978 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9979 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 2 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9980 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9981 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9982 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9983 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9984 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 0 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 8 state (2, 4) action 2 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 9 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 10 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 11 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9985 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9986 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "selecting random action\n",
            "step 3 state (0, 0) action 1 reward -5 next_state (1, 0) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9987 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 1 reward -5 next_state (1, 0) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9988 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9989 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 0 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9990 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9991 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9992 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 2 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9993 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 2 reward -5 next_state (1, 3) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9994 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9995 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9996 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9997 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 3 state (0, 1) action 0 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 7 state (0, 4) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 8 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 9 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 10 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 11 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 12 state (2, 4) action 0 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 13 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 14 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 15 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9998 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9999 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 0 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.802  | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -2.262 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -2.562 | -2.5   | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot(reward_list1, eps_list1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "ESzxfHjZ--yn",
        "outputId": "4d532644-2146-409b-cf5b-e7c068a8c628"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x800 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABlMAAAK9CAYAAACw1pqYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzddZgcVdYG8Ld7fDISnbgrEZKQECEKBEKQRcJii7M4u4t9u8Bii/uyuBPcIXgIMUJCBOLu7jozGZ/p/v7ovtW3qm5VV7XOJO/veXiYrq6uul1dXZM5p+45Hr/f7wcREREREREREREREREpeZM9ACIiIiIiIiIiIiIiotqMyRQiIiIiIiIiIiIiIiIbTKYQERERERERERERERHZYDKFiIiIiIiIiIiIiIjIBpMpRERERERERERERERENphMISIiIiIiIiIiIiIissFkChERERERERERERERkQ0mU4iIiIiIiIiIiIiIiGwwmUJERERERERERERERGSDyRQiojro/vvvh8fjSfYw6pSNGzfC4/Fg3LhxyR4KERERERFFaOTIkRg5cqT2mP/Op3bt2uH0009P9jCI6AjAZAoR1UnLli3DxRdfjJYtWyIjIwMtWrTAxRdfjOXLl7vajsfjwU033RSnUR6eLr/8cuTk5CR7GHXSrl27cPvtt6Nbt27Izs5GvXr10K9fPzz00EM4ePBgsodHREREROTYuHHj4PF4LP+bPXt2socYd5dffrnuPefk5KBDhw4499xz8cUXX8Dn8yV7iDFTVFSEhx9+GP3790d+fj4yMjLQtm1bnH/++fj++++TPTwiooRITfYAiIjc+vLLL3HhhReiYcOGuOqqq9C+fXts3LgRb775Jj7//HN88sknOPPMM5M9zLi6++67cccddyR7GHVK27ZtUVZWhrS0tKTs//fff8epp56KQ4cO4eKLL0a/fv0AAH/88Qcee+wxTJ8+HRMnTkzK2IiIiIiIIvXAAw+gffv2puWdOnWKy/5q27+ZMzIy8MYbbwAAysrKsGnTJnz77bc499xzMXLkSHz99dfIy8tL8iijs3btWowePRqbNm3C2WefjUsvvRQ5OTnYsmULfvjhB5x++ul49913cckllyR7qEREccVkChHVKevWrcMll1yCDh06YPr06WjSpIn23D/+8Q8MGzYMF198MRYvXqz8B31tVVpaiuzsbMfrp6amIjX1yL6El5SUoF69eo7X93g8yMzMjOOIrB08eBBnn302UlJSsGDBAnTr1k33/MMPP4zXX389Jvtye1yIiIiIiKIxZswY9O/fP2H7S09PT9i+nEhNTcXFF1+sW/bQQw/hsccew5133omrr74an3zySZJGF73q6mqcffbZ2LVrF3755RcMGTJE9/x9992HiRMnoqamxnY7/DuFiA4HLPNFRHXKk08+idLSUrz22mu6RAoANG7cGK+++ioOHTqEJ598Mmb79Pl8ePbZZ9GjRw9kZmaiadOmuPbaa3HgwAHdel9//TVOO+00tGjRAhkZGejYsSMefPBB0z8qR44ciZ49e2LevHkYPnw4srOzcdddd2m1fp966im89tpr6NixIzIyMnDsscfi999/121D1TNFlCwbP348evbsiYyMDPTo0QMTJkwwvadp06ahf//+yMzMRMeOHfHqq6/GvA/LnDlzcMoppyA/Px/Z2dkYMWIEZs6cqVtn06ZNuOGGG9C1a1dkZWWhUaNG+POf/4yNGzfq1hMlBH755RfccMMNKCgoQKtWrQCEjufy5ctx/PHHIzs7Gy1btsQTTzyh24aqlrIoWbZt2zacddZZyMnJQZMmTXD77bebPrd9+/bhkksuQV5eHurXr4/LLrsMixYtclSf+dVXX8W2bdvwzDPPmBIpANC0aVPcfffd2mOPx4P777/ftF67du1w+eWXhz0un3/+ubZcNRaPx4OlS5dqy1auXIlzzz0XDRs2RGZmJvr3749vvvnG9j0RERERETkh/53z3//+F23btkVWVhZGjBih+zcpAOzcuRNXXHEFWrVqhYyMDDRv3hxnnnmm7u8DY88UK1OmTMGwYcNQr1491K9fH2eeeSZWrFihW0f8DbR27VpcfvnlqF+/PvLz83HFFVegtLQ0qvd9xx134OSTT8Znn32G1atX65778ccftbHl5ubitNNOw7Jly0zbWLlyJc477zw0adIEWVlZ6Nq1K/79739rzzv5e2r9+vXweDz473//a9r+b7/9Bo/Hg48++sjyfXz22WdYunQp7rnnHlMiRTj55JMxZswY7bHd329u/wacPn06rr32WjRq1Ah5eXm49NJLTX+LCzNmzMCAAQOQmZmJDh064N1337V8X0REkTiyb2smojrn22+/Rbt27TBs2DDl88OHD0e7du3w7bff4qWXXorJPq+99lqMGzcOV1xxBf7+979jw4YNeOGFF7BgwQLMnDlTKxs1btw45OTk4NZbb0VOTg6mTJmCe++9F0VFRabkzr59+zBmzBhccMEFuPjii9G0aVPtuQ8//BDFxcW49tpr4fF48MQTT+Ccc87B+vXrw5aomjFjBr788kvccMMNyM3NxXPPPYexY8di8+bNaNSoEQBgwYIFOOWUU9C8eXP85z//QU1NDR544AFTcioaU6ZMwZgxY9CvXz/cd9998Hq9ePvtt3HCCSfg119/xYABAwAESl/99ttvuOCCC9CqVSts3LgRL7/8MkaOHInly5ebZuvccMMNaNKkCe69916UlJRoyw8cOIBTTjkF55xzDs477zx8/vnn+Ne//oVevXrp/lGvUlNTg9GjR2PgwIF46qmnMGnSJDz99NPo2LEjrr/+egCBhNoZZ5yBuXPn4vrrr0e3bt3w9ddf47LLLnN0PL755htkZWXh3HPPdXMYHTMel9NOOw05OTn49NNPMWLECN26n3zyCXr06IGePXsCCPQfGjJkCFq2bIk77rgD9erVw6effoqzzjoLX3zxBc4+++y4jJmIiIiIDg+FhYXYu3evbpnH49H+/hDeffddFBcX48Ybb0R5eTn+97//4YQTTsCSJUu0v4fGjh2LZcuW4W9/+xvatWuH3bt34+eff8bmzZvRrl07x2OaNGkSxowZgw4dOuD+++9HWVkZnn/+eQwZMgTz5883beu8885D+/bt8eijj2L+/Pl44403UFBQgMcffzyiYyJccsklmDhxIn7++Wd06dIFAPDee+/hsssuw+jRo/H444+jtLQUL7/8MoYOHYoFCxZoY1u8eDGGDRuGtLQ0XHPNNWjXrh3WrVuHb7/9Fg8//DAAZ39PdejQAUOGDMEHH3yAW265RTe+Dz74ALm5ubZlsr/99lsAMM2+cUL195vbvwFvuukm1K9fH/fffz9WrVqFl19+GZs2bcK0adN0NwOuXbsW5557Lq666ipcdtlleOutt3D55ZejX79+6NGjh+uxExEp+YmI6oiDBw/6AfjPPPNM2/X+9Kc/+QH4i4qKwm4TgP/GG2+0fP7XX3/1A/B/8MEHuuUTJkwwLS8tLTW9/tprr/VnZ2f7y8vLtWUjRozwA/C/8sorunU3bNjgB+Bv1KiRf//+/dryr7/+2g/A/+2332rL7rvvPr/xEg7An56e7l+7dq22bNGiRX4A/ueff15bdsYZZ/izs7P927Zt05atWbPGn5qaatqmymWXXeavV6+e5fM+n8/fuXNn/+jRo/0+n09bXlpa6m/fvr3/pJNO0i0zmjVrlh+A/91339WWvf32234A/qFDh/qrq6t164vjKa9fUVHhb9asmX/s2LHaMnF83377bd17AeB/4IEHdNvs27evv1+/ftrjL774wg/A/+yzz2rLampq/CeccIJpmyoNGjTw9+7d23YdGQD/fffdZ1retm1b/2WXXaY9tjsuF154ob+goEC3fMeOHX6v16t7vyeeeKK/V69eunPU5/P5jzvuOH/nzp0dj5mIiIiIjizi36Kq/zIyMrT1xL/Ds7Ky/Fu3btWWz5kzxw/Af8stt/j9fr//wIEDfgD+J5980na/I0aM8I8YMcK0ffnf5H369PEXFBT49+3bpy1btGiR3+v1+i+99FJtmfi76sorr9Tt4+yzz/Y3atQo7DEI97fRggULdO+xuLjYX79+ff/VV1+tW2/nzp3+/Px83fLhw4f7c3Nz/Zs2bdKta/wby0j199Srr77qB+BfsWKFtqyystLfuHFj3d8XKn379vXXr1/ftPzQoUP+PXv2aP8VFhZqz9n9neL2b8B+/fr5KysrteVPPPGEH4D/66+/1pa1bdvWD8A/ffp0bdnu3bv9GRkZ/ttuu832/RERucEyX0RUZxQXFwMAcnNzbdcTz4v1o/HZZ58hPz8fJ510Evbu3av9169fP+Tk5GDq1KnaullZWbqx7t27F8OGDUNpaSlWrlyp225GRgauuOIK5T7PP/98NGjQQHssZuGsX78+7HhHjRqFjh07ao+PPvpo5OXlaa+tqanBpEmTcNZZZ6FFixbaep06dQo7g8OphQsXYs2aNbjooouwb98+7ZiVlJTgxBNPxPTp0+Hz+QDoj1lVVRX27duHTp06oX79+pg/f75p21dffTVSUlJMy3NycnR3SqWnp2PAgAGOjhkAXHfddbrHw4YN0712woQJSEtLw9VXX60t83q9uPHGGx1tv6ioKOx5Gw3VcTn//POxe/duTJs2TVv2+eefw+fz4fzzzwcA7N+/H1OmTMF5552nnbN79+7Fvn37MHr0aKxZswbbtm2L27iJiIiIqO578cUX8fPPP+v++/HHH03rnXXWWWjZsqX2eMCAARg4cCB++OEHAIG/DdLT0zFt2jTLMk5O7NixAwsXLsTll1+Ohg0basuPPvponHTSSdr+ZKq/B/bt24eioqKIxwEE/k4BQn+b/vzzzzh48CAuvPBC3d+XKSkpGDhwoPb35Z49ezB9+nRceeWVaNOmjW6b8mwMp39PnXfeecjMzMQHH3ygLfvpp5+wd+/esDNOioqKtPch+/e//40mTZpo/1100UWmdVR/p7j9G/Caa67RVWi4/vrrkZqaavocu3fvrqtg0aRJE3Tt2tXx34RERE6wzBcR1RlOkyTFxcXweDxo3LgxgEDAuLKyUns+KysL+fn5jva5Zs0aFBYWoqCgQPn87t27tZ+XLVuGu+++G1OmTDH9o7uwsFD3uGXLlpaNE43/WBaJFSd/UBhfK14vXrt7926UlZWhU6dOpvVUyyKxZs0aALAtgVVYWIgGDRqgrKwMjz76KN5++21s27YNfr9ft45R+/btldtr1aqVqd9LgwYNsHjx4rDjzczMNJU4k48ZEKjr27x5c9OUc6fHLC8vLybJPSuq4yL61XzyySc48cQTAQRKfPXp00crMbB27Vr4/X7cc889uOeee5Tb3r17t+6PXiIiIiIi2YABAxw1oO/cubNpWZcuXfDpp58CCNxw9vjjj+O2225D06ZNMWjQIJx++um49NJL0axZM8fj2bRpEwCga9eupueOOuoo/PTTT6Zm6HZ/g+Xl5Tnet9GhQ4cAhP6WFX8rnXDCCcr1xb5EAkCU5rXi9O+p+vXr44wzzsCHH36IBx98EECgxFfLli0txyLk5uZi3759puU33HADTj/9dADWJcBUf6e4/RvQeN7k5OSgefPmph4r4f4WJiKKBSZTiKjOyM/PR4sWLcIGyBcvXoxWrVppyYpzzjlH14j7sssuC9swXPD5fCgoKNDdwSMTQfiDBw9ixIgRyMvLwwMPPICOHTsiMzMT8+fPx7/+9S9tJoYg341jpJp5AUD3j8x4vDZWxHt98skn0adPH+U64s6mv/3tb3j77bdx8803Y/DgwcjPz4fH48EFF1xgOmaA9XGLxzGLpW7dumHhwoWorKy0TKI5UVNTo1yuOi4ZGRk466yz8NVXX+Gll17Crl27MHPmTDzyyCPaOuIY33777Rg9erRy27FKshERERERhXPzzTfjjDPOwPjx4/HTTz/hnnvuwaOPPoopU6agb9++cdtvvP6OWrp0KYDQv6nFv7/fe+89ZYIoNdVdmM7N31OXXnopPvvsM/z222/o1asXvvnmG9xwww3weu2L1oi/ZbZt26a7yapLly7aTVqZmZnK16r+TnH7N6BTteFvYSI6/DGZQkR1yhlnnIFXX30VM2bMwNChQ03P//rrr9i4cSNuvfVWbdnTTz+tuxtFLm8VTseOHTFp0iQMGTLENgEybdo07Nu3D19++SWGDx+uLd+wYYPjfSVCQUEBMjMzsXbtWtNzqmWREGXG8vLyMGrUKNt1P//8c1x22WV4+umntWXl5eU4ePBgTMYSK23btsXUqVNRWlqqm53i9JidccYZmDVrFr744gtceOGFYddv0KCB6RhUVlZix44drsZ9/vnn45133sHkyZOxYsUK+P1+rcQXAHTo0AEAkJaWFvazIiIiIiKKhpiVIVu9erWpGXzHjh1x22234bbbbsOaNWvQp08fPP3003j//fcd7adt27YAgFWrVpmeW7lyJRo3bqyblRJP7733HjweD0466SQAob+VCgoKbP/9Lf6dLpIxVtz8PXXKKaegSZMm+OCDDzBw4ECUlpbikksuCfseTj/9dHz88cf44IMP8M9//jPs+uG4/RtwzZo1OP7447XHhw4dwo4dO3DqqadGPRYiIrfYM4WI6pTbb78d2dnZuPbaa01Tjffv34/rrrsOeXl5uOmmm7Tl/fr1w6hRo7T/unfv7nh/5513HmpqarSp0LLq6mrtH3ziLhj5rpfKykq89NJLbt5e3KWkpGDUqFEYP348tm/fri1fu3atsq5xJPr164eOHTviqaee0qa1y/bs2aMbj/FOoeeff95yBkayjB49GlVVVXj99de1ZT6fDy+++KKj11933XVo3rw5brvtNqxevdr0/O7du/HQQw9pjzt27Ijp06fr1nnttddcH5dRo0ahYcOG+OSTT/DJJ59gwIABuqn2BQUFGDlyJF599VVlokb+rIiIiIiIojF+/HhdP765c+dizpw5Wu/G0tJSlJeX617TsWNH5ObmoqKiwvF+mjdvjj59+uCdd97RBeiXLl2KiRMnJiwI/9hjj2HixIk4//zztVJVo0ePRl5eHh555BFUVVWZXiP+/d2kSRMMHz4cb731FjZv3qxbR/77yc3fU6mpqbjwwgvx6aefYty4cejVqxeOPvrosO/jvPPOQ/fu3fHggw9i9uzZynXczP5w+zfga6+9pjtWL7/8Mqqrq2PW85OIyA3OTCGiOqVTp0549913ceGFF6JXr1646qqr0L59e2zcuBFvvvkmDhw4gI8//tiyt4bKH3/8oQtkCyNHjsSIESNw7bXX4tFHH8XChQtx8sknIy0tDWvWrMFnn32G//3vfzj33HNx3HHHoUGDBrjsssvw97//HR6PB++9916tnFJ8//33Y+LEiRgyZAiuv/561NTU4IUXXkDPnj2xcOFCR9uoqqpSHrOGDRvihhtuwBtvvIExY8agR48euOKKK9CyZUts27YNU6dORV5eHr799lsAgbuc3nvvPeTn56N79+6YNWsWJk2ahEaNGsXyLUftrLPOwoABA3Dbbbdh7dq16NatG7755hvs378fAEz9WowaNGiAr776Cqeeeir69OmDiy++GP369QMAzJ8/Hx999BEGDx6srf/Xv/4V1113HcaOHYuTTjoJixYtwk8//aT1AXIqLS0N55xzDj7++GOUlJTgqaeeMq3z4osvYujQoejVqxeuvvpqdOjQAbt27cKsWbOwdetWLFq0yNU+iYiIiOjI8uOPP2LlypWm5ccdd5w2wwII/C03dOhQXH/99aioqMCzzz6LRo0aabMdVq9ejRNPPFEL3qempuKrr77Crl27cMEFF7ga05NPPokxY8Zg8ODBuOqqq1BWVobnn38e+fn5uP/++6N6v0bV1dXarJny8nJs2rQJ33zzDRYvXozjjz8er732mrZuXl4eXn75ZVxyySU45phjcMEFF6BJkybYvHkzvv/+ewwZMgQvvPACAOC5557D0KFDccwxx+Caa67R/u79/vvvtb/b3P49demll+K5557D1KlT8fjjjzt6f2lpafjqq68wevRoDB06FOeccw6GDRuGevXqYdu2bfjmm2+wefNmnHbaaY6253bMlZWV2nmxatUqvPTSSxg6dCj+9Kc/OdofEVEsMZlCRHXO2LFjMX/+fDz66KN44403sHv3bvh8PmRmZmLevHmuZp4AwJw5czBnzhzT8gcffBBDhw7FK6+8gn79+uHVV1/FXXfdhdTUVLRr1w4XX3wxhgwZAgBo1KgRvvvuO9x22224++670aBBA1x88cU48cQTLXtRJEu/fv3w448/4vbbb8c999yD1q1b44EHHsCKFSuUfwSpVFZWKhuWd+zYETfccANGjhyJWbNm4cEHH8QLL7yAQ4cOoVmzZhg4cCCuvfZabf3//e9/SElJwQcffIDy8nIMGTIEkyZNqnXHLCUlBd9//z3+8Y9/4J133oHX68XZZ5+N++67D0OGDLGsESwbOHAgli5diieffBLff/893nvvPXi9Xhx11FG44447dLOprr76amzYsAFvvvkmJkyYgGHDhuHnn3/WGsm7cf755+ONN96Ax+PBeeedZ3q+e/fu+OOPP/Cf//wH48aNw759+1BQUIC+ffvi3nvvdb0/IiIiIjqyWP2b8e2339YlUy699FJ4vV48++yz2L17NwYMGIAXXngBzZs3BwC0bt0aF154ISZPnoz33nsPqamp6NatGz799FOMHTvW1ZhGjRqFCRMm4L777sO9996LtLQ0jBgxAo8//rirG++cqKio0MplZWdno6CgAP369cO9996Ls88+29ST5KKLLkKLFi3w2GOP4cknn0RFRQVatmyJYcOG4YorrtDW6927N2bPno177rkHL7/8MsrLy9G2bVvdv+nd/j3Vr18/9OjRAytWrMBf/vIXx++xS5cuWLhwIZ577jl89dVX+PHHH1FZWYmmTZti4MCBuO+++7Rm9OG4HfMLL7yADz74APfeey+qqqpw4YUX4rnnngt7QxsRUTx4/LXxtmkiIpfeffddXH755bj44ovx7rvvJns4ddJZZ52FZcuWKWsZk9r48eNx9tlnY8aMGVpijYiIiIiIQjZu3Ij27dvjySefxO23357s4Rzx+vbti4YNG2Ly5MnJHoqtcePG4YorrsDvv/+O/v37J3s4REQA2DOFiA4Tl156KR599FG89957uOuuu5I9nFqvrKxM93jNmjX44YcfMHLkyOQMqA4wHrOamho8//zzyMvLwzHHHJOkURERERERETnzxx9/YOHChbj00kuTPRQiojqJZb6I6LDxr3/9C//617+SPYw6oUOHDrj88svRoUMHbNq0CS+//DLS09O1esVk9re//Q1lZWUYPHgwKioq8OWXX+K3337DI488gqysrGQPj4iIiIiISGnp0qWYN28enn76aTRv3hznn39+sodERFQnMZlCRHQEOuWUU/DRRx9h586dyMjIwODBg/HII4+gc+fOyR5arXXCCSfg6aefxnfffYfy8nJ06tQJzz//vK7XCRERERERUW3z+eef44EHHkDXrl3x0UcfOer5SEREZuyZQkREREREREREREREZIM9U4iIiIiIiIiIiIiIiGwwmUJERERERERERERERGTjiOqZ4vP5sH37duTm5sLj8SR7OEREREREceX3+1FcXIwWLVrA6+V9VBQe/2YiIiIioiON07+bjqhkyvbt29G6detkD4OIiIiIKKG2bNmCVq1aJXsYVAfwbyYiIiIiOlKF+7vpiEqm5ObmAggclLy8vCSPhoiIiIgovoqKitC6dWvt38FE4fBvJiIiIiI60jj9u+mISqaIaep5eXn8w4CIiIiIjhgs10RO8W8mIiIiIjpShfu7iYWTiYiIiIiIiIiIiIiIbDCZQkREREREREREREREZIPJFCIiIiIiIiIiIiIiIhtMphAREREREREREREREdlgMoWIiIiIiIiIiIiIiMgGkylEREREREREREREREQ2mEwhIiIiIiIiIiIiIiKywWQKERERERERERERERGRDSZTiIiIiIiIiIiIiIiIbDCZQkREREREREREREREZIPJFCIiIiIiIiIiIiIiIhtMphAREREREREREREREdlgMoWIiIiIiIiIiIiIiMgGkylEREREREREREREREQ2mEwhIiIiIiIiIiIiIiKywWQKERERERERERERERGRDSZTiIiIiIiIiIiIiIiIbDCZQkREREREREREREREZIPJFCIiIiIiIiIiIiIiIhtMphAREREREREREREREdlgMoWIiIiIiIiIiIiIiMgGkylEREREREREREREREQ2mEwhIiIiIiIiIiIiIiKywWQKERERESXclv2l2LyvNNnDICIiIiIiInIkNdkDICIiIqIjS2W1D8OemAoAWPngKchMS0nyiIiIiIiIiIjscWYKERERESXUoYpq7ecDpZVJHAkRERERERGRM0ymEBEREVFClVXVaD+XVtbYrElERERERERUOzCZQlRH+f1+XPfePNz44fxkD4WIiMiVssrQzJTi8mqbNYmIiIiIiIhqByZTiOqo/SWVmLBsJ75fvAMHSlgihYiI6obf1u3FqGema48PMZlCREREREREdQCTKUR1lNfj0X6u8vmSOBIiIiLnLnp9ju6x3D+FiIiIiIiIqLZiMoWojpJyKaiu8SdvIERERFGorOENAURERERERFT7MZlCVEfV+EIJFNabJyKiuqqymskUIiIiIiIiqv2YTCGqo6RcCu79emnyBkJERBSFg6Xs+0VERERERES1H5MpRHWU3x/KpszZsD+JIyEiInIuPVX/z8+Hvl+RpJEQEREREREROcdkClEdVeNnnxQiIiIiIiIiIiKiRGAyhaiOkst8Hd+1SfIGQkRE5IbhXoC0FE9yxkFERERERETkApMpRHWUT8qmdGuel8SREBERRa5Xy/xkD4GIiIiIiIgoLCZTiOooucqXnFghIiKq1QwTUfgrjIiIiIiIiOoCJlOI6ii5Z0oNI1FERFRX+G0fEhEREREREdVKTKYQ1VE+KZnCXAoRJUtFdQ3emrEB6/YcSvZQqA4or6pBZY1Pv9DPX2JERERERERU+6UmewBEFBm/LpnCQBQRJcedXyzBlwu2wfM9sOHR05I9HKrlHp+w0rSMNwQQERERERFRXcCZKUR1lHxj74a9JckbCBEd0b5csA0AJxeQM2/P3Gha5mehLyIiIiIiIqoDmEwhqqPkBMovq/fgYGllEkdDREQUmdLKmmQPgYiIiIiIiCgsJlOI6ihjzfmtB8qSNBIiIqLIrd9TgspqX/gViYiIiIiIiJKIyRSiOqrccCevx5OkgRARETngt6kFt7+EsyuJiIiIiIiodmMyhagO8vv9+OcXi3XLqmtYc56IiGov44zKK4a0036uqGapLyIiIiIiIqrdmEwhqoO27DeX9Kph92ciIoqTR39cgWd+Xh3VNgrLqrSfVzxwCi4/rp32uIJlvoiIiIiIiKiWYzKFqA5KSzXX9KrxMZlCRESxt7OwHK/+sh7PTV6D8qrIZ5As316k/ZyZ5kXbRvWQnZ4CAFFtl4iIiIiIiCgRmEwhqoO8igYpLPNFRBR/u4vL8fbMDbpZFoe7ovLQe62oinwGifg91SI/E57g77GmeZmB7XJmChEREREREdVyTKYQ1UGqil4+lvkiIoq7y9/6Hf/5djn+9fni8CsfJlbuLNZ+LotiBkm1L5AwaVE/K7S9ysD2DlVUR7zdSHw5fysueXOOLlFEREREREREZIfJFKI6SJU4qWaZLyKiuCqvqsHyHYFSVROX70zyaBJn874S7efSysiTHlXBmSmpKaHZlTuLygEAr0xbF/F23dpTXIFbP12EX9fsxTEP/Jyw/RIREREREVHdxmQKUR0kJ1Na5AdKpNT4WCKFiJLLf5jPkDvv1VnJHkJSPDUx1Hj+hyU7It5OVU3g91Raivmfn3M27I94u27tK6nQfuaNCEREREREROQUkylEdZCIV2alpaCplkxJ4oAM/H7/YR9UJSJz8uRwj0sv3lqo/ZzqPTL+CVVt+OXy3uxNUWwrcILIyZRUb2CWyi2jukS8XbfqpacmbF9ERERERER0+DgyIgFEhxkRv/R6gJRgE9/aMjPF7/fjsrd/xzkv/wbf4R5ZJTrClRuakR9JvZtSvJ7wKx0GDpTqe4rURHFdrwr+nkqVjt3ons0AAPlZiUtwHEGnKREREREREcUQkylEdZAIWHo9Hi2gVxtKlVTV+HD+a7MxffUeLNh8EFsOlCZ7SEQUR7+u2aN7/PyUtUkaSeKlHjHJlErd4yuGtI94W1XVwTJfqaF/fnqDNwQk8jfYkZT0IyIiIiIiothhMoWoDtICQZ7Q3dHR3C0cK1NX7sZcqe69qi4+ER0+jEHp5yavSdJIEk9uoh5Pb/y6Hv/5dlnSSicaP+MmORkRb0sk/dOkRJT4MZG/wphMISIiIiIiokjUmUjno48+imOPPRa5ubkoKCjAWWedhVWrViV7WERJIcJA8syU2pBMMY7hSLlzm4iOPMbyV/Hy0Pcr8PbMjVi2vSgh+zMyVpCMJhFRFeyZkiol2sVviUQmi+RfVc3yMhO2XyIiIiIiIqrb6kwy5ZdffsGNN96I2bNn4+eff0ZVVRVOPvlklJSUJHtoRAnn18p8hRIWtaHMV6pxJgpzKWTj+8U78PvG/eFXpFpLFf/eUViW+IEcAQ5VVCdlv8bkSU1UyZRgma8URZmvBP4KkxM3YkxERERERERE4SSu22eUJkyYoHs8btw4FBQUYN68eRg+fHiSRkWUHCJv4pFmptSGZu+m3Enyh0S11Lo9h3Djh/MBAG9fcSyGdmrMsnAxUF5Vg9/W7cVxHRsjMy0l7vvLz0pTjOHICU7PXr8Pgzo0Ssi+klWZyphMieZXTVFZYDZPmlQizRNMpqzeVRz5hl2S30Jl9ZFzvhIREREREVF06mzkqrCwEADQsGFDy3UqKipQVFSk+4/ocCBiW16pZ0ptmJniNVxRkj8iqq12FpZrP1/x9u94ceqR07g8nu4evxRXjvsD//h4QUL210jRP+NI6kexbs+hhO1r7e5iVFTXJGx/gvFXSzSJ+w17A7OJ5d9XhWWBBvefzdsa8XbdkktSVnBmChERERERETlUJ5MpPp8PN998M4YMGYKePXtarvfoo48iPz9f+69169YJHCVR/IhgpcfjQWowg1EbeqZ4DHNT6lpQtbyqplYcxyOBcRbT81OYTImFz4MB6Z+W7UrI/vyKlGkd+9pHJSsBs3+Ee75ehn99vjiqbfh8fuwprnD3GmOZryiukQV5geRbtZTA2HOoUvt57e7EJKfem71J+7my2pfQfi1ERERERERUd9XJZMqNN96IpUuX4uOPP7Zd784770RhYaH235YtWxI0QqL48kk9U7y1qAG9xxAhr0vxqUMV1eh2zwSMfnZ6sodyRKjyxS5ASyG5mYmt3im+442lGSqf/nHk/K5t2yg7ofsbv3B7VK/vcNcPOPbhSfhusfPtGGeiRJMkF83s2zaqpy3LSA39U/Sh75dHvG03PpyzWfe4krNTiIiIiIiIyIE6l0y56aab8N1332Hq1Klo1aqV7boZGRnIy8vT/Ud0OBCxLA88WgP62hCM9hiyKckfkXPzNh0AkLg7o490v6zak+whHJaKyxPbpFy7Fklf/demr0/oGBKpZf0s3eNacNmNyE0fOi8DZyrzFUUypUa7ESB0wtRLD83umZak60IF+6YQERERERGRA3UmmeL3+3HTTTfhq6++wpQpU9C+fftkD4koaWprzxRj6aZoauvT4e2tmRuSPQSKAVHmy/jdP1wZg+7xvsbFs/zUea/MwvtSuSsrsWxAL8+qFOQkfP+2DSLfuAtn9mmhe7xsG3vqERERERERUXh1Jply44034v3338eHH36I3Nxc7Ny5Ezt37kRZWVmyh0aUcHLPFFH/fu6GfckcklIymiVH6kgJBhPFkmpmyuGsrFI/8yfe+eKqGvMOKmM0i2Luxv24e/zSsOvFsmeK2FSKV33CtGyQpVwea83z9ft5+IfElBcjIiIiIiKiuq3OJFNefvllFBYWYuTIkWjevLn23yeffJLsoRElXCiZAvyyOlAWZWotKJtkDLp9t3hHkkbi3pESDCaKJbnk4OHO7/ejpLLGtCyeflhivoZOWLYzrvs08vmMj6PomSLdCCDIZ061InkUSxXVNXhm4irM27Rft3wpZ6YQERERERGRA4ntVBuFeAcsiOoS8W3wxiEDMHXlbmSlp2BQh0auX2v8muZk1JlLjE5FdQ0yUlPCr0gxtae4Ak1yM8KvSLWGKPNlMdHgsLJoa6FpWbxnphRXmHvgbNxbEt+dGsS2zFfg//oyX6Gf490I/qLX52j9sWR9WteP636JiIiIiIjo8FBnZqYQUcDuonLMWR+4qzbWAcy9hypwxbjfccFrsyNKYBrLv7RqkB2rocWdfGf9C1PWJnEkR65oygdRcoTKfB3+2ZSdheWmZdE0Y3ciO82c1H3m59Vx3aeRqcxXFO/Zp2hAL6uKYzJl1c5iZSIFAEZ2bRK3/RIREREREdHhg8kUojpmwCOT8fiElQACAan7z+gOAGjfuF7U2z5YWqX9rKrVH44xyFZtrA9TR9Sl8mSHEy9/I9U5R1L667r355mWxTuZkpsZ39l9TrZvmpkSVc8U+5lM8SzztXl/qeVzzOMSERERERGREwxdEdVlHqBto0ASpV5G9GWp0lNCl4TCsiqbNdXKDP0E4l3/PpbkG6X3HqpI3kCOYKzmWPeImQTpqUfmPyfifc5mp8c3mVLPwfaN1/FoEkilwd8R+plMoZ/jWebLbiZnNAkiIiIiIiIiOnIcmdEPosOE1+NBRjCIWV4VfRAqLTUUbdpX4i6hsHlfKW7+ZKFuWTxLtsRaZXVorH3bNEjiSI5cczbsD78S1SrLtgX6iKSneDGmZzMAQNemuckcUkLFe2aKavu9W+XHdZ9GM9bu1T2ev1ldKiucovIqTFu1B4C+zJecV6mOazLFOpsS78+RiIiIiIiIDg9MphDVYV4PkBZMpsSi34S8iUd+WOnqte/O2mhaFkmpsFjy+/346zu/47xXZoW981hufD599R5ssSkJQ/Hx76+WJHsICXX3+CU46ZlfTDO66pIHvlsOAFi1qxhn9mkBAMjLiu9sikSrrPah3R3f65b1bVMfQPzLQ81VJBhbNUxsL6p3Z23SPW4dYS+smWtCSRmrRHs8f2fYtfWJpg8MERERERERHTmYTCGqwzzwaKVLYnFnrZxwmL56j7uxKAJVye6ZUlxRjUkrdmPuxv3YWWRuHm3n0rfmxmlUZKUulYWLhfdnb8aa3YcwbdXuuGw/EaWL5F2IO/+rD7OSSarPR7zXeM9oeGHqWtMyfwz3aZdgENoYkjeRzjhsUC9d+3nyytAxlYcQz9mMtjNTDrNzloiIiIiIiOKDyRSiOmzVrmKIUFQsgnqxjgsme2bK/kOV2s/hejqM+22j7vGgDo3iMSRH3p65AY9PcDcz6HBQVlV3Z2i4Jc8ky0qPvt+RSjz7T6ikBDO78QhMPz5hJV5UJBYSoUX9LNOytbsPAQiVOUukSHPUhyqqTcsc5FIwoH1D3ePxC7djZ6G75DSgb3bfVJoJKOc44tszxfxuU8U5y1wKEREREREROcBkClEdp81MsYhBVdf48PzkNZi3yb4fxe7icgx/cmrE4/AoAlXJ7pmyelex9nO4RNHn87bqHhvvxk4Un8+P/3y7HC9PW4f1ew4lZQyxUFhWhY53/YDXp6939bodhWVxGlHyvTljA75fvANA4PgIuZlpcdlfIpMpJ3YrgDd4MVq0tTCmCZXFWw/i5Wnr8ORPq1BYWhX+BTGWmWb+p5L4/J6bkvgET6SJ8x+X7IjodWJ3rRqEkkrPT1kT8XYA6+S23Lsq1lQTU9JSYlcmk4iIiIiIiA5/TKYQ1XHibltV6Zfi8iqc/dJvePrn1Rj78izb7Qx4eLLucbdm7ppIq+5wjmczYSeueW+e9rMf7oJl4s7zRCupDN09XpfDe+e8NBM1Pj8e/mFF2HVHdGmi/VyiuHveiQlLd2BpEmYJOLVqZzEe/G45bvxwPvx+Pw6WhmZNeT2BYO5tny7CmzM2xGyfVXEMTAt5wdkGN4/qot3lDwA/Lt0Zs3387aMF2s9vzozd8XGqtrXTiDSZEmnCQPxu2XoglOhcF2WiV34P8rASnUxJTUlMuTYiIiIiIiI6PDCZQlSHdWuWG0qmKJ6/5ZNFWJKoALMiUJXsMl86Lofyxfyt4VeKgwlSEDo9pe5cohduOagr/bNuT4nj157Uvan2c1ml+2Dq4q0Hcd3783H68zNcvzZR5BJLpZU1upkpPr8fr/yyDl/M34oHv1uO7QdjMzsnETNTxJ39GWlepEjR6s37S2Oy/V1F5di0L7StN39dj4rqxJaDU106Lh3cFgBwZp8WCR0LEHlJKtVsENWMQvP+zDtM8TopEKZnla/YLH2+iS7zVVwe+F4ymUJERERERERO1J1IHRGZDGzfULvbVhUMmrRiV8TbjkXZk2Q3oJct217k+jUlFdXYXey+N0A05MBxXSk9s3x7Ec56cSYGPRqa3dS7dX3b14w6KpBAuXBAG1xwbGtteSRB+Gjvkk8EOTF2qKJa99n6/MCTP63SHu89VBHRPvYZXhfPu/wF0Wze6/FoZb4A4GBZpdVLXBn4iH7GXEllDcbN3BiTbTulura2bhAoA2jX1DxeIg38VyuuJ9scJO7Ey04/urm2LMXr/p+P8uzAVg1CZRTlcpDxPGftPqskT6Kkw9yLL76Idu3aITMzEwMHDsTcuXMt1122bBnGjh2Ldu3awePx4Nlnn41qm36/H2PGjIHH48H48eNj8G6IiIiIiI5sTKYQ1WEej0cLEEUbdx/WubHusduAnUcxNaU2zUyZsXav5XOvTV+nXH7cY1Mw4OHJ2FMcWXA7Esd1CjW+r6kjd0ur+vHIsytUSSHRdL1L0xykpni1O91F2R2nyqtqkhLQdkv+LIvLq/Dlgm3aY5/Pr5XLAiIvK7Vxn342UCKSKaI3SorXo5ut8Oov61FeFd0MEqtSe6//mthSX6rPQ7zXeCY8rfrORLrLaovrsVV5PPH5id8FcgP5Q+Xue9fIx/Gqoe1D2zLM2koGVZlMolj45JNPcOutt+K+++7D/Pnz0bt3b4wePRq7d+9Wrl9aWooOHTrgscceQ7NmzaLe5rPPPutoBhoRERERETnDZApRHWIM6qd4PRA3CEcbDGqal6l77DZgp/pbPdkN6GWHyq17cTzyw0rlclGK6Y+N5mRBItSVAJ8qUCOfq6qgvgjQikTIMW3qA3D3nj/9Ywu63TNBVxqttqqRZmkVlVfjwzmbtcc+PzCwQ/RJNGOJtESU+RJjTfF4TEmHaEt9/W+yusl5hkXz8ngxvq+LB7VJSDLF6jywSrKE3Z7FTMHP55lLGr44dS263TMBv63bq73/VGk2yppd7meDiVG3rJ+FzLQUbbmxZFg8ktc1Pj/Oe9XcN+yEbgXa80Tx8Mwzz+Dqq6/GFVdcge7du+OVV15BdnY23nrrLeX6xx57LJ588klccMEFyMjIiGqbCxcuxNNPP225LyIiIiIico/JFKI65JtF23WPU7yxm5mSmaa/HLgNLmVLwbF+bRsAsL4TOlHk2Taf/LEl4u0k9F1IO6tFuSgdv9+Pv77zOy55cw78fr9uZshL09YC0Ae8SyrNiSy/lkwJPBYzm9ycdv/8fDEAfbPzSAPN8SZ/lsbkks/v183rijSJZgy+x3tmSmFplTaTwOs1J0/lGQeRsEqaNMpJj2q7bhln6V08qK1W0iyuM1MszoMZa/dGdI48+N0K5fKqGh/G/O9X3Pf1Um2ZKDt39/il2jjkWWMn91DfMe+EMff6wkXHoGX9LO3x94u3I9aszsXOTXMA1J1ZgFS3VFZWYt68eRg1apS2zOv1YtSoUZg1y5zci+U2S0tLcdFFF+HFF1+0nOEiq6ioQFFRke4/IiIiIiIyYzKFqJb5fvEOnPnCDCzYfMD0nPH+f48nFIyOtoFuZmqK7rHbIGH9eqEA5/FdmwAAyqIs8xOtjk1yHK0XrpnyC1PWxmI42F1Ujgtfm41vF1kHC+WjXlvvll60tRCTVuzGr2v2YnthOeTD98SEQBC2Qgrkq/rViJvkxawWu94/bqytpf1T5P5Bxrfo8/t17zvSJJrx2MU7mfLitND34lBFtakxfLSJrTkb9imXL95aiI17S5TPJUKq14Oa4Ic0YVn8ZkXZtZw6UOq+zJbVTKWfl+/Cih1FeGfWJtNzXo9HO6/kUo71MlJM64ZjlQDq17YBZt5xgvb4/m+Xu952+J2rF+dmBEqXMZdC8bB3717U1NSgadOmuuVNmzbFzp2RXTucbvOWW27BcccdhzPPPNPRdh999FHk5+dr/7Vu3Tr8i4iIiIiIjkBMphDVMjd+OB+Lthbiwe/MASXjHb0pHo8WjD5YWhVVs3TRw0Jwe+ezWH/UUU2Rnx1IrNiV1koEp4H5vwxsAwBobHHH+/IdsblD8/kpazFr/T787aMFKLPoDSAPOdrEQrw8O2m19nNNjT9sz5KbP15gWmYs8xWrGVb1MlLDr5QEcmLMb4js+vz69x1pEs2YvIh3ma+jmudqP7dukK2YcRPd9ts1qmf53JMTV0W3cReMX0Ovx6PreRMvdrMlDpZWxmw/dufJ2t2HtM9RPm8j6VMkXh3upSd3b2q/QgSsrqVZ6YHrRW1NXBNF4ptvvsGUKVMsm9er3HnnnSgsLNT+27Il8tm8RERERESHMyZTiGqpTfvM/QaMASy5zBcADHh4csT7S0sxlPlyEcgvLKvCMz8HAuwZaV7tbt/iCmd3Tz8xYSWGPj4l5j1WwiUjSiur8b9Ja7ByRzEAde+PWJIbcr85Y71yHf0MhdoZ4Nsi9cK4/fNF+OcXi3XPf2HowaC6i168NTGrJVa9f2prm13dZxlmZkqkx8B4usS7Z5GYqdAgOw31MlJxVPM83fPRnr92M8YSOTPFmPxK9Xotk6HCih1FeGbiKhRF0KhdsLt+rY6gZ4kVuUSjUcv6WcrzccGWg673IzbjsfiWjukZKEU0uGMj5fPRsDqS6cFScr8nqS8WHd4aN26MlJQU7Nq1S7d8165djkpvRbrNKVOmYN26dahfvz5SU1ORmhr4N9nYsWMxcuRI5XYzMjKQl5en+4+IiIiIiMyYTKG423qgFOMXbKu1geHaRA5+qo6WMdbv8XhgV6EqNUz5KtkBw53ObuKw9369FAeDAfMUjwe5mcFkioOZKX6/Hy9NW4etB8rw8Pfqmv6RCnfKPT9lLf47aTXmBgNp8Q7Et5D6Alg1WZaHXFtnprRumK39PHeDOQh522eLwm7jl9W7AYQShPsOBc6/cEFqWffm5mBPbT1mVVL/oDW79YFwn8+vn5kSac8U48yUOJf5Ekm1nOD3va1hJkm0n0Wsf2fMWb8PP0VQlmujIbGdkuIJWxrw1k8X4bkpa/HS1HWu9yfIM40m3zYCNx3fSXtcquhDZCfSBF2/tg20c7NJbqgZ9qIIkini6maVsxYJ/Xj8U0F1Ll48qA3qZ6UBgNb7hyiW0tPT0a9fP0yeHLrRxefzYfLkyRg8eHDctnnHHXdg8eLFWLhwofYfAPz3v//F22+/HfkbIiIiIiIiJlMo/oY/MRU3f7IQ788212Mnvfu+WWb7vDEGNXvdPttyK9WGqNQGm7u53565UffYTfDt5+WhOySXbCtETnBmipMyX3JvjY37Ynu3ebj3sHRboe5xnCemICMtdMlNTVFffuUxL9tehPdnb6p1TdWb52dG9frtB8u05II45it3BmYHPf3zaquXmeQoSnrV0lwKSqQG2MbvebXPr/vcI00iGM93q4RdrGQGZzQ0zQ2dD9lSucBokyF2n6Xb72pFdQ3Of202rn1vnu11UOXvH+nL1GWnpYQtc7UiWBpw8opdtuvZkY9fh8b1cMtJXbTHJRYN1a2oPoqhnRoDAMptkm5+ADsLA+UjC3L13/vtB8tcjSE0M0VN5KeinZ1mt2/hibFH484xR6Fbs1z1C4hi5NZbb8Xrr7+Od955BytWrMD111+PkpISXHHFFQCASy+9FHfeeae2fmVlpZYAqaysxLZt27Bw4UKsXbvW8TabNWuGnj176v4DgDZt2qB9+/YJfPdERERERIcfJlMo7kQQZ8bavckdSC3k9/ux9UDgrucanx8fztmse87IWIaqssbnKqh4/FPTdCWa7Li5O16+q3fD3hKkBUunOOnZsH5PKLB5QrcCx/t0wq6BM2AubZaXmRbT/RtlpIYCzVZ37ctL7x6/FHePX4q3Zm6I67jcKq+KbsbDNikIawxKu0kAqM792jozxS6xsHlfqS4wHelbMH5n3/5tY2Qbcrk/eUaK/D4jnWGj2paRVakoK2LmE2CehedWg3rpYWemCNHMeBBv3+MJXPtTvB6cdnRzAO4TVarvRWYwuSuXHzSqqvZpPaNW7dT3jjrusSmuxhDqmaI+dh6tb1I8kin6bZ53bGvUy0gNzYapZQlrOnycf/75eOqpp3DvvfeiT58+WLhwISZMmKA1kN+8eTN27Nihrb99+3b07dsXffv2xY4dO/DUU0+hb9+++Otf/+p4m0REREREFD9MplDCxONu07ru8QmrMPTxqXh9+noUG2rrq46WMfB8y0lddIHXpnkZCGfYE1Mxe/2+sOtFc1d5WrABRnWN/TY+/X0LTn3uV+1xVZj13ZKDcqpZDOmGZMroHpHVMHdKLrtmTORoFIfglV/U/VWSxS746oQcuBS9UjoV5AAALj+unePtqJMp0YwsfuwCxGt2F2OdlFSMuAG94WWihJETkZQEE+9JPpXltxltgDqWQXW5LFWVy/faqkGWaZnTZIrf74/4+yLev3zd92oJB3fbUp1TYmaRcXxyqb0JUlm03zcewBPnHu1ux0EV1TVaGUurI+fRZqZEtAuN6ly22qT4HI2zOIli6aabbsKmTZtQUVGBOXPmYODAgdpz06ZNw7hx47TH7dq1g9/vN/03bdo0x9tU8fv9OOuss2L4roiIiIiIjkxMplDCsGeK2Su/BOrpP/zDCuw23JGvCigZg8cdm9TTBeU7NM5xtN/3HJRciyYQmpoiAlT2QctHf9T3SPlm4baI96kivwXVWIwB0az0FGQpmjGrlkU2ntCAVP0+AHOza0Dfq6A2+HGp+74TMnnGgggOj+jSBEDgM4hGbZ2ZYjesT//Yqnsc6YwOY8LaaW7ypWlr0eXuH5X9b+yIa4Qc7JePf7TX/Fh+luXVoQTB/M0HXb22WzPzdzUj1dk/n7YXlqPfgz9j8z5nMwJl4vilSMdX/OT22KhWF8kU48f0xq+h5G2HxqFZR14P0M7QF8eJQxXV6Hr3BFz0+hzb9SJNFMn+8+0y9Lr/J2wylIy0Ol7idwD/fUJEREREREROMJlCCcNYhb2T/zvd9Wsy01J0s1HqZTgLRNslSi4c0BoAUFJZg2o3XeglacFkSriZJileQ5ktF3fSOxGuD8X+EnO5n8cVd17HKqgrj8GyzJdi8Z/7tXK0fb/fj6vf/QNXjvu9Vs8Ek4cmSvuI4HRFlCXEauv7dnMORfoejOe41fd85c4iDH9iKs56cSaGPDYFT0xYBQC466slrvYXKkMVCvb7dc9Hm0yxfs5tzxQ5l/r4hJUuxxEayH/P7w0A+MeJnR2/vqSyBuMjSBRrM1Oky6TDCTEmqgRdisVBlPsWyWXKvB5PRH2lJi7TJ1/XW/SsEe8tmvPm7ZkbUVHtw38NvZfkTd48KvTZidmC0ZakIyIiIiIioiMDkymUML+s3hN1eaBY2XqgFH95Yzamrtqd7KFYUgVUG2Sn6x5npqXA4/FopVeMwdSW9c3laQBggXRndlWND+3u+F57/JeBbbWfZzkoB6aSGoz+hbvbN9UQGSx20LDeqakrd+PLBaEAZlWN33RMjYFBr8eD0T3MNcdjFWaTg4RWh0YV00tzeBf8oYpq/Lx8F6as3I2dReWRDDEhDpaGStqJU0D0k6modn6NUB2r2pq0tRuXMUAeaU8aucdG4LF6p3d+uQSb95di4ZaDuv41bpOn2swJ6fTUz0xxtTnL7TtVWe3DNe/+gbdmmHsMycHyq4e5a8As3tOT5x6Ns/sGEpsFeYFm7PWznSWAjWUcHe03ePzUZb6i75my2UH/LPk6cs/p3U0lupwk/rYe0Deq79e2gXI98d6M2/T5/PjHxwvwv0lrwu5LmGBI4Ij3n5Hqxc2juoT2Kc1MMe63sLQKl741F18t0M8cM/p+8Q5c8uYc7DvkvN8TERERERER1U1MplBCyQ3Wk+m2Txdh5tp9uOLt35M9FEuqEJVclqppXoZWfirVou57g3rqQJ8cWDOWbBJNiQFgo8UdxOGkajNT7KOpxoB/WRTNmo2uGGf+bI2x2S5Nc3WPPR713dqR9JNQkfdvFQxV3SHttOSaHHStqq6lWQUAL0xdq/0sxpwRPO8qqn2YsnIX7h6/xFViRaitZb7sxmVsyB1p03LxfRWl/6ySEVaTC9z2LBLBZ/m8k99mtHf7u/0s3/ltIyYu34UHvluOZdsL8c/PF2FnYblurADQsYmzcoihcQT+L19/xY9OEz6RJMhEbyvd+aAlytxt6+sF+pkxPVrkuU6WH90q35SAdjKOZwyzRKxm13gUZb7emrEBV73zO75euB3/nbRa/UIxFumFJ3Qr0D0nPn7j+OWEvvGzfHHaWkxfvQe3fLLIdr83fjgfv67Zi0d/dDfjiYiIiIiIiOoeJlMorn5evkv3+FBF7GYeRMN4p2ytpLzrPrCwVYMs/HTzcC24J/7vNPgoB5QKS/WlrrwejxaIenHqOkeJBLl/wNhjWmnB3HBNfY2NnVX9QmLJmNzJy9Q3pU/1enSBYdkWB3dxhyMH66zu6K5QHG+nn6u8VnkEiQincg3Hza0DivJqWpmvah+uHPcH3p+9Ge/Pdp98/Xrhdny3eHtU44sH1efdNZjMa5yjn3EWaUJo0daDAELJP6vtNKyn7sETLvlpVKNIpsiiLblmdxwWby00LXvip1Aw+7TnZuDTP7bi7x8tAKCfJeN2VKqkUWgWhbNtRFIe659fLDYtc7tf4Z6vl2k/z/jX8fj02sGuxxNIdujfSLi+WMrtWKTzjDOqqmp8eOC75Zi6ao+j7RaWhWb/9GpZX/ecOF7Gc1VOkBmTf25nE+0p5swUIiIiIiKiwx2TKRQ3Nb5A/wZZhOXej0iqWJmI9TTLy0R9qeSXKKtlnI1QbXGnuRwzMiY8vB4PsoNNwHcWlePtmeaSOVbbe2Ls0Xj47J6hOvSK0ikyY1IrlpMKmgXL8MhMPSUM+8vJSLUMelZGW7MI+junrTanSl45vftdDj7LTaRjLS0l8l8dc9bv081ImrQikHAVZb52S8/N3WB/57zqs3p52jrc9OEClNSSxK1g/Ajzs9JwTNv6AIBdRRWGdSP7IoiXdWwSaBJudd5YzQwIl/w08lkEqIVom3q7/cqpZtYs2x5IusjH1O3hFe9Dfpt2CWzVNS9Wv/ti0VekVYNs1MtIxV2ndlNu226/m/frZytGMuPG6hor9iHempO3WFZZg68WbMWBkkrM33xAWy76dgkiUW/ctXwt+/T3LboESqrUrGbNruKwY/ll9R5liTkiIiIiIiI6fDCZQo7V+Px449f1uhr7dlbsKDIti+TuXAoRd44b7yAXpbnmbtyvW75ypzoAJMc4jUHnFK9HV3Zo2Xbz5yjz+/1aouH4bgXITEtBqhSgclM6KJbzUprlm5MpxuSSMSBZ4/fDY9Fk2apZsxOrdxVjZ2G5ozJfRWXmu6Edz0yRTos/Nh6wXjFKkQbJSyurcf5rs3XLNu8LzPgRM1PmbAidwz8t089sc6OksrYlUwz9GMqqdHfFyyI9vuIU7dUyH4D7pMF+xYwhOyI5KOfWRCIHiP5OfbtEbEGuenaNUVVwjLpkissrjU8xM8WuL43VddeNORYluKz6ikSiXoZ+hpnPDyxRzPiR91tYqr8+vTxtnev9Wl3PInlvD3y3DLd8sgiXvT1X93vRuAmrxF9Gqlf7Ht7z9TLc9mmopJc8U+VAqXqWynbDv4ce+G45dhfX3n5VREREREREFB0mU8ix2z5diIe+X4Ehj01xtL6qpJexN4DM7/dj64HSmASJwqkLSR3V8XthSqDXxCJDwEsOIv22di8AYOVO6ySIfIyfmqivQ+/x6O/eDXes5MCvuBtYvit4zW6LhI4iYBzLfhdtGmYD0Dc7NpakMQat04NRYVXiJNKR7Sgsw8n/nY5Bj07WBeeszvMHv19uWuY0ti4fP7ezDNxw2sNFEOXUth80BxlbBku9lVbFtizZvkPuEgPxpuo/JN/5Lov2o8sJHm+rniVywioaqiTDuCsGaD9H00Nid3G57Wyw045ublo26qgCxZoBPl2JPXdjUfdMCc5MUQzxQKn53LP73aey2mImhMcwe8OJcovvVqoimXfGCzOU64Z6G6XolouZZW4Ul6sTnWIf1T4/dheVO0p6fbMwUNJv8dZC3Uwmnz8wK1L0zNGOgeEtezwe5EhJpYlSadJqaYNWic/jFP8eEvskIiIiIiKiww+TKeTY+IXu+hC4vbv6zRkbMPTxqXjnt42uXheJeCZT/H6/LggTq+blALDKIsBWVBYKTl30xhws3HLQdmaC3ZgCM1Ocj0meeSJKpsilU2atU99hrQqUWpUli4QI9J7Wq7kWCDOek8aEw+lHtwCgLlsUaZJPvktdDuhWOijBJkRS5kvMVooHt0kvMVNJ1VD++pEdAQD7I0h+2B2WC1+fbf1kgv22bi/embXJtNwqQOs2WaW9Lvi5iCSN6ns+ecUuXW+JaIhhyomC1sEkZjTmbTqAAQ9Pxvo95gSUoDoF7Q6b/HVze3TFcZQ/LnGNUCWsYpETzrHoS6Rq0h5OkUXvD6vybOr9Bv4v98cKjMP9m7VKpoh9vDRtHQY8MhkzgzcG6PZn88blZLkfwItT12LQo5Px8rR1GPO/Xy33bVW2UP7dZvVdVY8j/jeEEBERERERUXIwmUJxowoA28VuHvp+BQDg/m/Nd+bHWkUEdd6duvHD+Rj06BQUl1fhf5PW4Oj//IR5m+JXcgkATjXcpX3HF4tRLyPFYm2gX7vQbI2TujfVPef1WLUHVpOTIiIoJd/xrGqobnydsHl/qamMTKTE2ef1hAJhVYZz0niOZgV7xajO01jEx+TA64PfLbe8Y9y0b8fJlNDPFxzbxtXY3LCa8WBFBFxVL8vLTAOgLxUlUyVghLk2MywOxug8ioV3fzMnUgD1zADA/fEVRCxZnL87CstRVqk/fv9SNDWPVI2izJeR3edn5YM56uMlU313lElHrf+GnE1xfnwLy6qwcMtBAECZtM+s4AyNGp/flLRy+7tPpami5xMQWc8Uqx7xqSnOByUSL8b34WQYjeql6x5bNXY3/uZ59Rdz3yfj7w05kScfd5/fr826fHyC/QwpqzyJPOPTTZnH+XH+fU9ERERERETJw2QKxU20zYfjabdUyz+SYJ+dH5bsxN5DFZi4bBf+O2k1yqt8eHbS6vAvjEK9dH3iJCcjFQW56mAcoL+7uGOTHN1zXo9Hdxd3uLtsq3TJFBFwCwWeXpy6Vvk6q9kx3yx2NwPKknTXvAha1xhmgxjLfgmqBNCoZ37Bb+vMd0qHI4fgjAFQp31Nlm5X9zEw8utmplgn06JlFZy1Xt/ct0IQn81FA9sqX7tY0cPh9s8W4cLXas/Mk3BUcdgbRna0npkSaTIl+Dq5V8nWA6W6da4c2j6ibRs9+dNKjAvOIrSb4bBm1yHX23aSzi1WlEG0+j4D+t9Hbo7uV/O3aj/P3RD6vsozR4zJAdXn5y5FDctBiu24eQ9WybkUizJzKlYJhw2G8nU+nx8XvzEHt366MLTMsH/VZ6fah6ocnTGZIpfDrI6wlJvV91DXg8XFERc3hhAREREREdHhh8kUcsx4d2k4qiC8VUApmYkXq5IjkZDvAt96INSY9tc17oPwbhjr8cuzS7o0zTGtL8ejjOWrvB598+MFYe6yFQGntBSPsi9AaaU6WWWVTEl3cbe0nVA/h9AYJi7fqVvH7Xl30etzcMmbcyybNIcdk2F/aYr3ekyb+qZljXKcNdt20uA+FtzOnBCrqw63NxjIbGhxfTHO3vD7/fh83lbMsmjQXVfcdEKnmJf5Esd5aKfG2rKXDA3CW+RnRbRtoxenhrZrTKac0C3UuyReJRVV1227MoHyIXVz+spJyWwpaZ3i9SA9mJQuN1zLYvHVszoFxCnjpuygMYksWM2MUu9XNIe3X2/VrmLMWLsXX87fpo3R+F6strHDQa8Ru1mlcklJN8fHKhko/45SfR5rLMpunnNMS8f7JiIiIiIiorqFyRSKG1Wg+vEJK7G/xNwb4ZfVuxMxJCVjGZxoyHevtm0Ufe8AK+f0tQ/WeL2hgLcqUCQH2o2fktfj0R2T7WECXFXVgS1Y1Z23fJ1Fc2mrptxuaW9Rev/GO4Yj6dHy65q9GPvyb47XlxNMb87YYPlcdY0PQx+fgvmbD5q2UWJxJ7eR7nONYzLFKlHTv20D5XKxvuqakJ+VZruvXEPvCLum5LWVKlbr9Xgskymqt/ji1LV4fbq57JFMXH/qZ4eO6VcLtmk/l1fV4N6vlzoYsb29hyp0j43XGDlJGKvra0aqF3/cPQr/u6BPcLvm70TPlvmmZZU1PpRX1UT83ZDf2hm9W+ie00puhSkfaNyOE1bfsVDPFBfJFIt13SSTnTa+l89p8V11OtZercyfn5Hd918+193kI60+G7lniupYXfnO78rXtWtUz/nOiYiIiIiIqE5hMoUccxsMsgrUqMrzHKqIbaktIBAwe+e3jZi4bKfterGcFSNvKi9L3UA4GgW5gRkKlw9pZ7ue1+PRxqJKpog7bg9VVOM1Q4DW6/WgykUdp0ptZoq7y4nVzJS01NhclkSfgxqb4JscZHST/Io0oG881VbvKsb4BdswafkujF+4XTebSVbi8PthlySLFb/fbxlQHWXovyOSK+J9q4LY4c4bYymi8so6mExRzMhL8XosZwYYg897D1XgyZ9W4eEfVujKGplfF9yfxcX6jV/XoygGM/GM10zj25CHXxJBMsU4/PevGohp/zcSjXMykJEamB2iumxbzW56a+YGXcJDldC3Il7WtWku+rSur3suxWK2RixmhVltwekMEZn8eT17fh/tZzfXPDGDLNxu5e+zKJfodKaVk74kcmLZLinm5vhYzUzp1izXdl9W1+s45rGJiIiIiIgoyZhMobixql+/SlEaIx530S/ZVoj7vlmGa96bZwrmyCXLwvUEceOAiyBdJETMx65HARC4A1285xSvxxQEXL6jCADwsKK2e0aqF+U2pVSMxOdsDIq/cnE/APq75GVWCYnf1u7FRkMdfjd2F5fju8XbsbMoMKPmq4XWPVhEkDE3MxVf3TAk4n3asfuk7h6/FDd/shB/ffcP093+Fw9qo5X8qnGY3JK/RpGWigrHLvl4kiGZ8tjYowNjsSj344QxMF2maDyu0rFJLbo73GJmilUJPPlcWLqtED8v36U9tkpCAqFjZXXObdpXavGMO+OlGQBAKNAuyJ9YqcNZVXYa1ktH82B5MrsG7Fa/R3YWlusSp89NUfdxUhEvUyUevBazRFTnudtqZ/I2B7RrGNqO9v6db0v+zp7ZJzS7JjvdeV8l8V7Dfa/khIgoyeV0rMbzSOWVX0Ll5ey26/P7cWw79Uw5I6sZYh0LQiUyVfuy+meLm/4qREREREREVLcwmUJh+Xx+zN98QNmU2/Z1LhIkkZRbCmffoVBiwziW045urv3sdGZKdY0P8zcfsCxPBQCXvTXX5SjdsZttIqvx63uGPHNebzx4Vk+8eVl/AKE7s+duMPedMDYu71yQYxuYF2W+jL1OWtTPBABkWzRCtwoKf/z7Fox8aprl/sI5/bkZuOnDBdpju/icSKTddlIXy7vaE8U4S+Hk7s1w8aC2AJwn/OTzPF5tiOTtGvsoGftY1MsIfPahninuB2V8id3MDJmTmVKV1T78umYPPv1ji65peyJ4PdbXnjW7Q03bz3xxJu78con2uNrm+mN1fRDf31idEyt36hPiK4LJWSHSmSlLtxWiqLzKlHiQJyeJwLfqvVi9v1SvN+LkopagUlxHxDJjGa2YzEyRtvHyxcdoP0fUMyX43pvkZuhmLWWlh2ZPNs2z78tUXF4FAOjbpgGeu7Cv5Xrye6+orjEts+OkXdZcqSm93Xb9fr8ySfL1jeakudWvU/311Pnx5swUIiIiIiKiwxeTKRTWi1PX4pyXfnPdqN1NgsQuQeFUYVkVyqW71uW7Q1WzYYRJK3ZZPid74LvlOOel33D/N8ss11kfxYwKJ0SQJlw1lP0lFVoAzev1oEOTHFwyqC1a1A81ny6pqA6blAECgd13Zm20fF4r82UozxW6a9vidS6Tc07tNgTF7zujh+W6ojFzikXgvUeLvITNcDD2DpF7ajhN+OmabMdsZMZ9hLb8/EV90UCaeWScQWO8c98YkGycEz6BZQwaOz0WTgKat3+2CJe8ORf//Hwxjn14kqPtRkL1LbMqxQUADbMDx6XG5ze93yqb978vOKPF2HZIlO1zcse8kyC98Vxds+uQYQ2514Sz7/nMtXtx+vMzMOrpX0yjlGc7hMpcmcdp3Wck8kSSX0tKq0u1qcaiGofrninBw9a7dX00ygklOrT372ZbwfEYy2jlZISSKQ2y7b+Lcv+QPxl6x6j2BYTKfDm9EcPqOiw7vmuB9rPdtcDqmd6GWZqA9c0J8uZVyTirMn3MpRARERERER2+mEyhsF7/1b7psRU3vUiibSpdVF6F3v+ZiEGPTtaWyfGsWev0MzDkO/2/sSkDJXt31iYAwAdzNjtaP1wMcct+c8mdQR0a6h4bg3R2gT39tsuUd6nLn0lReZVpO59eO1i5vfdmb7LcV5VFzxSrEjjCJ39s0X4+rVdz5TqxYJw9UV3j046jOA+sgmIN66Xj42vUxyTWcjMNyRRvKFjrNDGZiAb08jnUt3UDPHx2L+3xMW1CZXX+MrCNVJLIHxyTflv9LBrWy4zvwukd4k76/nyzyNl3P1pWI7Z6L+J6WKposm41w2LL/lKth4MxUSMSl04O3bxNB8KuM7JrE93jHi3zdI/l/Ti9tP+wZAeAQDL083lbdc/J70ebDaI4Dla/cnq0yIs4YS82qbrmWiWMlWW+XGZT5JmF+g0Fn3fx+1Vc54wzNfKz0vC3EzrhkkFtdf1BoiEPq6LKhzIXM5Oc9EyRfxfZnc9uZpLIhyVNmh4jX0NVhzvVMJXmePG94NQUIiIiIiKiwxaTKRSWKgjkJFFiLH0i9FcEUKOdpbB0ayEA4GBplbZM3r2cPKnx+fGhlBCJV33zcMGcn5btNC07t19r3WPjHb2WATaDpnkZyruRe7TQBz13FIYa6N50fCcMaB9I5tw8qrNuPbvdbQsGcI0JCXF3vNWp8uX8UN+FF/9yjOn5WPX8MNbhH/r4VFwx7ncAobvmjUHGl/9yDHq3ro+HzuqJJrn25W/CcRpDNQZ7vR6P1rh87sb9jpIj+uBfnJIp0na93kCflOFdmuDmUZ1114qeLfNNweYnJqzUbSvVwZ3oxvfh9G2t31PiKqEbT1aluTZa9DAR54KbQPTj0rE1Bv7FcbA6h86QZhq8Nt198tw4+yuS8kh2H5X8/bSb8fb2zA3K149fuB2vSu/L2EPKdlxiR8oyXxY9UxSDc98zJfB/42cZbsafyuKtBwEA2w6aG6bfdnJXPHhWT2W/krzMVNOycORzrLy6Bhv3OZ+t6eByAEAuW2d9ENwcH/kYZ0ulz+TP8dc1e0yvSzNMARM3FNSSyw4RERERERHFAZMpFJHNilkVRiKA1725PoBfL8McoKmKsmeKqvSNvEQOqm43BJTidRPpNe/Ns33+QKm5Wb3xrnBjg2otrhcmQu/zq2v9ezwepAcDPn4/UCSVbttyIPSZ3jyqCy4J9usA7GfCiEDnakMpNbtyPAAwoksT5XLBaZ+QcIwxwp1F5Zi2ag963f8Tpq4KBsgMuxrTqzm+vnEI2jaKvsRXqrHmkgVjQjHF69Elu5z0npBj9vE6r+VZMmleL9JSvHj3ygG4eVQXAIEZKW0bZeP0o5vrAsi/rduLRcGkp2A1I0hmfB9ukkQi0ZdsVtc3ubE8EDoeIom6r8R8jbAKTn+3eIf2s/GwjnpmOj6Ys8kyyPvQmT21n52UZDJ+BMayX/LTjpMpNt93OSFsVVoLMPfsEaav1gfC2zd2/r2261MljrMxaRebRKY6ca71THFxE8C9X1uXpgxt1/z+hnZu7HgfgpxsrajyKWdXuRmDivicrW7YAALnqMd1Cktf+kze+quKJKNxZopIprABPRERERER0eGLyRQKq7CsyrQszUGnWBF07ViQo1uuCjRF2zNFdee3HGyTm9Ebg/TxuoM/nN6t6puWGQNPxsdOZ6b4pF4LpgCVFoyDrt/F14ZyZ+/PCZVTsQtyieM5rLM+OSLGaHV8WzcM9G/5x4mdlc/H6nOxGrsceF26vVC5Tixkpjm7zJpnpuiPgZMG6fI24nV3dFmwL1Faikd5N/vDZ/fCtNtHIjczTXfsL3p9jmldJ2V9ojkPqh3264i3DobgvQjmv3pJP93y9GDfIZFYO1RhDkTfZxEYl0tvGQ/Z3kMV+PdXS62PpfQxODne4dbRl/lyX6LOqF5GivazsXRcJNwkasWaqjM1lNjRL1du3m3PFIvEuUgQxPrXlup3Sro0VcSuT4pM/spVVNeYfq/YUTWMVxHXIL/N19vv95uSGrPuPEG57sqdoRsBstND55r8OZ7aq5npdf3b6ctyin8X+f2B937DB/Pw0VxnZUGJiIiIiIiobmAyJUlW7SzGPeOXYldRebKHYstqVoGxP4aKCHYZ7z5XbXL9HmMDY3fk4JgYs7ybiuoa5c8AMKp706j2bdyvU1lS0EYwBqxMd1rb3CXdu1W+9nON349fgrMujEFH8Uq/36/rcWEsv3bVkPah19jEuESpLOPn7AlTjqaiKvC6zDTzcQiMz3qfbng9HpzVxz4QGKtZMCpOA4R3j1+qe1zjA8oqQ9FCJ+Vy5MB1vJKE+4OJSbvZZOKzD3enuSoZIzTLywQQ3cyU8S4CufGUEUyo/WVgG6x44BT8fMtwAMDoHs0w564TtfWMyRRVImL9XvV5MKhDI+1nqxkaVofO4SkKANhZWI6r3vnDdp2LBrbRfnaeTLF+Tp7NKM4p43a/MPRZsfPtou2m3wNW/DYJbKu+ULEoUajNLDQsF4mEL+Y7f79OZoCpZnFkpIauzXJCCwDaNMxWbsfYgN5YWtLOfsVMLBVxDOyuBa9OX2/qNSWuKXbW7D6klUOrkTJDqtlMpn5cwc/9p2U78dkfW/HDkp2488slYfdJREREREREdQeTKUky+tnpeG/2Jvzri8XJHoqtaMpvWTW9VQXXUhyWQrKyUQowiviKHGeR92gM2BjLkEXKbexa1Q9hUIdGWHTfydrjnYZkW2hmijnw9el1g/HGpf0D6/n8+HJBoCfJnA37deuJl/r9+s/mhuM76ta7algomdK3jT7RIrP6nMM1oC8PBowzUtWfvV0JFze8HuCOMUfZrjO0k305m2uHd4h4/07fhjGhU1JZjWPbhY67k2RdImZiiLu9cxXl+ow8Yb7WVkHeVK9Hd57K3MSpa1wej1j16TFtN7jZjNQUZKWn6HrFNM3L1EoLndIjcPe7mGHkJnEkz/JRlRAEgO+X7FAul2c/hEt+/P3jBWHHMrpHM/QO9iVx+h7sSiOlKnqmGDd722eLHO1H+GiOsxkDYj+qa25olox+ueo9LzGUuAvHqrzYd4sDCUKrhJnK8DAlFYFQjytZunRtNv7evO+M7gCAni31vz/lt15R7UOrBuqkSyTENac8mEwJ9zviD0PZzHDlMYUb3g+U6JRLL6oS7sbviii1t25PiXJWGREREREREdV9TKYk2aqdxeFXSiKr8ltOAmRFwfJgxlI+q3aZ33N6qvva5rJHfww1XxYBlqLyUHmyGikptHZ3dLNgrLgNw+45ZC7b5PV4kJ+VppXfMh67UOkX8/YyUlPQKVhSrcgm0CbfgbxXGkNmqv7O47zMUAmw+tn6ngi6MQUHZawfr5X5sgjOVgQDYhkWZbBiNbMixetBs3z7O5LH9DSXcJGpZhE5pXoXFw8K3LmfbbPdJjkZOE5K8jjJC8jJz3gkBlbvKsaMtXsBAPk254TgdmbKqKMKAABn920ZCpobjqCbGWBuk7TxmqEkmldbndNTbx+Jd64cgHP7tQIAVIpkiotckLzt1hazBqzIH0O4791cQ3LWSufgtchpBUe7JI6cqBV5KKsZOk7tLApfNg+QjofNzBRzzxTzur+t2+dqfNqMGMMpfLC0yrROOMOCvU8GGMpSyVSJBjnR3b6xvlynVYkzfc+UGsd9UACgWzPrGxum3DYC9esFrjlOZqa40bJ+lu6x6O0kJ1NU11O7c9bJbCAiIiIiIiKqe5hMSTI3gYZksCpD5iTmKO4KNZZTUZXyUMVECsuqsHSb+14WPy7dCQD45+ehWT9ykNRYoixW1ZDclvkyJi+AUOCsZYNAcMdUPkbRVF7mpKSUfMf//M0HteUZhnJb9TJS0SKYhFCNVQjNTNEfV2+YMl9iZorVtu3q4bsR7m7kk7o3DbvOxigCt6rz4tSezQHYB+97tgyUbRNlcpz0FdKX+QokDp2WznHi5P9OxxMTVgFwdq6FW8UYcHz47F7496lH4f9O6aotM9/172ysqu2H47QklVtLtxUBAMb9tlH5fJPcDIzo0kQrq6SV+bK4puxVJGLloQ932ThcTrBuPVDm6rUiAWaUEmZmmpFqpp6g/36Gfj5oMQPHCae9jOwa0IcSCvr3GItZdVazEOU+JhXVzi6SYjh2SWXVV0UuwZhjKPNldX2XP2+/P3RsujbNDTvOIZ0a4eJBbbSkoqxdo3rICo6nPHiuxOp3d5PcDN1jccirpDennJliMwD5c1u89WDcZr0RERERERFRYjGZQrZOePoX5fJwgQGfz6/dwWw3S8LOde/Nw+nPz8Cc9fZ39BoDoKJXiOxAaSXKq2qwu6jcMkERLbutHN/VXGZFFbgVARirEll2JWcA+x4UgtYzxTBi1SyJE49qqhyHTLwPc88U2L5WzExJj3OZLxHw7NI0R/l8Wkr4Y7ZfuhvcLdW7EMHhSgfB0GXbA4H4F6etDbuunHDZuK8Eo575Bcc8+LOzgbrkpHm86jx97Jxels83zcvE1cM7oCA3U0ssmpNRzs8LJ8dXVhWHMmlugqhpwRl64cp89X9oknk/wXXP69/KcTkjAGjXKFuXnN16oMxxPxEAuP9PPZTLxbXIaYLK6VGStxdNotCqV5ORuE6qLq1WZb6cJNYLS6tQbZMgFaei8bOUZwCWOCwlFS4JD6i/q/LMFOM4QuXWrGeO+fx+7Tru5JT0eDx46KxeePLco83j83q0ZIqYmRKv5KdQKh3fPcXmBKZx/7m63j6h5X96YSZemb4u9gMkIiIiIiKihGMyheJCDoQ7uQNYFXuaFUyifDl/m+1r//LGbN1jVdBrysrd6HbPBAx4ZDKmrtQnW2I3M8Xd+qpkgSmZYoi12fVMCSwPv1+PFgTTL1fdOewNkxABwvdMsXqp2KZIZmQZgpuxSnKJYckNuvXPhz9o1wyLvGeKKpjutCm9TMxusCM3XP5jk7NyTE4Z34ez4Kj9MrvjIGZLRDMzxW1CriaKHlFWKp3WuUJo1kFFlSjzZT0eq5lK4ny+ZFBby9fefnIXfPe3oTilRzO8dfmxpufPfGGm4zFb9cQQEyicBrydfiPkBIRxlqEbVr2ajEJVvswjjLQB/baDZej9wESMffk36/1C7Fevfnao6XmpzWweFbtrneoZOdFtfK3V7wa51KAfoe+r/F23SqBrY/F4dNcJMXNEzJ4sD34/3PSNsWNK1wYXvDNrk7bsu8U78I5hZpnxvY+7MvRdMt7Y8OavG6IeJxERERERESUfkykUkXCBbvlpJ3dJ223PrpHr3kMVmL1eHzQWd+4aG+MKk1bs0j2eump32PE5YddAWRVbU5UNEfEXq0CV37CekZPZAqGZKYZ9KzbqsQgWyqxmpoRrQB/q/xJY70RDuaDYJVMC27cKnjpJprRtFFkT5f0llTj3lVmm5ZG+t3B3vOvK2bnsF2LnrRkb0Ps/E3XLnJX5UpxTUtjWrgxX6Cn17CxZ3zb1cX+wIbYs3HE2zlaKR88UJ+XZBJEcKK6oht/vt01EHDDMyhABfPF9evCsnpavTfF60bNlPl65pB86NMkxzdJYubMY/1A0mv9orrOm7YD7Ml9OJ9PIh8RrUWbLiYJc+z5K2v6CO1R9naz6QqlycvI1RCSrFtk0pQ8lzvXLX/rLMdrPJZUuZ6bYrKP6Pd1f6rFiHIdYf/UufQ8yOaF73zfL8NOyQNnNZduL8PE1g9CjRR4+uWZQ2DHLuxPXCTHb9cv5WwEAH//u/Hy05fD8ue+bZbrH1YYPuqvU88U0k4c9VIiIiIiIiA4LTKaQJbu7a8PFHOUAmtfjwZVD2tuub7yDvN0d32s/y43kjd6aYb7bUwQkuzio0Q4E7jiNBbt4jOop1fH1GGemWNSjt0pQqZb/74I+hpXEeMMHkETA3O7zFgEl08yU4NXFKpgqAsXivRrHHqsZQ6Fkirqsj5PZC8ZAWOOcDIs19axKbG3aF1kPlnDB/mppKlMs2zE98N1yFBuSmk6SUMpkirTILsDoCfMdkH11wxBcrrrGhPlo2xgatVe7LPN1/zfL8Jc3ZtsmPdzkZ/Kz07Sfy6pqbF8rH7u7vlqCp39eHVju4HN3MqHj64XbTcvu/HJJ+BcGic8vnqWYxDUskmuFXfJbv55gNzNFPS6ZfN6qet6Y9msxC/Go5nlaLysxgyn8tgL/t7uxYcUO/cy3WXeegAbS+Wh8aU6wpFVeZqpuuZw8rKz24cM5oYTHoA6N8P3fh6FvmwZhxyy/b+MxmLwycAPE2zM3ht2OE6rr6o7C8L2D5M/0vP6tdN894/eQDemJiIiIiIgOD0ymkCW7ILObmSleD3DvGd1t70a1S9zYBYw27y81LUsNRvGdBtj6tK7vbMUoqIJrdkFG1awOv98vBcXUr1PNFjizT0vluv+dtMZy/6FxwDQOo5pgANp6Zor6dX7DndfGkUcahDUeG7F9q5kpM9fuDbtN42GN5E54WXZ6aviVFIx3QhvFI3Bt9V6dJVPMy5z28xCrGXfvZlaP1bofzd2Mv3+0wNTAO9zxNRr320bMXLsPs236OsnXtifGmntByOTZRH6//XuVtysHrJ18LikxnLVkvY/wiVhZqmJM6SleU4P7Pm3qaz+L3Ne2g+ED30ZOx2U1QySwzKLMV/DxOce0xFc3HKcbq9vxqb4vacFrmdPkny/M7w0AmLNBP8OzeX6WroxWUZk+mdo0L5BQPhScRSW4mYllRz6PxWxTUQZP7DtWVL2Vrn1vXtjXievtE2OPxhPn9taN2ThLdEdheZSjJCIiIiIiotqAyRSyZBfICxdMloMTIqTQoYm6ATgA/LLa3DResIv7qWaVtGmYBcB5YHlwR3UvDbfsDonqWNomUxSzOvQJKnVULCdDH6RXJVdEgOzbReY7z03j0Hq3WI811DNF/0HJY1Tfqa1fz/iWIimFVVXjM30O4hiM6t5U/RoHTcqNgbF9JZURJ1ROP7o5+rcLf2e2cPOoztrP4RqkG/sVxMKW/epA9drdh5TLZapAsAehRs1/7tfK+rXB/5vOAxdvzOq0vfPLJfhm0Xb8ukafSFu6zbrskh2777KclP5zf+v3C+i/A/4w27WapeQkV+X0Lvn7v1mGwjLrmYF2QskUZx+YKtn5+79H4dVL+uuW5WSkaokNcWyfnxI+MWwUrq+Jtp7hOiWzmn0n8glej0ebKSl/lqf0aOZgv9ZJHPH5VTlM/olZOE4nR5zQLZDAkkuTGd9+bmZacJxAidS7xemYwtH1Vgo++EfwWjiiSxPb1w7t1NjVvoxJVcBZPxrxVsW5rpt1F8upgURERERERFRrMJlCln5ZZZ3gCBeH+mzeFu1nEVSQA/vGQNaBUuuAnbEnSjj1goFap0G8WPXmsO2ZooiB2838Ecfsnd82akE4fek09euMjX0dN3+22J4IhtvV9hd3Iou7h1VjVA0jVLIsuC/D85F8LD8sMSfXxHvo1kxd9s3JblRJgfELt7kam+D1eNA8P8u254hcfuqm4ztpPxsbpPv9fnwwZxMWbz0IQN+c+6DNd8oNqwSO08bqxsC9xwPMuOMETL19pG2CVXwHzDNTHO02uK67k+j6D+Y737Y0ELuyTfIYws3K8ei+M378ts56xovVd9vRjCHFuXf60c1Ny8b9thGDH52MK96ei6d+WmV63lgmTTUOp9cgVVPy/Ow05fdEbLKiOhDwXrdHXzZvYPuGxpcotuHw3DBcp2ThZqakSI3U5XXSUuUZSOpx/Lhkp24fMjGLx+mx1WY02nZNCRH9duRm98Z9ZaZ5tc+mWCrF6bZUnhVdMiW4H3EtETPIrJIqxpsKwlHNTKl2cH0TszLF+OTPirkUIiIiIiKiwxOTKUkgAp9A7f2D2+/34xqbMhfh4lBbD0h3swffo3x3v5M+FbLyqvB3iQpiyy5jZVGz75kSWZmv3zcewO2fLQKgD147LZXk1PUjOiqX/7w8ENCbu8E6oVUeLMNmbGItj1EOJC7dVoiFWw5qAdHQXb369xRJySq7cj/RHDNV3mP7wejKthwjlSsSxBBHdg0FCVNTvNpyY2Jj6qrd+PdXS/GnYENrebbCfqlBudO78FWiea1xTEDg3M7PSkP7xvVsXxcq82VIIBm+S6ogvLZu7KueaeTP4tZPFynXqaiu0YLidskzQQ52L91aaNvwPdYzU8RMA6PSyhpMXbUHL0xdq1v+/IV9tRJWKunB5Oq2A85KcNl9jlbe/HUDAHMi9u8ndsbrl/Y394uSLNh80NE+7GamiPNrb3GlYXlwJohXPUMnTUo8q5KShaVVmBEsP6hKTliVwLMij8dKQW6odFZaivm9Gk83j8ejJS1KpH5KqsREJMql8p5aMiU4y6cqOBirfxc4+a7Jhncxz2Qpd9CPRvyOUiVTYnWTBhEREREREdUuTKYkWFlljRb4BKCrSV6bhLvrPFygoIkUmBEBBjmQ4zZQrirDYUXctbq72FmwO5JgsepuYrutqHZhdwzkgPFXC7bB5/PjmYmrtWV2sSJxV7EbxkSIYLzjW6UsGNDKMmxDPzMl8H6qa3w4/fkZOOvFmSgKlg8KzUBQ393tRq7hjmQnjeKt3rtMFUjNz1IHn51SbXPyrSNw3xndcceYbrrlopeGsafHhr36nkFWd1O7TV7G6rUqTnNa2nlhWG782nx701DLbWzYG/78jZSToPGTE1bhvm+WATCXilORvzMXvTHHdl1x/TB+b+TAthVVsNnJ5yISHteN6IgzerdAI5vvV06wMfnGffH7DOZtOgDAPPa0FC9O6t7U9vv/3uxN+HjuZuwL0wze7jq0JFgW7p9fLNYtF19Dj8ejfe57D4USLqL3B6A+j+Tfv3uKzeMTiWHnsy+1V1qus1vaj2oGi2pf2kwR6UtpleSLxsqdxQBCSR5xnSu3+A6qZl7Zuef07rjTcM0Vs57siDxXKJkSei4ex4GIiIiIiIiSj8mUBCsq15feibQefbyVV0aZTJGCWGImjq7Ml8sArZumtnd9tQRAYFaHE5HEPFTDNwY1rx3RAZcMaqt8DggFQ1UlSWau1Zf3qfH78caMDdpju1I+bW1K7wDAM+f1Ni37U+8WynVbB/vP2CmtDARvs9LseqYE/i/X099eGLhjXSvzZXhPkXwuzfL14+1cYF1GSpADm1ZUwbmjmqvLhoUj3pYqoN2+cT1cMaS9qUF9qhZENM7yCP3827q9KLW4Uzuau6Stqva0rB/+3FBxe9e4cezG75J8jnYwzHb5ZfUe7C6KT+NnJ70hvloQKgXnJFnhZvZUqPyffnmJg14PqsTOBgeJ02Z5mQCAlg3Cf/ZNg+umOfh+AZHNIhLnhvGzCF1T7F9/x5dL0O+hSfbjCv7fTQ8MkYBM8Xh0MxxE4l4uiahKpsjfEdV5Jp51esjs+q8IclN31Xqq5L8YpzwLzkl5rEiJ8mbimJRbnOuKiTW2cjPTcPWwDrplTm502VcSSECF+n5ZlzIlIiIiIiKiwwOTKQkWSemiZLArlwSED3zJd9au3hVoVC0HiIzNXcOV/HFbOsRN8sWu14kVVXDauOTOMUdhSLARrup4iXOhfnb4GQ7G88YusBcucH7OMeYm2O0sjv8Df1LPcpED2j8EyxgZ+96oSp7IsxzEj2K97HT9DJFIGrwb33usqqGpg4uRbath8PNWfYZWwXSt4bQhsyG/3e8X78ATE8x9LQDrhIgTVueTPPvMDafBdaueKcbH8l30r13aH6OOaqrrpbA+itkpduegk2uSHDR3VubLOVH+ydSvw8HvGGN/I8C+74sgZqY4Gac4P5yWaDS+j+cu7Bv2NeIVC7cc1C0X3xenPUKcjMtNDrAsmGDOTk/R3TAhSnfJVLMu5fNO9VlZlcCzYrzWqjx4Zuhar1rPLply0etz8OGcQEm6yhg1oFcRlw6/lkSLzcwU1WvCzSxZubNIm7kpl80TP3JmChERERER0eGJyZQEqyt1tA+WVdo+/+/xS22fl4McvVvlAwAyUkPB8oOl+u2HSzK5TabIfWnCieQjUb2kSjFGEVhRfe4isTDqqKYY2qkxrh7W3nJ/bpIEbmc79W5d3/I5cXe5HDgvr6rB8U9Nw98+WqBbd9qq3ZZjFB+vsYE6YB3gi6S8lDG4GLtkirOyN7K7xy9RLr/Goj+NHZGAMM5MkR/ZfYWiKvNlsWG3M0wAoEeLPEfNwQEoG3erHssfTaeCHLxxWX/djCRj+Tk7PVvmaT/f+OF8nPj0L5bJACcJW/ma56TMl5vzVSTIjMfDybgiOZ/lbTsZpzjuxsS5FePerWbLyXx+vzJ5pCq7ZLtv+4ZXANx9NmJWQ05GKgZ3bKQtf/KnVcH9hdZV/W6T35Lqs7JKNFoR78/uPTTLz7Tdp+r82CfNSBEzQuM5M8VY3szquubkuxatD2aH+hml6JIpgZ8f+G656TVbD5SalhEREREREVHdwmRKgtWRXErYcS4y3AlsJAeIHj67l/Zzi2DApqRCH2ALF8iz6pnSoYl6RsXYl2fZbs/NvlVUL1GVBQkFf8zri4RPeqoX7/91IP59WnftOWMJJZ8fGOAwCH1QmiEy6qiCsOvvKrQugyT63MgBy9/W7cXGfaX4dtF2XX8G0SNBe61iZoqqmbJVwDOS2RSmJskWd6bLMxecUAYXbbIXa3YV4/3Z6ubh4rN1kyAUd03X+PwYN3MDznxxJgpLqwxBYOvxRFXmK0YBy75t6uP7vw9D/ex0R+uLY375279j7ob92nInb6Vc6nfw65o9Ydc/t19gtlamlPz4fvEOrN9bgt/WmWcTAOH7SgFAplT6LtZlvsR3ye54yCUEJ94yXPu5uRQ4F5ycISKZ52TGh+hF5LTfVSSn6BlHt1BedzsXBErwOZ2hYLfv92dvAgDsL3GepBbvOTMtRRdoX7o90GNFvnSoziN5tqRqxpA2M8XhrEonpcp0n6nDMl+qa5gxmdetWeCzaKE459zyGn6fGpPLQiSJXrfkRI5XkUxRnVMimUZERERERER1F5MpCVZXkinRliMTAZXz+7dGz5b52nIRYDPe7R22bJhFQC4WIZNIAs2q19glClR3PjcP9vdQlVT7q2GWSo3Pj2PbNdAe2/X5kINZGRZ35Z/fv7X28yk9m1luSwTM95VUYndxIOny8rR12vPfLd6u/WysOS/Hs/zBIalKn1jF9276cL6jJsCycDN4BnUIJKTkkjZO4tdexeG2+4rc/MnCsNucu3F/2HW0/UvB0/u/XY5FWw7ipV/W6taxO42jqd9v9f1QHRM7bnpOAPrP5ZEfVliOR7VZeTbEUxNXh93XtgOB76AfwKTlu3DDB/O056yOq5NkmG5mSowDvKGeKfoByvuUkznyeLs2U/T7cXCKVLqYmSJKglW6/A47ceGANgACv09UsxOygiUDHc9MsXlO9KD5Yv5Wx+PTSoMZBhAqcRfao/I8kgakSqKLrTpNODv5HSd/pqrj5vQSYuzx0r1FYLbXZce1c7YBG2JYv6zeg417SyzLkaqSaI9IN3VY6ab6XhhsD+5TPqRyYtnuulgQYWlEIiIiIiIiqj2YTEkwY1AjN9PcfLw2iLYcmaibnmFoSi6C6X9sOqBbHrbMV03sA3I3j+oMAJYzCGROatPLQSRx57d2l6pym4H/95KSTYIxFOTz+ZESjNKc26+V7R3XQzs31n7+5+iuynVSpDr8mTZlkOQ75f/z7XL4fH78vjH02Y1fEEqm9G3dQPda9cwU85FYuKVQue/1e0swU9FjwE648+iDvw7ContPRptG2doyuVSNlTSvF3mZqUj1erSZJXals9yWWgtHdbdzcXm17rHdKRpNctRqAsYFx7ZxtR23uQT53JODpsZ3opol4bRPhyC+Mz6/H3999w+tD1BgHOrXOCmntWRb6NxOddgrximRMDJ+tKWVoZka14/sBAA4pUczHJJmkdVLN//ecXKG7CkOzJJw8lGKhK+TGTzORxCQkSoakfuUSWzB6UyfSPoz2RHfN+M5LxInoo8YEL7/lmoG2PIdRQCAovIqbNhbgge+XY5dRdYzDJ30TJHJ6/VvG7iun9yjqe1rhgW/Q8bPQ+w7mmTinWO6AdD/u2HkU9Ms1zces8+uG4yLBoa/Xn37t6Fh1/l3sJzZnuLQ8ZaT/nbHWJTNJCIiIiIiorqLyZQEMwZO3N6tnSiq2NJTf+7t+PXLgkFEYwBl8/5AzXBjuYuwZb6qLGamRHH85IDLPENyx8g4PNX6csmRn28dERxf4LEyYBZcpgqWGZMlPr9fW79eun0PiG7N8vDZdYPx1Q3HoW0jdRk0eZ9lleYyOdo4pGF8v3gHfli6Q/e8nFDIMoxL3zMlWONeUZblkKJMj3DARWkdIPwMpxSvB/nBBvDiLuG7goE6O16vBxNvGYHJt41Ag3qB19uds7GehRAqbxPaZ02NX1fmx67kTzTJFKv3eWaf8D0tZG6/q/LaIogPmAPfqmNd5rBPh9C1aW5w2+bnUi1uNXeSTBHbBYCiCBNs2Rbf91DJKP2gLxrYVvv5iuPa4bu/DcVzF/bVlQ5UJWPdJNCd/N4SifTyKp+jZIUcg+/QWH3dEkTD78oan+25LY+ye/M8/PiPYTihm7n04cZ9JcrXr9xZZDsOq8MQLoGwdrecTLF+PaBuQC8S97d+uggXvDYLb83cgIGPTLacgSYWO/0Kyuu9dcWx+PamoRgZpjSi+N1unJkizqtofleLl6pKnqkYj3vDes5KC6Y5SHiqZsPI1ye770ZdmZlMRERERERE1phMSTBjrMPv96OkohoTlu7U9Z9INmNgbdKtI7S+Ak5MXhloRv7Nwu1h1hT7C/z/8bHqUhwVFoHLSMMzORmpuoDiwVL72QnGuv9/eWOOaZ2qYDSwVYMsrVeBCOqoarvXaEEm8/6Mi2r8fldBqWPbNUTfNg0sn5eDTT0VM2NU6wHATR8u0D0Ws1q8nlBZH8Hj8UjJpMD/VXeRt2ucbTmOQy6/E3JQK5zv/j4Ur17SD5cObudo/Wb5mWjbqJ40S8Q6MpYa42SK8TgCgfPN6cwUp03AVVQB2ltP6uI6OOq2x4rxEP4RLItmfJ+qgHW5RfLVihjaQkUvKKsAq5NeIP2k0ny5mWmuxiRYfXZPTAgkpI2fj9wnyev1oGfLfKSnetG6YTY+vmaQrneKzFWg18FHKfdr2WHTl0mQf+fcc3p3mzVDn7nP57dNphivB0c1z1MOfdQz05WvP+XZX23H8fjYowGYy0OJMVl9R+TSXqrxy4nRcImrXUWh9/iLRX8gsT27y5K+zFfoQV5mGnq1yg/7fd+0TyRT9N8L8fZicUlMUySWVIzHzM2159d/Hu9q24B+xqVqVyIh6rTHDREREREREdVeTKYkmDEA6/MDT09cjeven4dnfg5f2z9RjAGeTgU5EW3HSQklIBRIy7MIOFr2TIkwQHOoolprAA+EvyP1+SlrTMuMpZxEwkTelgiiqAKioSCT+U0YA1d+v/36bskBaLsgb7h91QSTI6pSZfLrxXmvChwe1zFQHua8/q1x/xn6IKrbZMrDUm8NwP68LcjNxOgezRw3qRbEZ2M3McH4Lod2aqxczynlzBSfX5dcsQvTffLHloj3bSxn1rNlHq4f2dH1dtyetsY73LccCARr5ff82XWDla8td9mnw+48t5pdII+vvcVMCrm3UZMo+iXcdWo3DFT0zgD0x6NnyzzboPegDo3Qpam6L4SbQK+TRFK2VEqs1Gb2myC/D2N5SCPxna3x2c+6khO8bvq9ONUkJ/CZGmePiO+pVRBfTraqZgTJb8nNLLd10owXmdiF3bkhl8uL5hgZkynr9wTG5Pb3Vpai/GTngvA9TV7+yzEw/jp3cwzDzWIR/6aRzzv5u616n2N6NgfgvO8MERERERER1V5MpiSY8Y/pGp8fb83cAAB4c8aGJIxILV5/9F8xpB0AoEW+vna4CChZ9RWwCt6p+iU4Jd+FbpxVYfSS1HRdMJYcEbMu5MCNKBGkCvhpTYoVb8EYkJmzYb92B3os7vCVg3npqTYBtjD7mrl2HwBg0VZ13xOxm9DMFPNxEMcrxevB5UPa654rKo+898jlx7XDbSere8ZEI0V7TzZfEsNT7101AH8/sTNeuKivtuyTawY53qfWgN5mn3bD+X2D82b3RsYSe0//uY+jcjhGbl9jnKknGquLoP+gDg1xbDt1gsGuzJeyFJLNeW51zOWyXRv2qstEyZyUBVO55/TuuGZ4R3xyrTpxFG1/K207LoaXk2FfalAQgWknb10+zuGu617pO6i6pggjpNJUHsVP0dJmjBnen921XX4+8FrrEpCAu0RAhsXvMb/NLEhBfs7p79UP/jrQtMw4C3PZ9iLdGJx66/JjTcuuGxE+ids4N8OUIHeTxwmX9NkX/L0vn3cnHRXqJ6P+fR74v9vyg0RERERERFT7MJmSYMa7f+2aWCdTrBryPhEsgyIcEyw9Jffy8Pv9Wi8NVX14wHpmSjRlM+SgSSRlmYxBI5EwkbdlVxJK65mi2LcxnvPNwu2hu51jkE3xKhI+KtHuy2OYUaFKKtmVYIm0zwQA3P+nHrpSQ7EiPtNNFn0WAHPiwOPx4NaTuuD0o0N9RgZ2aISNj52Gdo2yHe9TPnwewHHPlLEuSvQZlUnN3M/u2zLiWWq5me4+i/WGBIUIFDuZoZWhuKtduPKd303L7M5yVay+vKoGf/tIX/Ju3ExzMlwua2d1DQvn4kH2jbPlwHy2orG8U1bXXpX8LGcly7zaLC4HPVPkZEqYoaRI1xWrPiGB7ZivxU4D68bAt+p3hNi+cQThzlF5yKp/A8iL6oX5TOX+Mq0aqK8lWs8UmzNdX+bLdpeaIZ0a635HbNhbos0AMuphU05SZXDHRtrP4ngY+3KpeD0e0++UaH6PGcubju7RDID+fPUqfufLxKL/TV6ja1ZPREREREREdQ+TKQlmvIM10gBbvEU7M0UEXFs31Ad3RGmsWesDMxrKq2rQ/s4fUBy8Cz3NIrBvFYCIZpwdm4SCwpk2wVcrxgBhjTZzxFwuRTVOu7r6xthPVnqK1EQ4+mSKHBi0mzEQbUkx+Q5yQD0zxSaXg6kr1T0AkmldsGxNToZ1QHmARUkmlVcu6YfOBTl45eJjLNcJ3QEvB5w9jnumuO1XIhPJlOO7NsF/z+/jOjD56Dm90K1ZLv592lERjwGQZqY4uMv+yXOPtnxu2irzOWXbNFqRpFq9q9i07P5vl5uWydcIpzNT2hiumeJ9W45PGt6DZ/Z0tA+V964ciK5Ncx0llp1eg8SlxcnsGWOi0I4IXr87axMe+M583FXEkJ323VgklYEEgIfPNh9bsSVjslz7XWBxLPUzU+zHcfWwDqZlx0mJBjnpaJVQ9WvJHft9CW6u+29LM0jem7VJ2R8M0Cd94snrMSdP3LyfrPQUNJJKfV06uK3ueavZSKHnzfuST4+Ne0sdj4WIiIiIiIhqHyZTEkwEUcLV5U62aGfMiKCvMajx9cJtuse/rNYHNls3zFJuzypAE2mJmwHtG+KWkzo73s7lx7ULu2/xMEVxl6q87gtT1uDC12ZrTbKVd7IawolDOzUKWzrGDXmfdv0Jos3bhGbmBB7XKCJQdjNjWtTPtHwuWQZ1CAQy7b4jbhIO3Zrl4edbR+CUYF19FauZKTK72WTRfJ/Lg8kUJ3eFq1w4oA0m3DwczfPV322nQqXOxGPrY2zVF8SK3XmuOnThygIK8ul+oNRZ/6grg6UQnRLXhbQUD7o2c/e+ZW0aZeOnW4Zj2v+NDDvzxGlwWnX9s+LmWi7v/9c1ewEEZj61yM/EGb1bWL0MgPNyc83y9NceeTalcRzGoYe7Vsvrq963WJSdnoL8bPNnkW3xXbQ6hD4HCchIe6bI/aB6tcrDlJW7letFMmvqgmNbo1G9dJzXv7XlOm9e1l/3OMXrMSU0VD3L7My84wTtZ+NNE+I7bXW+qj5zufdXpOX+iIiIiIiIqHZgMiXBxN/fGaleXXPi2ibaMl8ieGt8ixccqy9ZY2zKbBWwtSoTY1fixc6Yns3Qr21DLWAWbjONFMkv65kpoWVe7c7s0LKnJq7GrPX7tESSMuBmWJae6pV6psR2ZkqDbOvEXjQzGgBzMFWVFLObmRLpzKO2DkpnRUprfm0TFItVmTxtn6pycR79Y7s9OimzZEUkUzLDzJCItxs/nI/JK3bFrEeIzO47pdqf02u3/NryKmdB1K7N8hytF9pH4P+xmLEGBEpFzb/nJGx87DTkWZRmczvDwcn5Jx/m7i3sj4EqWZmXmYZf/3UCnrugj/I1xeXVujFZ2XawDDd+MF/XU8vqdWKRcUaIKrEuPD95je68UCU6D5YFEm9WSQCrZF64r4bdexfXzPQUr6tzyev14Jg29QEEmsY3z1cnwDNtkvZWHht7NOb+exQa2Nx8kpORivWPnKo99sBc5stqTFYy01K077hxppj4rBdb9AkzJuv+cWJn/Lh0p/bYrscPERERERER1X61N5p/mArdsepRlhtZHywhlGzRBixVJa8A4OhWgbrp9YN32xob5loF7y2TKTbD/PWfx1uWTjquY+Bu2rRg8/WzXpxp20RaWabLWNpFfLYOe6Zo66gik8Y7nX1SHf6Y90yx3p7TxM3dFiWcRB8GUc7OSc8U+WGk5+Hdp3WP6HVOiPFaTJYKPhfbgJmqXJwHHt1ju11G831eui3QQDozwpkpsXKgtApXvfOHo5kpbtltSXXonM5ukM8Dp8MtLrfuE3TRQHP/FG0WYAyPh0gCDO3cWP284zJf5hlVVlbsCJxn/zylK3Iz3c+MSfF6lLMShG0HywCEL8F3yycL8f2SHbj5k4WG7ZvXDZX50i+fsTYwW0aMZZh0HJ/+ebUuoK66IeCd3zbZjtEqmWd1mLWZKTbbzExLwdL/jMbi+0+23beKmF3o8wMVwWu9MZEUabIv3Cy/1BSv7vdZm4bZps8qkjKei+8/GUvuP9l0g4f4rOVeUjK5N9Rdp3bDLSd1MbyeyRQiIiIiIqK6jMmUBBN/Rns8gSCA0W2fLUrsgCwcLA3f+NtuVohVmS9jQMUYV7AKuFjdzZmXZV06JDMtBce0bWBaPvGW4Vo5HDkod/xT0yy3papFb3z/WkN5XdPj4Lp2yRQHQSaf3x/TMl+x7pli1fBbNIA/GOyVc1DRUN54jiy672SMPSbQ9DfSJMAiw13lsSSOnapkmRDrm4+1O/zlpsce/ffHLy03imZmiggaFik+u2RwUrLI6NM/tmDlziLL5+0CvU7PQVVyPJLTt7NNibJ7TzcnCWN5XTCyan7uNDDu5Pq39UAp3psdSh44mfWjeq9Oer0AsC0ZBQBb9qt7Wqjes1i2o7Ac42Zu0HqCCb8GZx92sym/pvpqhiuzNqxzE4tt2fdMCfe55WSkRpR4CM3A9GNz8PjJpUxb1o+uxJ8d8bnPuetETLt9JPKz0ww3NES23cy0FORmpplKbob7SsvJFOPMWyD6G1WIiIiIiIgouZhMSTA5EKgKvi3YfDDBI1Kzm6UhGO++r6rxYeqq3Sgqr5LKfBkbwQb+LxIRpmSKxb6sAhAi2HditwK8ffmxurI0KV4PCnLN5T3kfgpOkwV2DeRDj4PblN6zCF7ZxbLVE1PMZWO0mSkxuAN9nTQDKtWmIbNdCS5ZhkUAVMxIWRm86/zfXy0xrWMM8OVlpuFPfQJ9D8I1Z7ayr8RZf4pIiM936so9lncZR1p+zoo4RJNX7NKWpaZ4deeJz+I7B0SXTBHB7aOauys/FS+hpJHz78E/P1+MU579VTsfhXeuHIBJtw533TNFtUwVqJavW05H296mUbcq0B3L64KR3JvjfCkJ4bjMl5Z4tD7/TntuBu4Zv1RrpO6k340que60T1GK14N2hjKAYsYkYH0c7cp8Haqoxv3fLsdLU9fqnt9RWA4AGNPLuh9StaJcYI9gmTOrJMSZfdR9Yazi9CKJHYdTBEDo2MwMzsgB9EmFP1mMNxYK8gIJi6Z5mWgX/O7ob2iI7k0bXx4uFyLPqlLtmy1TiIiIiIiI6jYmUxLMr91F7IHz8FriOWk2bQyQvTBlLa54+3dc8uZc7TlzMiXwuKi8GjU+vylpINeCT0/14rRgEMqqAb0IbJzbrxWO71agaz4sdj3p1uGW78EY69hdVG6xn8CO/iRt3xjPq1HOTNH3DFmiqLO+ZJt5maqhcahninKIrmzaF7r72q5kj/OZKepLye7iCgDAPV8vAxAo1SSzahbt5I52O07vUo+EOF6z1u/D3A37levE+u5j8Tn8vvGAtiw9xaM7T0QiMVxQ3y3x2rwwd8sniujhEslH/PTPq3SPR3Rpgk4Fua57pqhmqqlfG/o5Vj1NzPtwP1PHqSxpZor8HXdaajDFcP1TMc7mEMFxO6t3FZv35eKEqKi2jmhbbUe12Ljo1enrdY/bNQ4kbeyusXJzckH8/uxqMaNFNas1QH2cN+wNJM+NsyxiRSSNPpq7RVvWs0UoQXXJoLZx2S8ANM83J5xSvObfwbGyelcxfD4/OhfkAACObaef/SonkWJ9LSYiIiIiIqLkYzIlweJ5F3EsVThollxmaI775YKtAAIllmos6vjL7/uOLxabEhL1MlJx/xndcc/p3bH6oTFolBMoFWJVUskukCgCGZ0KcjHv7lG4eFAbfPe3oZbjAYDrP5hvu5+G9dLRNXjntDEooiU7pG+ViOn4/cCqncU444UZpm3P23TAtMwYbvH55fca/bkj3/1udy46PU/dBDJlVndeOwnCGq3dHQqw2s22iVaVdGvxesMMrspqH8qramJ+97E4vKKvBBAozyYfHfEVUZf5Um+3pKI6bA3/eJaRisTs9fuCP7kf0IdzNiuXx2Jmioo8Qyleh09L0MfhA2rZIPT9lMtvOb0uaMlkF9+H5nnhS0IZEzBuxgSEgv+CaE4P2CVTzMvDHfMLBwR63NgNTZVMqba4GSEcq/MyKzijSQ70x5JqJqucCLIqFxct0XvNSP6sYv1Prc37S/Hv8UuwZncgQTWmp37WkShtCaiTaEymEBERERER1W1MpiSYCK55PPEruRELlQ6iwaWGBqzyXa9aA3pTz5TQz5/N26oL5IrZBJcPaY+rhrYHEAomWTX0DvWgUQS6pEWNcjLw0Fm90LNlvm6dtbsP6R6rEhuAPglmVbrGp5t1BN3PPr8fk6QSTTI5+GIl0DMl8HOkiQvZv4MN4xvnpCPfIiAFOD9HrYJl4q7d47uqa/xbvRUn5dGMflkdKjETy2bcRnLw8/EJK7WffT4/TnvuVwx/YipKK80B0miozu8Ur0f3/VGdf8bnZPM2HUCP+37Cv8cvtd13bUsA/7BkJ4DYJnfstqWemaKgWKgr8xWnwxfPz+fcY1ph1FEF+M+feiAtVU6mOHu9N8z1W0WenWjlobN6mpYt32HdEwcAzrIpNbVhbwlmrAlcP6zem2q2pnFV0dy+QfCaKvpR2X02VYrftdXB7JOqFKjwvwv6mJZZXS/LgzdHtG9iXUIuGqpxZsjnS5z+pWl1dORyePH4XsgzcA6W6ktKyuevagYbcylERERERER1G5MpCRZJvf9kmGwR+JcZ+0LIQSgt8G+cmWKIVMmbGNKpsWkf4vVWZb602RqK52KRdDDux+sBxA3axgChqrSZePs+v99yBsB1IzqalhlX9UMuEed29Ga5mWnY+Nhp+OPuk2zXc3qaWiVkBndoBABo0zBQ7uZ8Q/Nn6/4Egf+76T0i9gGoG//GSomUKDlYWoVP/wgE1l6YuhZrdh/C7uIKXRmi1y/tH/U+VZ95ZlqKrnfEZ/MCM8OMfUEAdc+KZyetBmA9W0OI5XkXS5FcQuUZCIatWb5GdQqGm80Tem3sI6dXD2uv3Ec8Pp+s9BS8cdmxuOy4dlpiwI0UqTG5U3YJBKFVg2xsfOw0V2MZ2bXA9vkHvguUIky1iPyrZnUYz0HROD70ez7wf7u3r7pvQfy+S7HJQvSQymgJVuXntJsb4vTvDtV2M6SERix/Fzsh9/opM9z0EWvGIy5fYw4prjfR9K8iIiIiIiKi5GMyJcHkwJfb8MK8TQdw1oszsWCzevZELLVqkK1c/vcTO2s//+mFGdi0L1TeQ7573rpninGLocDCXaceZdpfTjCANXPdXtNzQChIJYI5cmP1WAaOtP14PaESVBYzU9Q9U6zvGm5YL928P1MDer/tzIN4SU/xonerfHQqyEGuzQwaqwCRSJ6JoH/T/Ezd81ZvRZw3boKwiZgFAJgDZP/8fDEA4JmfV2vL1u0JfC/uPb07TureNOp9qj5zv9+vbF6tas4dXc+UwP/j1fPDytuXH4tMi148QGy/B/abMh+7aav2KNZSHfcoBmVBNDMXyUNRQiven4+cpNtl0VvKSHxGTpNPgF0/kOhkhJnxImYUWJXuyss0J4ytjnloBmrg+d3F1sdLnt0miOupXe8nVYLC6jDHu1Sfapzy8U50MsVJzzenMlK96NXSnLgSjL/75GOhmpFl16+HiIiIiIiIaj8mUxLMGPx349xXfsPCLQfx51dmxXhUZlbB11tP6qIFOA+UVuGpiaEAsvyOxN2gVg3ohV1FgQblrRtmKZvtigDWsu3qEi4iSCc2O3t9qCF4LJuQy+XZrMp81SiCmnIwceqq3VHtvyYJQW2Px4OvbhiCn24ejmJFbX+hm1WjZGNSxHBeWb2XSMp8yeVy4tVoGYDtcTCKVRBRXboLqLKYsWVaN4qofjKSeABwfLcCLL1/tOXzsRzOjoPWwW7VoXvgu+WmZarPQk4ixOqclEsHyv+Pd7xaThY5DQhr3+M4xY9/+b+R2s9WpQSbBxO4A4Oz5KyIz8cql6NKxhgPuXhs/D2vKuUlKzFcU5z0TFE9ZfUtF+OJV/lDVQJMLv0Yr/1a/v6I8T6+vnEIXr2kn/J5Y8JETjqqZgkeqjD3+yEiIiIiIqK6g8mUBIvm7nnxUtWd57EmdvF/o7ti9UNjdM/JjYjlniOq9yPKnlitI95TmkU5kz5t6gOw7slh1y8glnc4a71Z4LFsjq7NTJF2K4ZVVePHgs0HldtWHTdjLktuQB/H3upKXq/HNqg3tFNj5Cru2gZCZWpE2Rrjmbtlf6nF69QJKztywHJwR/vAaTSMgU87sQpw7zlUYVr2wtS1jseiukPa6WSBUIkgZ+vHkt13OJZJxe4t8iyfczOr59c1+hkruvM3RsM1lo5KVLJLPgx2d+rLtHJ9cWoUIZcem6qYLQQAU28fiYX3nqScAahiVVpLdb4Zl3kM71c8qyrJJTtYpg+w1zjomWI1W01FG0+czhHV7we5OXy8ZqZYbbVjQU5M9+P1eiyvl6OO0s883C/1UOnTur5p/RO6RT9TkYiIiIiIiJKHyZQEc9Is2E1JlHgRY+jePM/UEFgug7Jhr5xMMb+neobSUMY7VLUAuMXhaJGfBQAot6h7Lu6WjvdN8/Ld36GZKaHny6tq8P7sTQD0gSPVnalGTu5Yl3uuWJWhSZbRPZtZPqf1l/Hp76IXrM71UMDYRTKlOrBu8/xM9HQY7I2Eqg6+lVh9Vhv2liiXiz4p4azaeSj8ShaimU0XT7EcT/vG9dDZIgDrJnd9yZtzLV/r5FrghLhebDtYpttHvD8feXZT+8bOGpl7I5hh5obcx2SMxXUoMy0F9bOdJVIAdbJ6zl0nKtfdbJEMDvVMCWysdUN12Uzh/m+W6R47mZniJAkv1CRo9pJQLz0FnaTvU7ySOFY9kDo2iW0yBVD/G6RzQQ76t22gWyYnUPu2CTzXQEosGW8wISIiIiIiorqFyZQE04LyXusgei3IpUh3spqfk4N25VU+aXn47RoDfiLAaBUIzAo2sa32+ZWlUoz9Ai4cEGhw3rphVvjBuCAHlMX7lO/2f2LCKqzcGWg6LgdNw9XpBwLnglGBoYH6tFV7UFkd37uLwzmvfytXy4HQXd77SgJ36+4vqVI+bxRJELYyeH70blXf+YsiMLhjY8fr1pYExKQVuyJ+ra+WJvHCjaZDE2cBf+Go5urZKdEktyOdkZFuMyNHJFGM+7DpVR4Tukk2Ds/rSJKibuRmpuHVS/rhooFt8NSfe0e1LfGWVN/ZpnmZpmUA0DhHn6QxlmBz+vVftOWg7rGYyZdq86EqZ6ZYFPqSe37Fw/6SSt3jB87siaZ5mfjw6oEYf+OQuOwTCF3zVez6LUW0L0UytFfLfNN3QTUb5bGxR8d0LERERERERJQ8TKYkmoO7iONVEsUNkaRQjVOOx8jJAiczLIzbq6iuCb5WLTM9tP2tB8pMz4fuAA78//9Gd8Mto7rg/asGhh2LG/LMFBHgkkuc/bh0h/bzpBWh3ihNDEkRFdVxO6FbAf55SlftjtYZa/dqAfFkxbQbWJTJsQv8igbp4g7uj+Zu1j1vda6L80RVnsqKSLalOUhgReOJc49GR4eB+nj1CghnUIeGMdtWonpyuBVuPKqgZiTcXo7FNS2S1wp2SVg5MVFaWY1pKwPXm7jPTIngzUTS+8it0T2a4ZGze5lmQbpVWqnu8yV6rqgMNvRhCZX50j8Ox3h8nPVMUZX5stp+YvseZQQTGcd1bByz76Fbse6bJZctE75csM207IaRnfDPU7ri51uGa8usypQSERERERFR3cNkSoKJEhAeWAdaEtASJSz74Iu5wToA7CyybuIspBhqqLwwda3NfoCM1BTtZ+Pdr4DUgD44pob10vGPUZ3RtpG7u9LDkWvOizten5u8BguDdxTvKFS/d4/HEzagpno+NcWLG0Z2wolHmeurJytAb/UZ2d2lLnpetKivDkhalvny2j+voiVT4txUpmG9dFw0sK2jdZM1MaVP6wbhV3IoUWWk3Ao3O+Km4zu52t43i7Yrl4vvflmlutSgkVx6SE5AGHsr2MlIS7F87qTuoe10v/cnPDclcA2N96cTyewStz1TnCYpo9W1aa5pmUiOGxMYd556lOV2LM9Bl9+ZyuoaFJWHZu2JJHSq2wb0YZMpjoYTtVgnMmQ//mOYszHEeAhOv79Z6Sm4YWQndJbOsVjPkiEiIiIiIqLk4V94CVYarLudlW4dLKsNM1NCZa3Mz8nL5J8LDU10VXIMdw/vKgo01rYLfIheBvId307GGUtyQFn+eBZuPmBa98Gzeuoehwuo2T2rei5ZQW15r8c5bPAuyuBYB/nUyyMq8xUsw2I3UyZWpq9WN7s2ilfj5XCc7NaqJJBpvSTPTHntkn7K5eG+Bg1c9Mmw4/cD8zbtx1H3TsAjP6wIu76cdJH7Jzx5rvNSP+k2CcFUi/M7K853v2/cp+4PYsdY9iqcUd0T05xb7rUim7hsp+k720AxI8GKSCK4nQlSVF6No++fiC3BGXxvzNgAwLonS3BnJlbH2W6maTzEczdyOb5mFuXXgNgnF71ej2nG65l9Wjh6bb+2DXBO35a49aQuMR4VERERERERJRqTKQlWHLz7NDczLe53EkeqstqHVbsC/T9Ud97KARm7u8NjFThYE7xjeOrK3abn5Bkj8SQHlOdLCZRsRWmZP/fT9xCpCZMRsBu6KviVrAkC8n7dJgmsYqlWR0Yr8+Uim1IZ7DOQloBkSrXPWTPxpCW+YrjbUMmi5LwXq14mxj4TRm7LvQ3rrO6F88em/Rj78iwAwGvT1wMIlXdSfQ/kRtVzNuwHADx/YV/LMnkqxhl8TlglCGLlF4cJRJmYTfjspDWO1k/U9+WlvxyjXH7Ne/NMn2m4Mf3jxM6hB4aZOG4TkPd+vVT3eOJyd72OrK6W8S7z9dHVg3SPE3WlGGyT1K+qif1NKUMN14geLdTXJiOPx4Nnzu+Dv8vnChEREREREdVJTKYkmCgBk5uRahmcTPbMFPmu+3AzU+xiM4M6qAMdlw02l0hyEuR5/dcNpmVuG/1GSm7g210KoKh6G2TalOhRsQtSq/oPJytALzfgdToG8d788CvLBFmW+XJZHgiQy3zF/7J2+8ldHa0Xq4/q7L4tXa1fUhG+JNXMtfscbSvR/RaMrHa7bk+J7evclnvr31bdZ+ajuVtMy+plBL7j/zfafB6ogrgrdhS5GstT5/ZGiteDu09Tl5jq0NhcDivL5XXHrXaNsl2/ZuXOQFJ+w177z0pI1BlWYDGjYUSXJpi2Sp80Cnfa33RCqJxcdvAz0HLALt/Qse315+ANIztartuonrkf1z8/X6xdB2XadzhOl0a7pEY8XD2sPdJTvbbJCbvm9LHSpqH77wQRERERERHVbUymJNghkUyxuYs4Xj1TSiurcftnizBlpf3drtXSAFR3XsvBf9sSVRZP/ufMnqZlkcZpQw3oE9N82eMBstJSdcsPVVRbvcwR+2NofjZZpaPku6SdjkGs5fe7C26J7bvJK1ZViwb08T8+VrMljJL1We0oLIvZtkQSLTXOvWisRDojJj3Fi5b1sxyv7+ajEjOmVD0tVOXTVAFuOwM7NMLKB0/BX4d1UK+gGGu8zzW5f1W8JLstT9M8c4IiXI+qtBQvbpNmYcozk9z+XhJlMFs3DJy3J3QrsFzX6vP+eO5m07JE9z2K927+fVp3LPvPaLRXJBUTKVmz9YiIiIiIiCh5mExJsNJK0TPFLpliH0GOtJnpK9PW4fN5W3HluD/CrBnav7LMl7R78bwcQNLWs4kziDu6GwVL30Qa5PFrJYgierljWqkj6BvK+3zA85NDJWwuVcy6Ccd2ZoriqWTFb7ZI9fudz0wJ/N8PYE9xhen5WJb5mrF2LwD7ps2x4rQvSzyCl33b1A+7zk/L3JUHsiOuWfXi3JPDSqQfp8fjwU+3DHe+Hxc7EqelKqCtunx/tUDd3N6O3QwrdS8l17tw5fxjWwMABrRTz+CJhXg2Lrciz+j59I+tpuednBeipJzPD/ywZEfotS6//+J3QTQ9Th6fsEr3ePrqPdq1N3Gzy+K/n3AzEJvkmhNjsZas2XpERERERESUPEymJJiTWurh7sZ3ele80baD5Y7Wk+PX6jJfHtPzn/xuLoVjF1AR5bHELJiIZ6YkqDm21edW4/dj64HQLIBsRcA53N2z9g3o7XvWJJJ8XjgdgjZ+v0VixOJcF9t3WuZr8opdWkmh7xfvCLN29JwG3uPxWTUK03tj7l0n4rgYlt0Rn0GyZtlEcwxzFD2NrLjZjdtjclKMG6uH62UVD5cf1w6fXTcY4648Nm77SMYp9vcTO1v2ywGcjUms88vq3bqSbq7fTvC8iuY7d6iiGs9NXqP1Z7v0rbnac6I8XbzVhhzD6Uc3BwB0a5Ybt30koKIkERERERER1TL8UzBJPB6golpd+sWqj4TRtoNlmLdpfyyHFdx/6GdVgE7VgH7ptkLFetb7MM48iDT2EorPxyZ606kgR7ncb1Emxe/3Y19JaMZFYVml6bW7iuyTWHaBJ1XJoGQlU4Z2CgUcqx2WLQrNTPFrn1WuFOBWvT/AXZmvLftLcdU7odlWm6QZNMkWj+Cw1XVDKMjLxBuX9ceJNiWC3Ii0mXasJOp0l79X4cqDWV0P5Ofk6/htJ3cxrRcN9cyU+B4or9eDY9s1VCaMY+WQg14/sdaxST2M7Gr9XXFSykkkjfceqtT19nIz2wkI3VwQbS+wZ35ejbu+Wmpa3jzfedm7aMzffCAh+7Hzr1O64ek/98YHfx0Yt32wzBcREREREdGRh8mUJNp7yFz2CHDeM+WC12Zh7MuzHDc3tgpc262nCtDJ9f/tYkV2gQYRLK8O1jOJNCgRbaB3ws3DkOr14MbjA41+rUqoycEtj245MHt9KKFVWFZleq0ok2TFLgj6wRxz/fuK6sQHHAHg2Qv6aD877X8iPhe/NDNFfrtWyRIt2eYgmyLPDEqWTfvUDbbjMZujtYOmx9npqXjz8tjMIIim5FAsWO339hgnKOSPqmuzXJzaqxkA4M/9WpnWddIzJVxSOhqqzW2uRUnESOUkaOYEAHxyzSDcfdpROKl7U+wvUf8uBpx9djstEub5WWmuxlSjJVOc79uKXG5MiOfssj/1bqH9XF0Tp8ZvLmSmpWBsv1ZolBPbcl/Xj+yo/cwyX0REREREREceJlMSTNytbFcb3unMlC37A0Hk3zfGdnaKvHtVrOBgqZww8FiuZxdmEDGd8iqRTHE3RsHuDnEnujXLw9pHTsWA9oGySFY5AqvglrEMVazvVFWdCpVhZibES2MpKOV8DMEZJgid13JAz+pUd1Pmy5gkTEZ4a8ST05TLYxVsk68JNx3fSffcyK5NLF9XYNM3wGnfl1ApvdqVTLlmeEfl8kjJ1+THxvZCpyaBWWrZ6eYAvwh6q2YeaDNTpGWxjmGrjsna3Ydiu5MYuOvUbgCAvEz1bBaf4c6BgrzMuI9JGNihEf46rAM8Hg9W77I+dk4+uy0OElmfXDMI5/dvbbtOVTAJIY5LNN85N/2mYuHoVvnaz1cObZ/QfSdSjxahMqvJmq1HREREREREycNkSoKJ8IZdjMRtDMRx0CSC1VTBnD6t60vPB/7vtreHMQhZpJjR4YQ/ynIoQopHlJVSHyR5Bsy2g6GZEMZgoGoYjXPse1zYjf3dKweYlo3p2dx2e4ngNGmklfny+5UJKcsyX9rn4Ty5KGSkJu7u9nDclvlxokX9LAzvEkqgXDigjWVi5Mk/97bcTlqKs7GJzy1ZN2GrDmF+VhrSUyP79dWxST0c264B3r9KX/5Hfn8FuZnaOV6tuL7ODSawM9PM55pYW04EJqKxeobFrLpk6tc20Ky+gUWfn72GGSFOz8lYU80wEpwkNJrnh08CDezQCI+fe7TtOjXBaWChnin22/zfBX0wtFNjXDu8Q9j9x5ucJHfTq6iukd9bCmemEBERERERHXFqX/TlCGH3J7gqePzZH1uk5yPbp9OXyfv3Ks6QxVsPhp732MxMsXmTxiDEuj3qMknhiJFGe9d8qOyYxVESiQCvB8u2h8qqGVdXjeOfo7vZ7tsuMSEHzYX8bHelY2Lp0sFt0bFJPYzu0czR+uKd+SGX+Qo/M0U+jmFzhYbnHzyrp6OxJYLT3jLROL5rAb668Ti0apCFZ87TJ0/aBEuC5SqCm04TPb4kz0xRnSORBt1fu6QfJt82Ep9ddxyGGpqOG9+feHyg1NwHScxWadcoGxmGpI64furGHeNDt3JnsWmZXUIgWcSQrBL+pYYeKSmqXzgJoPqMBSenfawmgTw1cbVue+GS1mf2aYn3/zoQjcIk7BNB/v7UxnMxVhpKiUH2TCEiIiIiIjryMJmSYE4SIarAzP99vjj2g3FAFUCV78beWVSO92ZtdJ1MiVVg1kkZKCdSg8FZ40wT436MwRNzmS/za49t39B233UpHPPAmT0x+baROPeYVshM8yqTPTJRtmbB5oPKu62tPj050G/3GT8zcRUuemOOblmHJvVsx5RI8SrJJic801O96NEiHzP+dQLOOUbf30MkLVW9Z5yed8memaJKcKZGGHQ/2SYJaOwnIc7TMkXPI3FONqyXjjcv0/em8Wv/l2amJODYWSaCk0gcU6uvsHFxsoLw3ZrlWT6nmn1k5LQfGQD0b9vA9vmqGp/rBKZqvd7SDNJEkD+6ePZmSbYG2XIyJYkDISIiIiIioqRgMiXBtKCLzV/hTgIzbksfuXmNHLxWBWnqG2ZG3PP1MqhCs3albWJV/ijanimCeL1VQNKq0b1xdVXgNdxbrYtNbPOz07D4vtF45wr7JuefzwvNqHITIJSPmV0Zu+emrFW8tvYcz1gNJdIwucg5qI6h07uqk90zRdXEOzUO5aCM331xfIyH7uO5m20bhGs9U+TeUzEbpbXa0PTbyKsdQ/vrqlBeZb5+JoJdibSG2eFnfYjeX05cE6YkV0W1T+qZ4mybqu9yzxbWCaJ4kH+nH84zUwryQn2oDtrMaCIiIiIiIqLDE5MpCSZiR3ahBic9UHYXh2rNfzhnc5Sj0iurDAWG9peYgwWqXgWuy3zF6MwTxyraO2HF662OvXZ3Pjy6sj7GmSwDOzQyvVYVcJXLFIWLUV8lNfMdrNh+sqSnesMG5IvLq7WflT1THJT5cps3rE1hvEhnUMSKOI4VihkyTnMjVonERElP9WLRvSdj0X0nY1CHwCyvvwxsG/P9DGinn0Emjt3SbYW65Xd8uSRMg/DIE1fRqPbFv6ScW+JtWyVTIrkpIB7sgv95imSe0QHF70krJ/dohhn/Ol53XZdVVNXYJutUisvNPccSnfyU93c4z0yRe3KprqtERERERER0eDt8u4TWcnZxjr2HKtGqQbZuWZuG2di8vxRAIFR32nMztOfW7D6EtbuL0akg13afTsNWhypCgRlVgM7ubuxw6zl5zo3S4EwQ0cMgUuFiP34poHxU8zws3HIQgLl8kqo3hfGtPnNeb/j9wG2fLQo8H2Zs/zqlG47vWoBqnw/929mXDKtt+rdriD82HQAA/LJqDwBjHx6LBvQOynz9uGSHcnltmpmSEqeG2k7Lh8nHcf2eQ+jQJEd7fLDUHICVbdxbgkVbDyp73SSa6BP05mXHYvHWQgwIUzovEr1a5eOrG45DywZZAELXhH2KQLmWYPKav9/idJVnqSUitlxVi2em7CqqQGFplanfkzF3naxSZXaNxJ0kBgZ2aIjJK3c73l+rBtmWs6vKq6UyXw5PnDW7DpmWvTd7E87o3QJZaSkoq6rBVzcc53h8kZCP4ZHSS6TCxYwkIiIiIiIiOjxwZkqCOQkVqUpFGYOnew9V6B7vL7EPjLqx/WC59rPqznpVmOSjuebZMfHumeLz+VEWLAuTFWUyRZQksy5HE/i/1+PRfYbGz6VtI30STLxGNqxzE30d/jCHIj3Vi6GdG2Nk1wLkKJI1tdnZfVtqP/93UqC5clFZaLaK1Y3p8iGbtW6f6fntB8tw/Qfzw7422eyCtNGYs2G/o/Uypbuoy1yWUBr51DT84+OFKArOLqoNN5vXy0jF4I6N4nbne982DVCQmwnAPoguXw+a5GbonhOntDjfAfuSh5G4bHDsZ+bEg3wMH5uw0vS88fvfUUr2JZTie/r6pf3x7U1DHb388uPa46k/98aZfVo436XFORGYmeJuNphVadDzXp2lbasgL9Px2CJRm667idIsP77HlIiIiIiIiGofJlMSLFTmKxR5MJbNUpXsCBcIddJDwGlFFbmBd70MRZLCcVNc6+diEQyVj0m0M1PEW7I6RqEG9PqVjMkUVdNfYzIlPcWrK3MW60BrbZKmOC8Ly0Lnt2UDeumYLd1eaHr+jOdnmJbVRskudyPPBMgKJvCKy6siqvVfm2b8uCESnKreK3ac3F3v9XjQpWkuHjqrp7ZMXB6+XbQ9tF6Mf9N2SFbSwSX5/Fcl3I3Ja2M/rkRRfdIndW+KXq3yHb0+PdWLc/u1wnUjOkY9lvIqnzbLKBbXD1GKKt6Xorp6fYjE+1cNxF2ndsOwzo2TPRQiIiIiIiJKMCZTkkSOO5zZW383q9xnQpATBzWK0lvpDpqQOC2gIs+M6dHCHExaFCxxFZ5dmS+HmwDwtxM6AQCaG+4ClY+JfAd+JMLV9reamWIs82W3bSE1xaML1B7OMai0MOelVYA73IwOVeklIVFBvfvP6B52nUybxtZuuE0EyESA2ucPzObqdf9E9HngZ9fbqavB0neuGIBz+rbEZ9cNdvU6J9cosc7Fg9qiQ+NAElqUBJQbwmdEeX2KZGy1QZuG5pl6Mvl6e8PIjjiqeWKbpguxOrfdjN9ql2VVod//aQ6zcE7a5cT7+5vsxHEiDe3cGNcM73jElDMjIiIiIiKikDqVTJk+fTrOOOMMtGjRAh6PB+PHj0/2kCIQnOEgLTEGOVQzU+QZEEu3FUW05woHZX4mLd+FR38MlGP5U2/nJUtUYlXm6+hW9QGYy5RU1QSOSVqKx3FteStidohVasQv9UiQ8yfG5IvqbRmXpaV4dduwa35c11nNmHr+wr4Y1KEh7jy1m/J5+ZiFS8jYvTaejutkf1dyeqoXx7RpEJN93TKqCwZ3aISn/9wbAHDa0c0dv1Z813x+Pypr1FFXYyPwimp3JcFqs3aN6+GZ8/ugS1P7nlJGTq5R8nVn/d4SAMCy7YHrc7z65QB1pydFuO+uOO2a5WXin6eorwWJEMvD6XSWpNUu5ZspnMw4BSJL6sea2+s0ERERERERUV1Up/76LSkpQe/evfHiiy8meygRU8U8qmp8uPf00F3uVgFPO1YzKmQlleYZL0b3fr1U+zna4ItdMNJNo2EtXml4j/JskWiJG4BVh3HC0p34dc1ebV9yfXrj+qqSXcbxpaV4dMHr1MM4CGV1t/IZvVvg42sGa/0pjORgcdc4BMET4YUL+8Ys6N2gXjo+umYQxvZrBQB45Oxe6NC4Hm4YGb6skDgeNTbfOeNzy7ebE7aRXJfqsrys8P2JVOfaa9PXAwDOP7Z1zMckGHfbPD8TH/x1YNz2Fy9+7Rqe3HHEktO3YiwRKZRLTc1VPctUjMlQlXhfF088qgBHNc/DRQPbxHU/RERERERERMlUp7pZjxkzBmPGjEn2MKKi9UyR4hqtG2bjyqHtsXR7Ib6cv80UoLcLggqV1T7c+MF89GqVb1m3/VBF+LvN90rlk6INvqjKkQm/rNqje3zN8A6W64burNcv9/n8uuejoc1MUQSlrnt/Xmg9j0f3+YjAaeh587aN4/MYSoUdzjNTVOW6+ij6yqj0bpWPRVsLXSf1EpVLSeanlp+Vhim3j3S0rsjV1fj8ltcS42LVXeaH02wVJ+qlO0mmmJeN6dUMAJATfP2FA2KfVFm/p0T7uW+b+vjqhiEx30cihHpRJfcaGMu9p6V6gcrw35USi3Wqpd+bTg+Lk3sT4p1MyUxLwY//GBbXfRAREREREREl2+F7SzyAiooKFBUV6f6rLTweD9oFGyOf2itQsseq1JTVHayyn1fswvdLduCxYIkulW0HyrSfixSlxAB9MiGS2IscKLe7WbZRTrr284J7TsJdpx5lvbJoDm84MiIwHIta7VoD+jDreT3270uVPFANT55JdDjXmlfdWe04eeQRCS53+0zU4Qz3/ejTpn5CxhFOipaM9FuWAzLObMtINX9u4fpfHG4a52aEXUcOUI89JjBrqGmwHGEsZ84ZyYktt9+P2iSUTEnuOKItEym7ZVQXAMAVQ9rZrjf2mJbK5VXSDDCnvxuczEw9jH/NEBERERERESXMYZ1MefTRR5Gfn6/917p1/MquOCUnBCbcPBy/3XECujYLlDHSAvrGmSkOAiWHFE3rjeQgjd8iPyPf4RpJELBVgyzt56b56hJOgL6ufIN66ZbryeOQJ7r8umYP3pm1Mfi862Eq9hH4//6SStuSKR54cI5FEOybm4Yog3Kqu67l43w4z0xRValxGiDUqru53mvyj+fcu060LGGWaB5pZpfP4hb2PcUVusfGz+jk7k2Rm5kWnwHWUk7OIvkaKb7HIskrrtvxSKY4SbDXBeJsTHZpvkEdGsVsW5cOboufbxmOu0/rbrte/3YN8es/j8eFA/RlsapqpES7w+Mibsiww54mRERERERERNE7rP+6vvPOO1FYWKj9t2XLlmQPSZcoyUxLQYv6oeRDKHhs7A0SPpwsx0i37C+1WEdaySJGo5uZEnavZqleDybdOgITbh6GPJvgq5u7qUVcVx7/JW/OxdszNwaej0EyQm76u2pXse1YrhjSHv3amhuLN8i2TwoBwNuXHwvAOAMo+cH/eFElTpw2VfYoPncnEpebUu9oWOfGKMirHYkUIPQZ2JX5uunD+brHxtUK8sLP0jjcOPleyueaaDgvjrH4jsdj5pmcTKnDE1O0Y5TsfLLxen581yYRb8vj8aBz01xHn3vrhtn4z5964L2rBqB78zwAQLWUTHH6u+3cY1rpbmQwyk5PQb2MOlXVlYiIiIiIiKhWOqyTKRkZGcjLy9P9l2wiTKKK01nNTLGaRSIrKguV7SossyrhFX470c5M8Xo86FSQg27N7I91m0bOSwapmrrLnMzKCadUql+/q6jCcj2Px4MUrwdDOzU2PWcV+JKDav3aBYJ2bhMEdZUqoJjisKmyllw0fh9qybGz+nq8cOExiR1IGOIz2HeownKW26KthbrHxmPs9A75w4nbmSniGL00bS0qq31aUiUeh65KvlDXku9DJMTbqG0J5WfP75uwfaWnejGscxPUywjM1pyzYR8Adwkmr9eDAe0aWj7PWSlEREREREREscG/sJNElSCwShrIgXerZsbfL9kR2o5FEMb1Hf6RnB0OA0BnHN0Ct5/cBR9dPSj8OMLMUKh20n03jCyp7NiUFbvCjkWVaLIKfuVkpOLJc4/G03/urc3W8R0eVXrCUgXhMxX9OFRCAVb95xvuNI7F+RCN/OzaVQ5rw95As/Jr3pvnOO5uPIS1LdidCE7espxAFUmr8iofhj8xVTuG8UhENa0lJeSi5YtjwikayfgOi98pXy/cDsD9jCa7spq17fgSERERERER1VV1Kply6NAhLFy4EAsXLgQAbNiwAQsXLsTmzZuTOzAX7IKZoZkphkbr0uN0B3eYWs0okZMRm/epS4Hpx6PeTq5NuZBws0gEr9eDm07ojMEdHdSqt5ixE0t9W9fXfk61Ocbi2KriXKpm68Kf+7fG2H6ttMd1915yd1QBQaclo7wWn3u4pGBFVWIyVXUxPmlVAtDIWA4sHqWqaju3M/Pk3kc7i8q18zSWzc2Fv53QSfu5Ll9LakvPlNrA+B1ze0xuPL6T5XM8vkRERERERESxUaeSKX/88Qf69u2Lvn0DJThuvfVW9O3bF/fee2+SR+ac6IfipsyXT6or76Rcxy+r9yiXy/HRn5fvDLsdqxjgh1cPwjFt6uOz6wabEivxiLlqDejjmE3xeDy4ZVQXAEB5VY3leiJfom4073x/R0qZL1VC7taTujp7bTBdYTxS4SaeVNZYf36xVBdna0xYGv57D5jPzyMxmRLOse30fTaMx0jMuohHILu+NHOiLl9KfLWkZ0ptYDx/3H7ncmxucuDxJSIiIiIiIoqNOpVMGTlyJPx+v+m/cePGJXtorqljG6HgcXWND39+5Tfc9dUSLVjm9XjCBpIB4LEfV6qfkF5rN/siNBr1KHu1yseXNwzBse0a4sW/6PtDxCO+LIKRm/eX4oSnp+GjufGZiZSZFjgmZTbJFBFAV73PRvXCN6AXakvfj0R79ZJ+aOj0OEUwM6Vnyzz0blU/ssHFwLnS7KPaqLLG2awd4yE+Eu9sD/eWjddHY4K1Jo6JgrqYyFMR55nTGY2HM+N3LLbfOR5fIiIiIiIiolioU8mUw4LWcNf8lAi67S4uR6/7J+L3jQfw4ZzN0t27Hkxead3PIxw5CO3krlcnQcAB7fVNb+MRdBWbrKrxY/2eEtz55ZKY7wMAMtMCfVPsykSFynypZqY4f+8ndW8GALZNgw9Hbs6OUMcUZz1T/nFiZ3x701BHicJYML6XDY+eiqf+3Dsh+45Uz5b5jtYzJqzSUo68YGzYAL/haeOMNjkJHk/G70ddUlEdOGbpDvsoHc6Ky6t0jw9VVMds25yZQkRERERERBQbjGAkmF3Ya/vBMgDA+7M362ZHlAeD+9HG5Do0qaf97KT3ipPkgHE78ZmZEvttqoiAXkV1KJlinEESakCvf223Zrmu9tWwXjpWPHAKPr5mkPuB1jEXDmit/dynTX3Hr1uyrRAAsO9QpW651cwUr8eT0Dv25V31aJFXJ2YLOB1hjeEYO7leHG7CfZxzN+zXPRaNw4V49kyR1eVJbg98uxwAkJtpXaIqUR46q2dS9y//3om1OnBpIiIiIiIiIqoTjrwIWZKJ4Lzqruepq9S9TjbtKwEQCLZkGO7gPbFbgeN915NqqrdqkBV2fSd3VBsDhfEJKNtvc8UDp8RkLxlaMiWUyJqycrdyXeOx+fEfw1zvLys9Je6B1trgkbN74YWL+uL/27v7OCnren/8790FljtZkFtRblRIRBQUlMAMUxJvvhnpz2MdvCMzNS0StaRS85hhZR7PKZPs5M053WiezDxmdJS0k4aaCKapqKmByoKGsAiyK7vX7w+Y2Zlld9mF3Zmdnefz8diHc3PNNZ+ZuXbl+rzm/Xk/OHdaDNqte4sft6lm6+dw9X3PZd3edJiy82PcVbmqhtlVzTWgz2w63zBE3LwlN31oOpLalqypmOG8afs0+vgi+BXfaa/9Y+vx+Ke//SPPI4k47YMj4mefmRzLrvhoXp7/pdXv7vI+Tjp4z0ZvL8Zl+gAAAKA9FMYMYCfUmrmNc/9rSfpywwb0rQkv6jImB/u1oG9FS3fdJ+Nbxe0xZbOjycge3cra5HlSlSk1Gd8Qfvvd6qxttmx7Dxsuk1YIVQn5UlJSEv/voKExalDvnXp8wx4rTU1x5zqYygxEuxTIjPm6995v8r7/e7E+zK3Zkv0uHzC0ZcuDdSatrRQ4+0PZYUrqz21ZO/9taFi5VYiG9Gl5yNqepo4aEH17trz3VVsa0HvXn/ebJx0Yt80+dLvbhSkAAADQNoQpObYzK7JkTuo1DFNaM4e7Jeub5zvevqX7zgwW2rMBfXtLLWWU2aS7YQVRbe3WN65QKhEK2cQR/SIi4sypI7NuT5qY484MA3KhEOcnm6rqicgODl9esyHrvmJc5qv6/dZV4zQMWFPhdXuHfJVVm9t1/7vq+/98cERE9OiaHXpnhvt9e3bN6Zg6oq5t0Deme9eyOHK/llerAgAAAK1TfDNkedZciPGZD+29w8c3bATdmqAhaxmfFsQ6Ld135iRiezagb2+p19Hc6j5dtr3/da1cAojW2zejx0/K2+9Wx5a6xtOUpSvWtfOImrbk7+/k7blb4yePrWjyvt4ZywA2bAjeFhO9hWZXX3O6Z0ohpm5taMKwvhGxfZB3y6Ovpi8X+3sUETGi//Z/79pKafH9+gIAAEC7cIqdY6nppMaWhRrRv+cOH9+wUqI1kyRZPQBakAW0dOmqzO3aZ5mv3Ey0pZ4n1S8iSZKsKpWI+gnnzIln2kc63Np23D614p2Y9I0H49O3P5ne5oQD98jL2ArJvOPGRMSOq0syq94aHPZx6Mh+bT6ujm7SiOzXvGffHfeZylSbtG/PlAOG9mmfHbex+r+r2bf/12N/T19uWNVTjL518oHxkf0Gtsm+bpp1SNb1xnq0AQAAAK0nTMmTxqY2WrIczOCK7LXlG5skyexhkqk2yaxM2bEWL/OVGaYU8DeMU0NPfYP6nP9cEl+759msbQ4ZvnWC9bgDh6QnV3/QYOKKtpE6llIZ4H/+6bWIiHh65bpt90cctvfu6e17N3Hct5dCOdQPHzUgIiL69Wp+KaXMvwm1DWa+uxThV9sb/i3r2YLeTCcfsldERHTvWpoOD9orKPj3Tx0c44f1jf84Y1K77L+t1Ff8ZR9Tme+KLCVij4oe8YNZE9PXG4Z5rXHcgXvEDadOSF/3/gIAAEDb8PX6HEtVPTQ2EduSCoyGkyKNPaRq85ZGH1vbyp4pLZ0szpwsfPaN9S17UCvkujIltYrUg8+vzrr/8FH904FXz25d4tHLjsrJuIpVaUa4tbxyQ9yz7M0G95fEohfWpK9f+JFRuRxewagPCZvfLsn4o9BwGbsuZmOjXwsak39x+uj45VOvR0T939v2Cpj3Hdg7fn3B4e2y77aUevkNA7qs96VQksl21paZZXYvM+8vAAAAtIXi+7pxnjU3n1nWggmPcz+8b9b11gQNW2rbp2fKG+veS19+sh16R+TqS/GZlSmbG2k+fdrkEbkZCBGRvezal375l0buj9hcU/85jd/WmyFXagukb06qem1HAWrmy9nS4LW1dxP1QtCSP4fdtzVZ3/x+XTo8KCvyty5zma/MwO7VtzemLxf7e5TSll8c6JIVprTZbgEAAKCoCVPyZGeX+Ro7tE9W74PWTJJkLrNy+T3PxpoNm5vdvqN8mzVX671nTmQ1FqbsO6h3TsbBVqUZy3xVN/J5lERJbN5Sf3uuD9eaLfWNRRZdPC23T94K9WFk82lKc5UptKyar7xr/d/m1DFb7EFU5pcEmnoP9UzZKvO9+ttb7+7SvjKPu1xVdwKNu/HGG2PkyJHRvXv3mDx5cjzxxBNNbvvXv/41Tj755Bg5cmSUlJTEDTfc0Op9rl27Nj7/+c/HfvvtFz169Ijhw4fHF77whVi/vu2rxwEAoNgIU3Jt22RSY0HFjuaTzpo6MiKye0O0ZpJk1fr68OS1f2yKr9z9TLPbd5T5rVyNI7MypbGqgw7ydhSNhj1sGrt/+v6D09dzPWFYk9Glfe/+vXL63K2RCiMby0fOm7ZvDNytPCIifp+xZFrDJZmK1UfHbj2+PnXYsKjO+Lyb0rNrfV+VNRuqI6LjhNL5kvl7mfpdTppb8quIZb4N72x6f5f2NTLjb5J3F/LnzjvvjLlz58aVV14ZTz31VIwfPz5mzJgRa9asaXT7TZs2xT777BPXXnttDBkyZKf2+eabb8abb74Z1113XTz77LNx2223xcKFC+Pss89ut9cJAADFQpiSY6nltRqbO9rRt3NTDZAzt2pqDuof71ZnXX/uzarttnl+1YZmn29nKkIu/ugHWv2YHWlunq1PGzYdz6yEaGzi2XxfbmV+Ho2p3lIXp0zaK3091x/P7r3qe2h05OqD5kKp3uVl6Uq3u5a8np7kLpQlzNrbv31yQvz4zElx5ccOiMP37b/D7btkVA2+X5ta5qvjHhu5UJaxhldq+biX12RXXRT3O1SvLUOljEPR7zPk0fXXXx/nnHNOzJ49O8aOHRsLFiyInj17xi233NLo9oceemh85zvfiU9+8pNRXl6+U/scN25c/PKXv4yPfexjse+++8ZRRx0V11xzTfzP//xPbNnSeF9FAACgZYQpOdbcF7539M361P1ZfXubmIbaVJO9LNLr72zabpumvvFf/3zN3p2WOancs7ztwo2U5iaY7v5c2zVgrp+8T7b75vS2kbTZc7FjqeOv8c9iqy6lO7fkXVvYo6JHLDhtYvzsnMm5feJWSr2P79Vsv1RaWWlpVs+j1N8Ny3xt1bNblzh6/8HRvWtZfOHo0XH1xw9owWO2ht6p97AD52w5kVmt82711km8DdXZk3mW+Wp7mX2PXsnoTwPkTk1NTSxZsiSmT5+evq20tDSmT58eixcvzuk+169fH3369IkuXRr/d3p1dXVUVVVl/QAAANsTpuRJoz1TdhimbP/o/3vprUa3bRiUNBZINGwyvd3ztXCC64ChfdKX26ORcHPvy5CK7m34PFv/m6hM6RAyw62m5HsS9thxQ2LqvgPyOoYd2/oevd/IMlVdGrx/qb8JlvnaXveuZXH6lJGxZ98ezW6Xeke31G19vzty1VIulJaWRLcuW/+p8ea24K7h4ZXv3+OOaNQu9uhqLDwFcuvtt9+O2traGDx4cNbtgwcPjsrKypzt8+23346rr746PvvZzza53/nz50dFRUX6Z9iwYTs1PgAA6OyEKTmWnkRqZGa+bAefRkkjlSlvbahudNuGQUljc1U7WvqjpeFBZtjRHhOHze2xLZ8tq2dKI5PJxb5cT66V7GCZr4jsSVjFFI2rP663v++dTTVZ1y3ztesaHreaf0fUbNkaLG2s3jrBr2fKjo0ZstsuPd5xB0REVFVVxQknnBBjx46Nr3/9601uN2/evFi/fn36Z+XKlbkbJAAAFBBhSo6leqY0pusO0pTU5Mig3RpfQzlTw8nQxiZH126s2e62xp5vRzIntNtjAqe5fXbPWEJmV6Um9JKk8WWOfHs6t0ozwq2mZIcpAoDGNPf78/ira7Oup/5OCFOalqo4aUrq3X5129JK79VYnz5VZfGpHz0WZ936xHbB3m7tsDxkodvV/5ceuGdFG40E2FkDBgyIsrKyWL16ddbtq1evbrK5fFvuc8OGDXHsscfGbrvtFr/61a+ia9euTe63vLw8+vTpk/UDAABsT5iSY6n53samSVJLoTQlNW98xf8bG326d4lrPjGuyW1P//HjWYHAzkyOtnQqJzNjaI8wpbldtmXAkd0zpXXjoO2VZoRbTcmsFmq4ZBVbNfeuNKy2Sv2ZsMxX01KN5ZvU4A3/n7+sar/BFIh3N9cHSg8vf2u74POij47O9ZA6vF39/01paUmM6N+zbQYD7JRu3brFxIkTY9GiRenb6urqYtGiRTFlypR23WdVVVUcc8wx0a1bt7j33nuje/e2WxYXAACKmTAlTxqbKNlhZcq2yeLJ+/SPZVccE7Mmj2hy29VV1fH06+vS13fUH6XR52vhbE7mdjtaqmxnNDaMvj27xmvXntCmz5PdM0VlSr61pDKlvEtp7DOgVwzo3W2Xewx0Vs39Gp96aPaa6Kn3WgP6pl398a0h9heOGtWi7bc00qum2FRWbc66nvk7femM/WLUoF1b0qozaov/2/g/FuTf3Llz40c/+lHcfvvt8fzzz8f5558fGzdujNmzZ0dExBlnnBHz5s1Lb19TUxPLli2LZcuWRU1NTbzxxhuxbNmyePnll1u8z1SQsnHjxvjxj38cVVVVUVlZGZWVlVFbq58SAADsCmtr5Fi6ZUoj0xw7Ci8y725Jb5LN79dP4u1MZUr1lpadcLX3Ml+9G1kC5tIZ+7X582RWpjT2zfwupbLHXCrJ+Dwac+FHRkVpaUncP+eIqK1Lomc3f84ak+pT0Zjxw/pmXU+91y+vebc9h1TQTjhoj/jQ6GOiokfjy6U0/AtoybTtZf5KC+4a1xb/L9U3BfLv1FNPjbfeeiuuuOKKqKysjAkTJsTChQvTDeRXrFgRpRn/vnzzzTfj4IMPTl+/7rrr4rrrrotp06bFww8/3KJ9PvXUU/H4449HRMSoUdnB/6uvvhojR45sx1cMAACdm9nHHGtu9Zwd5SONBTDNeT/jG9E7U5myYXPL1vrPakDfDpM3fbpnT1rOmjw8PnXo8DZ/npSty3xt/341NXlK+6gPtyKWr96w/f3bfmHasm9OZ9RcKNrw1zU18d+jm/e0Oc39LWjYTH3aBwa293AKzqMvv52+vDP/byoG773fBt8el6VAh3DhhRfGhRde2Oh9qYAkZeTIkY3+G7Q1+zzyyCNbtA8AAKD1fNU+TxrLHBpOwjXU2p4QmdUVtTtomtyY51ZVtex5MibD3tnUfFP7ndGwCmfs0D4tqsxp9fNkTN43Nr+3o542tK3UR/zXN9ZvF6hl3k/zBvQub/K+hm/huk3vR0REt/ZYr69IrH/v/azrHz94zzyNpOP6wcN/S19WudO4sXvsevNnlSkAAADQtsyY5dzWiaPGpjgaTg4fPqp/1vXW9uzIXD6lpd/+HVpR36DyvZqWfTP2wedXt2pcu6q9JohSqyw01TOF3NpQvbUy6unX18fEEf22u79h83QaN2z3nvGJJib0Gwa4s2/7c0Q0Hiayc1obghebPfv1yPcQOqTduu964bAjDwAAANqWMCXHUnP0LalMadijo7EwZfjuPZt8rswJ0Z359m//3t1atF1mUNNeQccH99k9fbm9lttKjT1JEt+W7gCqMr7hX7l+83b3t0d1Umd1wUcab5be8B18a0N1RESjPYPYOaoDmnfKxL3yPYQOqV+vlv3/tzmOPQAAAGhbwpQca26KcmN1do+Sht9obmz941vOOrTJ/WUGAltqWzY5mtpqaEX3+PqJB7ToMX171ocbjTWLbwszJ9R/s37GAUPa5TlSb3ddksQ7G7OX6vnCUY1PRtN+ajJ6/jS25JyJwpYbNah3o7c39hYmTfQMYucI/ZrXxZJyWa75xLg46eA944QD99jlffkTCQAAAG3LLEaeNNZMvmG/kYaVKJmTyylNTZJGZC9V1dJKi9Rjbj5jUuxR0bLlV27NCHTaYmmSxsw8eM+YNKJfzDl6dKuXO2upkoyeKUmD2Gv/Nli/ntap3tJ8nx9zsLuusUDq4KsfiPufqczDaDony9HRGrMmj4jrT50gZAIAAIAOqH1mvmlS+hvfjcyvNawe6VKWvdGI/r12uP/xw/rG0yvXRUR2gPJ8C5vJN7cMWVMOHt4v/t9Be8Qzb6yPj4wZ1PIHtkL3rmXx3+dPbZd9p6Recl2SbNczonc7hUQ0rfr95nv2qExpH6km9LQNhSkAAAAAnYOvPuZYao6+sfm1D40ekHW9YQ+VD+6T3ZC+Mf8vY2mQzMqUlvY/SYUIrZ2o/v4/HxIPXXxkdO9a1qrHdST1PVO2b0C/V7+me9PQPja/33xlijBl15WURHzthP2b3eb4A9tnWb1iYZkv8mXe8Vt/tz99+N55HgkAAAB0Dr5un2PNtSIY0Ls8zj9y37jp4b81en9LGq9nLoGVWZmSet6y0pIdLPm19b6dmacu9EnDzAb0DXtGdC0r7NdWiKq37KgyJUcD6cRKSkriM0fsExura+NfH3xxu/u/evz+8ZkjTMTuCst8kS/TPjAwll7+0ay+ZgAAAMDOU5mSJw2rTlIyA5OSiOjTyuWlMsOUzMwkaeT+xtSvQlZ8E4Cpj6QuiahrUBTRzfr1ObejypT26p1TTFLv4KA+5Y3e369Xtyb/VtEyPboVbrUehc/vMAAAALQdM8Q51twyXxERazdmN6EfUtF9h/v86Wcmpy+Xd6n/SDOXqkpd3tG3pFPbFeM8daqyZmvPlIb9a/yq5Nphe+/e7P0mCHdd6i1s6ve9GP8O7KrdyrMD8EJe+hAAAACAemaIcyy1fFRT88DrNmWHKWWlO/6I9uzbI3359XfeS1+ua2SZry47qkzZ9t9inKdOvTVbe6Zk36cKIvcuPuYDzd6/pbb5yhV2rLzL1on+poIpfWlab8QA/ZUAAAAAOiNhSgeTOWmfRHZQ0pLHvPr2xvTl2iQzTNlWmbKD3h+pAKYYv/WfWtqsrkHPlP930B4t6ldD2+rZrUuMGtS7yfurtwhTdlXqb0dToUmh90HKh4ZLBAIAAADQOWhAnydNZRWZk5pJksQ1nxgXEUmcPmVkk/satnvPOGvqyOjTvUu89o9N6du7lmYu+bX1v5tqmm/qvaNlyDqz0nTPlCT9fh229+7x/X8+JH+DKnLN9arZ0rB8iFZLhYSW+Wo7jkoAAACAzkmYkmM7avCeGaaUREkM7tM9/uPMQ3e436+feEBERNz40MsRT297roxpvdTlmh19mz/ZfhzFIlWNk0Rx947pSLp1aTpMSRLT1jtr7B594v+buFf6epOVKUX4d2BXrd1Yne8hAAAAANAOLPOVY8kOvrecuWTXzsxjZj4ms6VES7/EX7eDni6dWXbPlFSYUoRvRAfSsDLl1xccnqeRdC73zzkiPv2hvdPXm66Uy9GAOpHVVcKUhi6dsV++hwAAAACwy4QpedKSZb52tW9Jbcbi/S39En/9Ml/FN4ua+d4v+MMreRwJKaUN/kIJt9qHyhTa04wDBud7CAAAAAC7TJiSYzsKNTK/iP/sG+tbvf8Pjx6YvrylLon1m96Pa37zXCxbua6J8WQPKNVTpRjnUDMnjp9fVRUREX9+bW2+hkNkV2pFZB+Xuxo2Uq8l4S7sLMcRAAAA0BkIU3Is3TOlicmlzNs3bN7S6v2P27Mixu7RJyIiauuSuPLeZ+NHf3w1HQ40nJx+ftWG9OXMfirlXYvw0GjkI3m/Vl+OfCprWJpCu2iyMsXbv0sOHt4330MAAAAAoI2YKsuxHfVMyZzU7FVetlPPsf+2MGVLXRL3LHsz676GYco5//lk+vL7GU1WdivvulPPXcj0h+h4yhp8Jr7h3j6Wrnin0dtV/7Ren+5d0pdvO+uwPI6k4+hd3mXHGwEAAAB0cMKUPGlqirJrxuxxr247NwHVZVsqUNtI1/kuDRKDN9a9l75cm7HkV8PQpRiYqO94mlvmi7bTVAVWmTe81X7zhSPisuPGxF++fkxU9Cy+ULoxg/p0j3/75ITtbh+3Z5/cDwYAAABgJwlTcqx+ma/G78+8eWjfHjv1HGVlTYcpzYUktRkTqg1Dl2LQrYtfh45GmJIbp0zaq9HbBYytN2z3nnHetH2jT3dBSqaPT9gzHpt3dNZt150yPk+jAQAAAGg9s8c5loorSpqqTcmYvLxkxgd26jlSQciW2rrYrXt2dUtdIwFLypaM+0qLMEzpUlpisr6DaRimmNxvH93KGv9fQRH+GaAdDanonnV9zBCVKQAAAEDhEKbk2g76mW+qrm86v7MTTakJ53///cuxpcHyPRtrapt8XN22splirEqJ2Nofolhfe0fVMDzx6bSPpnqj6JkCAAAAAFsJU/KkqTnKde+9v8v7fv2dTenL773fdHjSUKoypRj7paSYPO5YGoZbPp720dTvfDH/LQAAAACATMKUHEu2laY0NUU5a/LwiIiYvv+gnX6OB59fs1OPS/VMKeYJ1OJ95R1Tw+XmhF0779IZ+0VExNdO2H+7+5r6lS/iPwW0s2L+/wwAAABQmLrseBPa0o4a0B88vF888dWjo3+v8p1+jt7lXeLdjOXCWmr1hs0REbGpmaXAOjs9OTqW7SpT8jSOzuCCj4yK/2/iXjG4T/ft7mvquBde0V5uPevQfA8BAAAAoFVUpnRAg3brvkvf2v3A4N5N3jekkYnUlOv/98Wdfs7Owtxxx7JmQ3XW9czJ/fF79c3xaApfY0FKRNPHveIB2svI/r3yPQQAAACAVlGZkmP17eDbb5ayS1nTGdnsw0fGvgN7x0tr3o1vLXwha7J08Sv/aLcxFQqVKR3Lw8vf2u623188LV5e8258aPSAPIyoc2oqvFWZQlv7zRc+FO9sfD+G9++Z76EAAAAAtIowJceSbet8teccZdeypnc+64Mjond5lzhwr4ptYUr9tr26lcXGIl7iK2L7iKt7V8VbHUltXV2MGrRb7DOw6eorWk+ISK4cMLQi30MAAAAA2ClminMsVZnSnlOXZaWNf6z/38S9ond5l6znr0vqa2WKPUiJ2D7k+sW5U/IzECIi4r7PfyjrenmXsjyNpHOzzBcAAAAANE+Y0gk1bNqdknlzavmeuqTRTYtWaYP3bpxvUefVuD3r3/+KHl1j2O6WBmoPTVWmqFgBAAAAgK0s85VjqUKQ9uxF0FSYUpJRD5O5yevvbIq9+tVPUl80/QPtNraOruE7Zy45/1679oR8D6HTS5oIVR3/AAAAALCVypQcy8UyX12baED/2Kv1DeYzv3F+7W9fiIiISSP6RUTEfkOKtx9F5vtSUqIBN8VhS11do7erTAEAAACArYQpudbUV8Db0JlTRzZ6+9//sSl9OXOO9IlX10ZEff+UYp5AzXzpxfw+UFz69ypv9Ha/AwAAAACwlTAlT9pzjvKwvXdvwfPXD2DNhuqIqO+fUswTqJnvS62GMhSJbl0a/19Bl7Li/VsAAAAAAJmEKTmWXuYrz3OUjbVVSVKVKUV8VJg6hnoDmqhYAQAAAIBiU8TT5vmRbkCfh2n7Tx46LH25seqTV97aGBHF3Sck830ZtJuJZIrXyP49o6Jn13wPAwAAAAA6hC75HgC507dnt/TlxvKSDdVbIiLi3c1bcjWkDkfPFIi4+KMfiLMOH5nvYQAAAABAhyFMybEk0qUpOVfaTFCQWuIrIuKtbT1UilHm+9LYUmjQ2fXp3iU+f/TofA8DAAAAADoUy3zlWJK/LCUrKGhYdJGRpRR1iJD5vhTzcmcUL8c9AAAAAGxPmJJj6TAlDxOWWUFBgzgnI0uJ0iJOU7KW+fLbQRGSpQAAAADA9kwXF5GSZpawylzmq5i/mZ69zFfxvg8AAAAAANQTpuRYKrLIxzR919Kmg4LMypSyIg4RMl95Mb8PAAAAAADUE6bkWKoCJB/z9BOG901f1jOlcc31lYFiIEQEAAAAgO0JU/KkYc+Stta1bPv9j+zfq/75t6tMyVzmq/3G1eFl9kwp6jeCYtWjW1m+hwAAAAAAHY4wpZN68msf3e62Ybv3bHL7zMoUPVO2vwzFomuZ/y0AAAAAQENmzXIsFVq09zx9RY+u8fuLp8XMCUNb/dhiDhEyX3kRvw0UsWJe5g8AAAAAmtIl3wMoNqnltHIxX7nPwN5xwycPjuH9e0XfHl2bH1dGZUoxfzFdZQrFbkDv8nwPAQAAAAA6HGFKvuRwnn7uRz+ww20ye6YUc4iQ+dJLizhUovj8xxmT4uY/vhLXnTI+30MBAAAAgA5HmJJjmRUgHYmeKVuVqEyhSE0fOzimjx2c72EAAAAAQIfku/c5lsosSnJZmtICmRlPMfdMyHztZcX8RgAAAAAAkCZMybFkWwlIRyt6SDJKU8o62uByKPOld+9Slr+BAAAAAADQYQhTiIjsypT9huyWt3Hk29p3a9KXy7v69QAAAAAAQJiSc/XLfHUsmT1T9urXM38DybM3129OXy7v4tcDAAAAAABhSu5tCy06WpP3l9dsSF/WKmSrcst8AQAAAAAQwpScS1emdLDA4r2auvTlLmUOi4iI7pb5AgAAAAAghClss6Vua5iy/x598jyS/Jq+/+D0ZZUpAAAAAABECFNyLtnWnKSDFaZE9ZatYUpHG1eujduzPkx6flVVHkcCAAAAAEBHIUzJsY66zNfm92sjIqK0yI+I0owP5sm/v5PHkQAAAAAA0FEU+dR5PnWsNOWLdy6LiOwwoRiVFvfLBwAAAACgEcKUHEuSHW+TD6lxlRR5mJL5+ou9fwwAAAAAAFsJU3Is2bbQV0fNLDrosHImszJn9tSR+RsIAAAAAAAdhjAlx9IVIPkdRkREPHrZUdvdVuzLXGW9/iJ/LwAAAAAA2KpLSzaaO3dui3d4/fXX7/RgiklHWE5rz749trtNz5T611/c7wQAAAAAACktClOWLl2adf2pp56KLVu2xH777RcRES+++GKUlZXFxIkT236EnUxH7ZmSUuRZStbr7wiBFwAAAAAA+deiMOWhhx5KX77++utjt912i9tvvz369esXERHvvPNOzJ49O4444oj2GWUn1FGn6Ys9QFCZAgAAAABAQ63umfLd73435s+fnw5SIiL69esX3/jGN+K73/1umw6uM0qSjt2AXs+U+ssd9TMCAAAAACC3Wh2mVFVVxVtvvbXd7W+99VZs2LChTQbVmaVW+SrpoHUPxd4zZc2G6vTlIn8rAAAAAADYptVhyic+8YmYPXt23H333fH666/H66+/Hr/85S/j7LPPjpNOOqk9xtippHqmdNSJ+jfWvZfvIeTVDx7+W/pysQdLAAAAAABs1aKeKZkWLFgQl1xySfzzP/9zvP/++1t30qVLnH322fGd73ynzQfY2dRtS1M66kT93/+xKd9DAAAAAACADqVVYUptbW08+eSTcc0118R3vvOd+Nvftn6Lf999941evXq1ywA7m7oOXplS7I4/cEjc/0xlRESU+JAAAAAAAIhWLvNVVlYWxxxzTKxbty569eoVBx10UBx00EGClFbp2JUpxe78aaPSl31CAAAAAABE7ETPlHHjxsUrr7zSHmMpCqnKlFIz9R1Sr/Ky9OXqLXV5HAkAAAAAAB1Fq8OUb3zjG3HJJZfEfffdF6tWrYqqqqqsH5qX6pnSUQpTfnHulHwPoUPpUlr/K9GvZ9c8jgQAAAAAgI6i1Q3ojz/++IiIOPHEE7N6SiRJEiUlJVFbW9t2o+uEknTPlI6Rphy29+4xtKJ7vLl+c76H0iFkfiw9upU1vSEAAAAAAEWj1WHKQw891B7jKBrpypQ8jyNTeVehQUppxvpr+toAAAAAABCxE2HKtGnT2mMcRSNJ90zpOBP1PYQpaZm9bDrSZwQAAAAAQP60OkxJ2bRpU6xYsSJqamqybj/ooIN2eVCdWbItTelIE/U9LWeVlvm5lHacjwgAAAAAgDxqdZjy1ltvxezZs+O3v/1to/frmdK8unTPlPyOI5PeIPUyP5eO0tcGAAAAAID8Km3tA774xS/GunXr4vHHH48ePXrEwoUL4/bbb4/Ro0fHvffe2x5j7FSS2NYzpQPN01vmq57KFAAAAAAAGmp1Zcrvf//7+PWvfx2TJk2K0tLSGDFiRHz0ox+NPn36xPz58+OEE05oj3F2GnUdsGdKmdQgLfNzUZkCAAAAAEDETlSmbNy4MQYNGhQREf369Yu33norIiIOPPDAeOqpp9p2dI248cYbY+TIkdG9e/eYPHlyPPHEE+3+nG0p1TOlI83Td6RgJ9/kSgAAAAAANNTqMGW//faL5cuXR0TE+PHj44c//GG88cYbsWDBgthjjz3afICZ7rzzzpg7d25ceeWV8dRTT8X48eNjxowZsWbNmnZ93raUdMDKlA40lLxTjQIAAAAAQEOtDlPmzJkTq1atioiIK6+8Mn7729/G8OHD49///d/jm9/8ZpsPMNP1118f55xzTsyePTvGjh0bCxYsiJ49e8Ytt9zSrs/blupUpnRoKlMAAAAAAGio1T1TTjvttPTliRMnxt///vd44YUXYvjw4TFgwIA2HVymmpqaWLJkScybNy99W2lpaUyfPj0WL17c6GOqq6ujuro6fb2qqqrdxtdSqZ4pJdFxZu0FCPUESwAALFq0KBYtWhRr1qyJurq6rPsK6YtcAABA22l1Zcorr7ySdb1nz55xyCGHtGuQEhHx9ttvR21tbQwePDjr9sGDB0dlZWWjj5k/f35UVFSkf4YNG9auY9yRVL+UiI4VYAgQ6nkvAACK21VXXRXHHHNMLFq0KN5+++145513sn4AAIDi1OrKlFGjRsVee+0V06ZNiyOPPDKmTZsWo0aNao+x7bJ58+bF3Llz09erqqryGqhkZCkdatI+s0/IEaPbNxTr6DrQxwIAQB4sWLAgbrvttjj99NPzPRQAAKADaXVlysqVK2P+/PnRo0eP+Pa3vx0f+MAHYq+99opZs2bFf/zHf7THGCMiYsCAAVFWVharV6/Oun316tUxZMiQRh9TXl4effr0yfrJp7qMNKUjTdqXZRwF/3zY8PwNpAPoSCEXAAC5V1NTE1OnTs33MAAAgA6m1WHKnnvuGbNmzYqbb745li9fHsuXL4/p06fHL37xizj33HPbY4wREdGtW7eYOHFiLFq0KH1bXV1dLFq0KKZMmdJuz9uWMgpTsqpB8i0zQOhAw8qLzNefuSwbAADF4TOf+Uz87Gc/y/cwAACADqbVy3xt2rQpHnnkkXj44Yfj4YcfjqVLl8aYMWPiwgsvjCOPPLIdhlhv7ty5ceaZZ8akSZPisMMOixtuuCE2btwYs2fPbtfnbSt1HbRnSklWmNKBBpYHZRmvv3+v8jyOBACAfNi8eXPcfPPN8eCDD8ZBBx0UXbt2zbr/+uuvz9PIAACAfGp1mNK3b9/o169fzJo1Ky677LI44ogjol+/fu0xtu2ceuqp8dZbb8UVV1wRlZWVMWHChFi4cOF2Tek7qsxCh44UWmQGOx1nVPlRWloSt551aLyzqSaG9++Z7+EAAJBjf/nLX2LChAkREfHss89m3deR/g0PAADkVqvDlOOPPz4eeeSRuOOOO6KysjIqKyvjyCOPjA984APtMb7tXHjhhXHhhRfm5LnaWnYD+vyNo6HMZb70DIn4yJhB+R4CAAB58tBDD+V7CAAAQAfU6p4p99xzT7z99tuxcOHCmDJlSvzv//5vHHHEEeleKjQte5mvjhNaZFWmdJxhAQBAXr3++uvx+uuv53sYAABAB9DqMCXlwAMPjMMPPzymTJkShx56aKxZsybuvPPOthxbp1PXQRual6hMAQCAiIioq6uLf/mXf4mKiooYMWJEjBgxIvr27RtXX3111NXV5Xt4AABAnrR6ma/rr78+Hn744XjkkUdiw4YNMX78+Pjwhz8cn/3sZ+OII45ojzF2GplRSkcKLUqzGtDncSAAAJBnX/3qV+PHP/5xXHvttXH44YdHRMQjjzwSX//612Pz5s1xzTXX5HmEAABAPrQ6TPn5z38e06ZNS4cnFRUV7TGuTinJ+CJbx+qZUn9ZU00AAIrZ7bffHv/xH/8RJ554Yvq2gw46KPbcc8/43Oc+J0wBAIAi1eow5c9//nN7jKMoZC7z1ZFCi7LSzGW+8jgQAADIs7Vr18aYMWO2u33MmDGxdu3aPIwIAADoCHaqZ8of//jHOO2002LKlCnxxhtvRETEf/3Xf8UjjzzSpoPrbLKX+crbMLazbtP76ctvv1udx5EAAEB+jR8/Pr7//e9vd/v3v//9GD9+fB5GBAAAdAStrkz55S9/GaeffnrMmjUrli5dGtXVWyff169fH9/85jfj/vvvb/NBdhYdtTLlvfdr05ff3bwljyMBAID8+va3vx0nnHBCPPjggzFlypSIiFi8eHGsXLnSuQ4AABSxVlemfOMb34gFCxbEj370o+jatWv69sMPPzyeeuqpNh1cZ5MKUzpQjhIREbV19SFP9Za6ZrYEAIDObdq0afHiiy/GJz7xiVi3bl2sW7cuTjrppFi+fHkcccQR+R4eAACQJ62uTFm+fHl8+MMf3u72ioqKWLduXVuMqfPallmUdrA0ZUtdfYDSvWtZHkcCAAD5N3ToUI3mAQCALK2uTBkyZEi8/PLL293+yCOPxD777NMmg+qs6tJhSn7H0VBmZcr/N3GvPI4EAABy7y9/+UuLf1rjxhtvjJEjR0b37t1j8uTJ8cQTTzS57V//+tc4+eSTY+TIkVFSUhI33HDDTu1z8+bNccEFF0T//v2jd+/ecfLJJ8fq1atbNW4AAGB7ra5MOeecc2LOnDlxyy23RElJSbz55puxePHiuOSSS+Lyyy9vjzF2GullvqJjpSlrNtQ3nVeZAgBAsZkwYUKUlJREktHjsDElJSVRW1vb7DYpd955Z8ydOzcWLFgQkydPjhtuuCFmzJgRy5cvj0GDBm23/aZNm2KfffaJU045JS666KKd3udFF10Uv/nNb+Kuu+6KioqKuPDCC+Okk06KRx99tEXjBgAAGtfqMOWyyy6Lurq6OProo2PTpk3x4Q9/OMrLy+OSSy6Jz3/+8+0xxk4jdWrWwVb5ihp9UgAAKGKvvvpqm+/z+uuvj3POOSdmz54dERELFiyI3/zmN3HLLbfEZZddtt32hx56aBx66KEREY3e35J9rl+/Pn784x/Hz372szjqqKMiIuLWW2+N/fffPx577LH44Ac/2Oavs6396W9vR9V77+d7GAAA5Fif7l1j6qgB+R5Gs1odppSUlMRXv/rVuPTSS+Pll1+Od999N8aOHRu9e/eO9957L3r06NEe4+wU6rYtp9XReqZcOmO/OPv2J+OMKSPyPRQAAMi5ESPa9t/BNTU1sWTJkpg3b176ttLS0pg+fXosXry43fa5ZMmSeP/992P69OnpbcaMGRPDhw+PxYsXNxqmVFdXR3V1faV6VVXVTo2vrXxr4fJ4euW6vI4BAIDcO2ivirj3wg/lexjNanWYktKtW7cYO3ZsRGz9B/j1118f3/72t6OysrLNBtfZpFYN6GBZShy9/+B4/CtHx6DdyvM9FAAAyLl77703jjvuuOjatWvce++9zW574okn7nB/b7/9dtTW1sbgwYOzbh88eHC88MILOzXGluyzsrIyunXrFn379t1um6bO0+bPnx9XXXXVTo2pPYzdY7fo2tGaTAIA0O72Hdg730PYoRaHKdXV1fH1r389HnjggejWrVt86UtfipkzZ8att94aX/3qV6OsrKzJtX3ZKomOWZkSETG4T/d8DwEAAPJi5syZUVlZGYMGDYqZM2c2uV1reqYUinnz5sXcuXPT16uqqmLYsGF5G8/8kw7K23MDAEBzWhymXHHFFfHDH/4wpk+fHn/605/ilFNOidmzZ8djjz0W119/fZxyyilRVqZ5eXPqOmhlCgAAFLO6urpGL++sAQMGRFlZWaxevTrr9tWrV8eQIUPabZ9DhgyJmpqaWLduXVZ1SnPPW15eHuXlKtQBAGBHSlu64V133RX/+Z//Gf/93/8d//u//xu1tbWxZcuWePrpp+OTn/ykIKUF6rat8yVLAQCAwrFu3bpWbd+tW7eYOHFiLFq0KH1bXV1dLFq0KKZMmbJTY2jJPidOnBhdu3bN2mb58uWxYsWKnX5eAABgqxaHKa+//npMnDgxIiLGjRsX5eXlcdFFF0WJMosWS/VMKbUGMAAAdEjf+ta34s4770xfP+WUU2L33XePPffcM55++ukW72fu3Lnxox/9KG6//fZ4/vnn4/zzz4+NGzfG7NmzIyLijDPOyGomX1NTE8uWLYtly5ZFTU1NvPHGG7Fs2bJ4+eWXW7zPioqKOPvss2Pu3Lnx0EMPxZIlS2L27NkxZcqURpvPAwAALdfiZb5qa2ujW7du9Q/s0iV69+74TWE6kiTpuD1TAACAiAULFsRPf/rTiIh44IEH4sEHH4yFCxfGL37xi7j00kvjf//3f1u0n1NPPTXeeuutuOKKK6KysjImTJgQCxcuTDeQX7FiRZSW1n+37c0334yDDz44ff26666L6667LqZNmxYPP/xwi/YZEfGv//qvUVpaGieffHJUV1fHjBkz4gc/+MGuvi0AAFD0SpLUDP8OlJaWxnHHHZdeT/d//ud/4qijjopevXplbXf33Xe3/SjbSFVVVVRUVMT69eujT58+OX/+5ZUbYsYN/xf9e3WLJZd/NOfPDwBAccn3v38LUY8ePeLFF1+MYcOGxZw5c2Lz5s3xwx/+MF588cWYPHlyvPPOO/keYrtyzAAAUGxa+m/gFlemnHnmmVnXTzvttJ0fXZFKYlvPFJUpAADQIfXr1y9WrlwZw4YNi4ULF8Y3vvGNiNhaZV5bW5vn0QEAAPnS4jDl1ltvbc9xFIW6uq3/1TIFAAA6ppNOOin++Z//OUaPHh3/+Mc/4rjjjouIiKVLl8aoUaPyPDoAACBfWhymsOvqklRlSp4HAgAANOpf//VfY+TIkbFy5cr49re/ne4TuWrVqvjc5z6X59EBAAD5IkzJAw3oAQCgY+ratWtccskl291+0UUX5WE0AABARyFMyaFUZYowBQAAOq7ly5fH9773vXj++ecjImL//fePz3/+87HffvvleWQAAEC+lOZ7AMWkLsn3CAAAgOb88pe/jHHjxsWSJUti/PjxMX78+Hjqqadi3Lhx8ctf/jLfwwMAAPJEZUoOJanKFBEWAAB0SF/60pdi3rx58S//8i9Zt1955ZXxpS99KU4++eQ8jQwAAMinFoUp9957b4t3eOKJJ+70YDq7VGWKZb4AAKBjWrVqVZxxxhnb3X7aaafFd77znTyMCAAA6AhaFKbMnDmzRTsrKSmJ2traXRlPp5aqTBGlAABAx3TkkUfGH//4xxg1alTW7Y888kgcccQReRoVAACQby0KU+rq6tp7HEUh1TJFZQoAAHRMJ554Ynz5y1+OJUuWxAc/+MGIiHjsscfirrvuiquuuiqral9VPgAAFA89U3Kobts6X7IUAADomD73uc9FRMQPfvCD+MEPftDofRGq8gEAoNjsVJiycePG+MMf/hArVqyImpqarPu+8IUvtMnAOqNUz5QSaQoAAHRIqvIBAIDGtDpMWbp0aRx//PGxadOm2LhxY+y+++7x9ttvR8+ePWPQoEHClGYk2xb6KpWlAABAh3L88cfHz3/+86ioqIiIiGuvvTbOO++86Nu3b0RE/OMf/4gjjjginnvuuTyOEgAAyJfS1j7goosuio997GPxzjvvRI8ePeKxxx6Lv//97zFx4sS47rrr2mOMnca2/vN6pgAAQAfzu9/9Lqqrq9PXv/nNb8batWvT17ds2RLLly/Px9AAAIAOoNVhyrJly+Liiy+O0tLSKCsri+rq6hg2bFh8+9vfjq985SvtMcZOoy6VpgAAAB1K0uDf6g2vAwAAxa3VYUrXrl2jtHTrwwYNGhQrVqyIiIiKiopYuXJl246uk1GZAgAAAAAAhafVPVMOPvjg+POf/xyjR4+OadOmxRVXXBFvv/12/Nd//VeMGzeuPcbYaaQqU0pbHWEBAADtqaSkJEoafOmp4XUAAKB4tTpM+eY3vxkbNmyIiIhrrrkmzjjjjDj//PNj9OjR8eMf/7jNB9iZpCpTSsJJGQAAdCRJksRZZ50V5eXlERGxefPmOO+886JXr14REVn9VAAAgOLT6jBl0qRJ6cuDBg2KhQsXtumAOrMktlWmyFIAAKBDOfPMM7Oun3baadttc8YZZ+RqOAAAQAfT6jDlqKOOirvvvjv69u2bdXtVVVXMnDkzfv/737fV2Dqdurqt/7VcAAAAdCy33nprvocAAAB0YK3u3vHwww9HTU3Ndrdv3rw5/vjHP7bJoDqrVM8UWQoAAAAAABSOFlem/OUvf0lffu6556KysjJ9vba2NhYuXBh77rln246uk9nWMiVKpSkAAAAAAFAwWhymTJgwIUpKSqKkpCSOOuqo7e7v0aNHfO9732vTwXU2SaJnCgAAAAAAFJoWhymvvvpqJEkS++yzTzzxxBMxcODA9H3dunWLQYMGRVlZWbsMsrOo21aaUhLSFAAAAAAAKBQtDlNGjBgRERF1qS7qtFqSClNkKQAAAAAAUDBaHKZk+tvf/hY33HBDPP/88xERMXbs2JgzZ07su+++bTq4zqYuvcyXNAUAAAAAAApFaWsf8Lvf/S7Gjh0bTzzxRBx00EFx0EEHxeOPPx4HHHBAPPDAA+0xxk4jFabIUgAAAAAAoHC0ujLlsssui4suuiiuvfba7W7/8pe/HB/96EfbbHCdlcoUAAAAAAAoHK2uTHn++efj7LPP3u72T3/60/Hcc8+1yaA6K5UpAAAAAABQeFodpgwcODCWLVu23e3Lli2LQYMGtcWYOq26uq3/LZGmAAAAAABAwWjxMl//8i//Epdcckmcc8458dnPfjZeeeWVmDp1akREPProo/Gtb30r5s6d224D7QySbf8tlaUAAAAAAEDBaHGYctVVV8V5550Xl19+eey2227x3e9+N+bNmxcREUOHDo2vf/3r8YUvfKHdBtoZpJb50jMFAAAAAAAKR4vDlCTd76MkLrroorjoootiw4YNERGx2267tc/oOpn0e5jncQAAAAAAAC3X4jAlYvteH0KU1tmWpeiZAgAAAAAABaRVYcoHPvCBHQYBa9eu3aUBdWZ16TAlv+MAAAAAAABarlVhylVXXRUVFRXtNZZOL4lUz5Q8DwQAAAAAAGixVoUpn/zkJ2PQoEHtNZZOL7XMFwAAAAAAUDhKW7qhPh+7LpWllGhBDwAAAAAABaPFYUqirGLXbXsP5VIAAAAAAFA4WrzMV11dXXuOoyikK1OEKQAAAAAAUDBaXJnCrksV91jmCwAAAAAACocwJYeS+jQFAAAAAAAoEMKUHKpvQA8AAAAAABQKYUoOpQtTNE0BAAAAAICCIUzJIZUpAAAAAABQeIQpeaAwBQAAAAAACocwJYfSDegBAAAAAICCIUzJA4UpAAAAAABQOIQpOaQBPQAAAAAAFB5hSg4l21rQi1IAAAAAAKBwCFNyKN0yRZoCAAAAAAAFQ5iSQ/VZijQFAAAAAAAKhTAlh+p7puR3HAAAAAAAQMsJU3JIzxQAAAAAACg8wpQcUpkCAAAAAACFR5iSB3qmAAAAAABA4RCm5IHKFAAAAAAAKBzClBxKUut8AQAAAAAABUOYkkN6pgAAAAAAQOERpuRQfV2KNAUAAAAAAAqFMCWHVKYAAAAAAEDhEabkULKtNkWWAgAAAAAAhUOYkkMqUwAAAAAAoPAIU3Io1TOlRG0KAAAAAAAUDGFKLm0rTVGZAgAAAAAAhUOYkkP1lSkAAAAAAEChEKbkQYnSFAAAAAAAKBjClBxKNaAHAAAAAAAKhzAlh5KQpgAAAAAAQKERpuRQqjLFKl8AAAAAAFA4hCk5VN+AXpoCAAAAAACFQpiSQypTAAAAAACg8AhTcijVM0WWAgAAAAAAhUOYkksqUwAAAAAAoOAIU3Io3TNFmgIAAAAAAAVDmJJDSWKZLwAAAAAAKDTClBxK0qUpeR0GAAAAAADQCsKUPCiRpgAAAAAAQMEQpuRQsuNNAAAAAACADkaYkkOpZb70nwcAAAAAgMIhTMmhJDSgBwAAAACAQiNMySGVKQAAAAAAUHgKJky55pprYurUqdGzZ8/o27dvvoezSzSgBwAAAACAwlEwYUpNTU2ccsopcf755+d7KDst2VaaojIFAAAAAAAKR5d8D6ClrrrqqoiIuO222/I7kF2wbZUvdSkAAAAAAFBACiZM2RnV1dVRXV2dvl5VVZXH0dT3TFGaAgAAAAAAhaNglvnaGfPnz4+Kior0z7Bhw/I6nmRbbYooBQAAAAAACkdew5TLLrssSkpKmv154YUXdnr/8+bNi/Xr16d/Vq5c2Yaj33kKUwAAAAAAoHDkdZmviy++OM4666xmt9lnn312ev/l5eVRXl6+049va+llvgAAAAAAgIKR1zBl4MCBMXDgwHwOIafqG9ArTQEAAAAAgEJRMA3oV6xYEWvXro0VK1ZEbW1tLFu2LCIiRo0aFb17987v4FooVZlimS8AAAAAACgcBROmXHHFFXH77benrx988MEREfHQQw/FkUcemadRtZYG9AAAAAAAUGjy2oC+NW677bZIkmS7n8IJUlSmAAAAAABAISqYMKUzqA9TpCkAAAAAAFAohCk5lKRb0AMAAAAAAIVCmJJDlvkCAAAAAIDCI0zJoVRdSokW9AAAAAAAUDCEKTmkMgUAAAAAAAqPMCUPZCkAAAAAAFA4hCk5pAE9AAAAAAAUHmFKLlnmCwAAAAAACo4wJYc0oAcAgOJx4403xsiRI6N79+4xefLkeOKJJ5rd/q677ooxY8ZE9+7d48ADD4z7778/6/7Vq1fHWWedFUOHDo2ePXvGscceGy+99FLWNpWVlXH66afHkCFDolevXnHIIYfEL3/5yzZ/bQAAUGyEKTmUbOtArzIFAAA6tzvvvDPmzp0bV155ZTz11FMxfvz4mDFjRqxZs6bR7f/0pz/Fpz71qTj77LNj6dKlMXPmzJg5c2Y8++yzEbH1XGLmzJnxyiuvxK9//etYunRpjBgxIqZPnx4bN25M7+eMM86I5cuXx7333hvPPPNMnHTSSfFP//RPsXTp0py8bgAA6KyEKTmkYwoAABSH66+/Ps4555yYPXt2jB07NhYsWBA9e/aMW265pdHt/+3f/i2OPfbYuPTSS2P//fePq6++Og455JD4/ve/HxERL730Ujz22GNx0003xaGHHhr77bdf3HTTTfHee+/Fz3/+8/R+/vSnP8XnP//5OOyww2KfffaJr33ta9G3b99YsmRJTl43AAB0VsKUHErSPVOUpgAAQGdVU1MTS5YsienTp6dvKy0tjenTp8fixYsbfczixYuzto+ImDFjRnr76urqiIjo3r171j7Ly8vjkUceSd82derUuPPOO2Pt2rVRV1cXd9xxR2zevDmOPPLIRp+3uro6qqqqsn4AAIDtCVNyqL5nCgAA0Fm9/fbbUVtbG4MHD866ffDgwVFZWdnoYyorK5vdfsyYMTF8+PCYN29evPPOO1FTUxPf+ta34vXXX49Vq1alH/OLX/wi3n///ejfv3+Ul5fHueeeG7/61a9i1KhRjT7v/Pnzo6KiIv0zbNiwXXnpAADQaQlTckjPFAAAYGd07do17r777njxxRdj9913j549e8ZDDz0Uxx13XJSW1p/WXX755bFu3bp48MEH48knn4y5c+fGP/3TP8UzzzzT6H7nzZsX69evT/+sXLkyVy8JAAAKSpd8D6CYqEwBAIDOb8CAAVFWVharV6/Oun316tUxZMiQRh8zZMiQHW4/ceLEWLZsWaxfvz5qampi4MCBMXny5Jg0aVJERPztb3+L73//+/Hss8/GAQccEBER48ePjz/+8Y9x4403xoIFC7Z73vLy8igvL9+l1wsAAMVAZUoe6JkCAACdV7du3WLixImxaNGi9G11dXWxaNGimDJlSqOPmTJlStb2EREPPPBAo9tXVFTEwIED46WXXoonn3wyPv7xj0dExKZNmyIisipVIiLKysqirq5ul14TAAAUO5UpuZTseBMAAKDwzZ07N84888yYNGlSHHbYYXHDDTfExo0bY/bs2RERccYZZ8See+4Z8+fPj4iIOXPmxLRp0+K73/1unHDCCXHHHXfEk08+GTfffHN6n3fddVcMHDgwhg8fHs8880zMmTMnZs6cGcccc0xEbO2rMmrUqDj33HPjuuuui/79+8c999wTDzzwQNx33325fxMAAKATEabkUBJ6pgAAQDE49dRT46233oorrrgiKisrY8KECbFw4cJ0k/kVK1ZkVZBMnTo1fvazn8XXvva1+MpXvhKjR4+Oe+65J8aNG5feZtWqVTF37txYvXp17LHHHnHGGWfE5Zdfnr6/a9eucf/998dll10WH/vYx+Ldd9+NUaNGxe233x7HH3987l48AAB0QiVJqit6EaiqqoqKiopYv3599OnTJ+fPf/5PlsRvn62Mqz9+QJw+ZWTOnx8AgOKS73//UngcMwAAFJuW/htYz5QcSsdWSlMAAAAAAKBgCFNyKL3MV57HAQAAAAAAtJwwJYdSlSkKUwAAAAAAoHAIU3IovcqX2hQAAAAAACgYwpQcUpkCAAAAAACFR5iSU3qmAAAAAABAoRGm5JDKFAAAAAAAKDzClDzQMwUAAAAAAAqHMCWHkh1vAgAAAAAAdDDClBxK0ut85XccAAAAAABAywlTcihVmSJLAQAAAACAwiFMyaH6BvTiFAAAAAAAKBTClBxSmQIAAAAAAIVHmJJDqZ4pClMAAAAAAKBwCFPyQJgCAAAAAACFQ5iSQ+meKRb6AgAAAACAgiFMyaEkLPMFAAAAAACFRpgCAAAAAADQDGFKDqWW+QIAAAAAAAqHMCWH0j1TrPMFAAAAAAAFQ5iSQ+meKXkeBwAAAAAA0HLClByqr0zJ7zgAAAAAAICWE6bkUKplSonaFAAAAAAAKBjClFxSmQIAAAAAAAVHmJJDeqYAAAAAAEDhEabkkJ4pAAAAAABQeIQpOZSkL0lTAAAAAACgUAhTcijZVpqiMgUAAAAAAAqHMAUAAAAAAKAZwpQcSi3zpTAFAAAAAAAKhzAlh+ob0ItTAAAAAACgUAhTckhlCgAAAAAAFB5hSi5pQA8AAAAAAAVHmJJD6coUYQoAAAAAABQMYUoOpXumWOgLAAAAAAAKhjAlh5JIpykAAAAAAECBEKbkUCJLAQAAAACAgiNMyaF0mKJpCgAAAAAAFAxhSh6IUgAAAAAAoHAIU3IoyfcAAAAAAACAVhOm5FCybZ0vq3wBAAAAAEDhEKbkQYmFvgAAAAAAoGAIU3KovgF9fscBAAAAAAC0nDAlh5JtXVNkKQAAAAAAUDiEKTmUqkyRpgAAAAAAQOEQpuRQfZYiTQEAAAAAgEIhTMmhZFtpip4pAAAAAABQOIQpOWSVLwAAAAAAKDzClFzalqaUKE0BAAAAAICCIUwBAAAAAABohjAlh9LLfClMAQAAAACAgiFMyaF0A/o8jwMAAAAAAGg5YUoOqUwBAAAAAIDCI0zJoSSVpqhNAQAAAACAgiFMyaFkW22KyhQAAAAAACgcwpQcSlWmyFIAAAAAAKBwCFNyKB2mKE0BAAAAAICCIUzJA1EKAAAAAAAUDmFKDiWJnikAAAAAAFBohCl5UKI2BQAAAAAACoYwJYeSfA8AAAAAAABoNWFKDtU3oM/vOAAAAAAAgJYTpuRQojYFAAAAAAAKjjAlh1SmAAAAAABA4RGm5FCqLkUDegAAAAAAKBzClBxSmQIAAAAAAIVHmJJTW9MUYQoAAAAAABQOYUoOpStTLPMFAAAAAAAFQ5iSQ+meKbIUAAAAAAAoGMKUPJClAAAAAABA4RCm5FCSWucLAAAAAAAoGMKUHLLMFwAAAAAAFB5hSg7VF6ZIUwAAAAAAoFAIU3IotcyXyhQAAAAAACgcwpQcSi/zlddRAAAAAAAArSFMyaVtaUqJ0hQAAAAAACgYwpQcUpkCAAAAAACFR5iSQ3qmAAAAAABA4RGm5FB9ZYo0BQAAAAAACoUwJYeSdM+U/I4DAAAAAABoOWEKAAAAAABAM4QpOZSkF/oCAAAAAAAKRUGEKa+99lqcffbZsffee0ePHj1i3333jSuvvDJqamryPbRWscwXAAAAAAAUni75HkBLvPDCC1FXVxc//OEPY9SoUfHss8/GOeecExs3bozrrrsu38NrsXQDemkKAAAAAAAUjIIIU4499tg49thj09f32WefWL58edx0000FFaak0hRRCgAAAAAAFI6CCFMas379+th9992b3aa6ujqqq6vT16uqqtp7WM1K9UxRmAIAAAAAAIWjIHqmNPTyyy/H9773vTj33HOb3W7+/PlRUVGR/hk2bFiORti4dM8UtSkAAAAAAFAw8hqmXHbZZVFSUtLszwsvvJD1mDfeeCOOPfbYOOWUU+Kcc85pdv/z5s2L9evXp39WrlzZni9nh+p7puR1GAAAAAAAQCvkdZmviy++OM4666xmt9lnn33Sl9988834yEc+ElOnTo2bb755h/svLy+P8vLyXR1mm0m2labIUgAAAAAAoHDkNUwZOHBgDBw4sEXbvvHGG/GRj3wkJk6cGLfeemuUlhbeCmWpyhRpCgAAAAAAFI6CaED/xhtvxJFHHhkjRoyI6667Lt566630fUOGDMnjyHaOnikAAAAAAFA4CiJMeeCBB+Lll1+Ol19+Ofbaa6+s+1JLZxWCAhoqAAAAAACwTUGslXXWWWdFkiSN/hQiDegBAAAAAKBwFESY0hlkBj+yMYFXzgAAHI9JREFUFAAAAAAAKBzClBzJLKIpUZoCAAAAAAAFQ5iSI5kLkolSAAAAAACgcAhTciRrmS9pCgAAAAAAFAxhSo5kV6ZIUwAAAAAAoFAIU3Iksc4XAAAAAAAUJGFKjiRhmS8AAAAAAChEwpQcyaxMkaUAAAAAAEDhEKbkQYnSFAAA6PRuvPHGGDlyZHTv3j0mT54cTzzxRLPb33XXXTFmzJjo3r17HHjggXH//fdn3b969eo466yzYujQodGzZ8849thj46WXXtpuP4sXL46jjjoqevXqFX369IkPf/jD8d5777XpawMAgGIjTAEAAGhjd955Z8ydOzeuvPLKeOqpp2L8+PExY8aMWLNmTaPb/+lPf4pPfepTcfbZZ8fSpUtj5syZMXPmzHj22WcjIiJJkpg5c2a88sor8etf/zqWLl0aI0aMiOnTp8fGjRvT+1m8eHEce+yxccwxx8QTTzwRf/7zn+PCCy+M0lKnfgAAsCtKkiSrNXqnVlVVFRUVFbF+/fro06dPTp/7vZra2P+KhRER8derZkSv8i45fX4AAIpPPv/9W+wmT54chx56aHz/+9+PiIi6uroYNmxYfP7zn4/LLrtsu+1PPfXU2LhxY9x3333p2z74wQ/GhAkTYsGCBfHiiy/GfvvtF88++2wccMAB6X0OGTIkvvnNb8ZnPvOZ9GM++tGPxtVXX71T43bMAABQbFr6b2BfT8oRDegBAKA41NTUxJIlS2L69Onp20pLS2P69OmxePHiRh+zePHirO0jImbMmJHevrq6OiIiunfvnrXP8vLyeOSRRyIiYs2aNfH444/HoEGDYurUqTF48OCYNm1a+v7GVFdXR1VVVdYPAACwPWFKjmQ3oJemAABAZ/X2229HbW1tDB48OOv2wYMHR2VlZaOPqaysbHb7MWPGxPDhw2PevHnxzjvvRE1NTXzrW9+K119/PVatWhUREa+88kpERHz961+Pc845JxYuXBiHHHJIHH300Y32VomImD9/flRUVKR/hg0btkuvHQAAOithSo5krqWmMgUAAGiNrl27xt133x0vvvhi7L777tGzZ8946KGH4rjjjkv3Q6mrq4uIiHPPPTdmz54dBx98cPzrv/5r7LfffnHLLbc0ut958+bF+vXr0z8rV67M2WsCAIBConFHjhRRaxoAAChqAwYMiLKysli9enXW7atXr44hQ4Y0+pghQ4bscPuJEyfGsmXLYv369VFTUxMDBw6MyZMnx6RJkyIiYo899oiIiLFjx2btZ//9948VK1Y0+rzl5eVRXl7euhcIAABFSGVKjqhMAQCA4tCtW7eYOHFiLFq0KH1bXV1dLFq0KKZMmdLoY6ZMmZK1fUTEAw880Oj2FRUVMXDgwHjppZfiySefjI9//OMRETFy5MgYOnRoLF++PGv7F198MUaMGLGrLwsAAIqaypQc0TMFAACKx9y5c+PMM8+MSZMmxWGHHRY33HBDbNy4MWbPnh0REWeccUbsueeeMX/+/IiImDNnTkybNi2++93vxgknnBB33HFHPPnkk3HzzTen93nXXXfFwIEDY/jw4fHMM8/EnDlzYubMmXHMMcdERERJSUlceumlceWVV8b48eNjwoQJcfvtt8cLL7wQ//3f/537NwEAADoRYUquZIYpshQAAOjUTj311HjrrbfiiiuuiMrKypgwYUIsXLgw3WR+xYoV6V4nERFTp06Nn/3sZ/G1r30tvvKVr8To0aPjnnvuiXHjxqW3WbVqVcydOzdWr14de+yxR5xxxhlx+eWXZz3vF7/4xdi8eXNcdNFFsXbt2hg/fnw88MADse++++bmhQMAQCdVkhRRM4+qqqqoqKiI9evXR58+fXL63Os3vR/j/+V/IyLi5WuOiy5lVlgDAKB95fPfvxQmxwwAAMWmpf8GNqOfI0kUTWYFAAAAAACdijAlR7J6pljnCwAAAAAACoYwJUcy61JEKQAAAAAAUDiEKTmS2ZpGYQoAAAAAABQOYUqOZFWmSFMAAAAAAKBgCFNyJNF/HgAAAAAACpIwJUeSbbUpilIAAAAAAKCwCFNyZVtliiwFAAAAAAAKizAlR1KrfOmXAgAAAAAAhUWYkiOJyhQAAAAAAChIwpQcU5gCAAAAAACFRZiSI0l6oS8AAAAAAKCQCFNypH6ZL6UpAAAAAABQSIQpOZKuS5GlAAAAAABAQRGm5EiyrTRFlgIAAAAAAIVFmJIj6WW+pCkAAAAAAFBQhCk5pmcKAAAAAAAUFmFKjqhMAQAAAACAwiRMyZEk9EwBAAAAAIBCJEzJkfrKFHEKAAAAAAAUEmFKjolSAAAAAACgsAhTciTJ9wAAAAAAAICdIkzJkSS9zld+xwEAAAAAALSOMCVHUpUpshQAAAAAACgswpQc0YAeAAAAAAAKkzAlZ7amKbIUAAAAAAAoLMKUHNEyBQAAAAAACpMwJUfSPVOUpgAAAAAAQEERpuSIyhQAAAAAAChMwpQcSfRMAQAAAACAgiRMyZFUZYraFAAAAAAAKCzClBxTmQIAAAAAAIVFmJIj9ZUpAAAAAABAIRGm5Ei6Z0qexwEAAAAAALSOMCVHUpUplvkCAAAAAIDCIkzJsRK1KQAAAAAAUFCEKTmiMgUAAAAAAAqTMCVH9EwBAAAAAIDCJEzJkfrKFHEKAAAAAAAUEmFKjiT5HgAAAAAAALBThCk5kmwrTVGYAgAAAAAAhUWYkmPCFAAAAAAAKCzClByxzBcAAAAAABQmYUqOpBvQh9IUAAAAAAAoJMKUnNEzBQAAAAAACpEwJUfqK1MAAAAAAIBCIkzJkVTPlBKlKQAAAAAAUFCEKTmiMgUAAAAAAAqTMCVHEmkKAAAAAAAUpC75HkCx6N29Sxw6sl8M7dsj30MBAAAAAABaQZiSIwcMrYi7zpua72EAAAAAAACtZJkvAAAAAACAZghTAAAAAAAAmiFMAQAAAAAAaIYwBQAAAAAAoBnCFAAAAAAAgGYIUwAAAAAAAJohTAEAAAAAAGiGMAUAAAAAAKAZwhQAAAAAAIBmCFMAAAAAAACaIUwBAAAAAABohjAFAAAAAACgGcIUAAAAAACAZghTAAAAAAAAmiFMAQAAAAAAaIYwBQAAAAAAoBnCFAAAAAAAgGYIUwAAAAAAAJohTAEAAAAAAGiGMAUAAAAAAKAZwhQAAAAAAIBmCFMAAAAAAACaIUwBAAAAAABohjAFAAAAAACgGcIUAAAAAACAZnTJ9wByKUmSiIioqqrK80gAAKD9pf7dm/p3MOyIcyYAAIpNS8+biipM2bBhQ0REDBs2LM8jAQCA3NmwYUNUVFTkexgUAOdMAAAUqx2dN5UkRfQ1tbq6unjzzTdjt912i5KSknZ5jqqqqhg2bFisXLky+vTp0y7PQcfnOMAxQITjAMcA+T8GkiSJDRs2xNChQ6O01Aq/7Fguzpmak+/fGfLL51/cfP44Boqbz7+45fvzb+l5U1FVppSWlsZee+2Vk+fq06ePX3wcBzgGiAjHAY4B8nsMqEihNXJ5ztQcfzeLm8+/uPn8cQwUN59/cevo502+ngYAAAAAANAMYQoAAAAAAEAzhCltrLy8PK688sooLy/P91DII8cBjgEiHAc4BnAMQGv5nSluPv/i5vPHMVDcfP7FrVA+/6JqQA8AAAAAANBaKlMAAAAAAACaIUwBAAAAAABohjAFAAAAAACgGcIUAAAAAACAZghT2tiNN94YI0eOjO7du8fkyZPjiSeeyPeQ2Anz58+PQw89NHbbbbcYNGhQzJw5M5YvX561zebNm+OCCy6I/v37R+/evePkk0+O1atXZ22zYsWKOOGEE6Jnz54xaNCguPTSS2PLli1Z2zz88MNxyCGHRHl5eYwaNSpuu+229n557IRrr702SkpK4otf/GL6NsdAcXjjjTfitNNOi/79+0ePHj3iwAMPjCeffDJ9f5IkccUVV8Qee+wRPXr0iOnTp8dLL72UtY+1a9fGrFmzok+fPtG3b984++yz4913383a5i9/+UscccQR0b179xg2bFh8+9vfzsnro3m1tbVx+eWXx9577x09evSIfffdN66++upIkiS9jWOg8/m///u/+NjHPhZDhw6NkpKSuOeee7Luz+Vnftddd8WYMWOie/fuceCBB8b999/f5q8XOhLnU4XPuRSZnEcVH+dPxcu5U3Ep2nOmhDZzxx13JN26dUtuueWW5K9//WtyzjnnJH379k1Wr16d76HRSjNmzEhuvfXW5Nlnn02WLVuWHH/88cnw4cOTd999N73NeeedlwwbNixZtGhR8uSTTyYf/OAHk6lTp6bv37JlSzJu3Lhk+vTpydKlS5P7778/GTBgQDJv3rz0Nq+88krSs2fPZO7cuclzzz2XfO9730vKysqShQsX5vT10rwnnngiGTlyZHLQQQclc+bMSd/uGOj81q5dm4wYMSI566yzkscffzx55ZVXkt/97nfJyy+/nN7m2muvTSoqKpJ77rknefrpp5MTTzwx2XvvvZP33nsvvc2xxx6bjB8/PnnssceSP/7xj8moUaOST33qU+n7169fnwwePDiZNWtW8uyzzyY///nPkx49eiQ//OEPc/p62d4111yT9O/fP7nvvvuSV199NbnrrruS3r17J//2b/+W3sYx0Pncf//9yVe/+tXk7rvvTiIi+dWvfpV1f64+80cffTQpKytLvv3tbyfPPfdc8rWvfS3p2rVr8swzz7T7ewD54Hyqc3AuRYrzqOLj/Km4OXcqLsV6ziRMaUOHHXZYcsEFF6Sv19bWJkOHDk3mz5+fx1HRFtasWZNERPKHP/whSZIkWbduXdK1a9fkrrvuSm/z/PPPJxGRLF68OEmSrX9USktLk8rKyvQ2N910U9KnT5+kuro6SZIk+dKXvpQccMABWc916qmnJjNmzGjvl0QLbdiwIRk9enTywAMPJNOmTUufBDgGisOXv/zl5EMf+lCT99fV1SVDhgxJvvOd76RvW7duXVJeXp78/Oc/T5IkSZ577rkkIpI///nP6W1++9vfJiUlJckbb7yRJEmS/OAHP0j69euXPi5Sz73ffvu19UuilU444YTk05/+dNZtJ510UjJr1qwkSRwDxaDhiUEuP/N/+qd/Sk444YSs8UyePDk599xz2/Q1QkfhfKpzci5VnJxHFSfnT8XNuVPxKqZzJst8tZGamppYsmRJTJ8+PX1baWlpTJ8+PRYvXpzHkdEW1q9fHxERu+++e0RELFmyJN5///2sz3vMmDExfPjw9Oe9ePHiOPDAA2Pw4MHpbWbMmBFVVVXx17/+Nb1N5j5S2zhmOo4LLrggTjjhhO0+J8dAcbj33ntj0qRJccopp8SgQYPi4IMPjh/96Efp+1999dWorKzM+gwrKipi8uTJWcdB3759Y9KkSeltpk+fHqWlpfH444+nt/nwhz8c3bp1S28zY8aMWL58ebzzzjvt/TJpxtSpU2PRokXx4osvRkTE008/HY888kgcd9xxEeEYKEa5/Mz9P4Ji4nyq83IuVZycRxUn50/FzbkTKZ35nEmY0kbefvvtqK2tzfqffUTE4MGDo7KyMk+joi3U1dXFF7/4xTj88MNj3LhxERFRWVkZ3bp1i759+2Ztm/l5V1ZWNno8pO5rbpuqqqp477332uPl0Ap33HFHPPXUUzF//vzt7nMMFIdXXnklbrrpphg9enT87ne/i/PPPz++8IUvxO233x4R9Z9jc3/7KysrY9CgQVn3d+nSJXbfffdWHSvkx2WXXRaf/OQnY8yYMdG1a9c4+OCD44tf/GLMmjUrIhwDxSiXn3lT2zgm6IycT3VOzqWKk/Oo4uX8qbg5dyKlM58zdWmXvUIncsEFF8Szzz4bjzzySL6HQg6tXLky5syZEw888EB0794938MhT+rq6mLSpEnxzW9+MyIiDj744Hj22WdjwYIFceaZZ+Z5dOTCL37xi/jpT38aP/vZz+KAAw6IZcuWxRe/+MUYOnSoYwAAdsC5VPFxHlXcnD8VN+dOFAOVKW1kwIABUVZWFqtXr866ffXq1TFkyJA8jYpddeGFF8Z9990XDz30UOy1117p24cMGRI1NTWxbt26rO0zP+8hQ4Y0ejyk7mtumz59+kSPHj3a+uXQCkuWLIk1a9bEIYccEl26dIkuXbrEH/7wh/j3f//36NKlSwwePNgxUAT22GOPGDt2bNZt+++/f6xYsSIi6j/H5v72DxkyJNasWZN1/5YtW2Lt2rWtOlbIj0svvTT9DasDDzwwTj/99LjooovS37R0DBSfXH7mTW3jmKAzcj7V+TiXKk7Oo4qb86fi5tyJlM58ziRMaSPdunWLiRMnxqJFi9K31dXVxaJFi2LKlCl5HBk7I0mSuPDCC+NXv/pV/P73v4+999476/6JEydG165dsz7v5cuXx4oVK9Kf95QpU+KZZ57J+sPwwAMPRJ8+fdL/uJgyZUrWPlLbOGby7+ijj45nnnkmli1blv6ZNGlSzJo1K33ZMdD5HX744bF8+fKs21588cUYMWJERETsvffeMWTIkKzPsKqqKh5//PGs42DdunWxZMmS9Da///3vo66uLiZPnpze5v/+7//i/fffT2/zwAMPxH777Rf9+vVrt9fHjm3atClKS7P/uVRWVhZ1dXUR4RgoRrn8zP0/gmLifKrzcC5V3JxHFTfnT8XNuRMpnfqcqV3a2hepO+64IykvL09uu+225Lnnnks++9nPJn379k0qKyvzPTRa6fzzz08qKiqShx9+OFm1alX6Z9OmTeltzjvvvGT48OHJ73//++TJJ59MpkyZkkyZMiV9/5YtW5Jx48YlxxxzTLJs2bJk4cKFycCBA5N58+alt3nllVeSnj17Jpdeemny/PPPJzfeeGNSVlaWLFy4MKevl5aZNm1aMmfOnPR1x0Dn98QTTyRdunRJrrnmmuSll15KfvrTnyY9e/ZMfvKTn6S3ufbaa5O+ffsmv/71r5O//OUvycc//vFk7733Tt577730Nscee2xy8MEHJ48//njyyCOPJKNHj04+9alPpe9ft25dMnjw4OT0009Pnn322eSOO+5Ievbsmfzwhz/M6etle2eeeWay5557Jvfdd1/y6quvJnfffXcyYMCA5Etf+lJ6G8dA57Nhw4Zk6dKlydKlS5OISK6//vpk6dKlyd///vckSXL3mT/66KNJly5dkuuuuy55/vnnkyuvvDLp2rVr8swzz+TuzYAccj7VOTiXoiHnUcXD+VNxc+5UXIr1nEmY0sa+973vJcOHD0+6deuWHHbYYcljjz2W7yGxEyKi0Z9bb701vc17772XfO5zn0v69euX9OzZM/nEJz6RrFq1Kms/r732WnLcccclPXr0SAYMGJBcfPHFyfvvv5+1zUMPPZRMmDAh6datW7LPPvtkPQcdS8OTAMdAcfif//mfZNy4cUl5eXkyZsyY5Oabb866v66uLrn88suTwYMHJ+Xl5cnRRx+dLF++PGubf/zjH8mnPvWppHfv3kmfPn2S2bNnJxs2bMja5umnn04+9KEPJeXl5cmee+6ZXHvtte3+2tixqqqqZM6cOcnw4cOT7t27J/vss0/y1a9+Namurk5v4xjofB566KFG/x1w5plnJkmS28/8F7/4RfKBD3wg6datW3LAAQckv/nNb9rtdUNH4Hyq8DmXoiHnUcXF+VPxcu5UXIr1nKkkSZKkfWpeAAAAAAAACp+eKQAAAAAAAM0QpgAAAAAAADRDmAIAAAAAANAMYQoAAAAAAEAzhCkAAAAAAADNEKYAAAAAAAA0Q5gCAAAAAADQDGEKAAAAAABAM4QpAOyS1157LUpKSmLZsmXt9hxnnXVWzJw5s932DwAA0J6cNwEUPmEKQJE766yzoqSkZLufY489tkWPHzZsWKxatSrGjRvXziMFAADID+dNAHTJ9wAAyL9jjz02br311qzbysvLW/TYsrKyGDJkSHsMCwAAoMNw3gRQ3FSmABDl5eUxZMiQrJ9+/fpFRERJSUncdNNNcdxxx0WPHj1in332if/+7/9OP7Zhufo777wTs2bNioEDB0aPHj1i9OjRWScczzzzTBx11FHRo0eP6N+/f3z2s5+Nd999N31/bW1tzJ07N/r27Rv9+/ePL33pS5EkSdZ46+rqYv78+bH33ntHjx49Yvz48VljAgAAaGvOmwCKmzAFgB26/PLL4+STT46nn346Zs2aFZ/85Cfj+eefb3Lb5557Ln7729/G888/HzfddFMMGDAgIiI2btwYM2bMiH79+sWf//znuOuuu+LBBx+MCy+8MP347373u3HbbbfFLbfcEo888kisXbs2fvWrX2U9x/z58+M///M/Y8GCBfHXv/41LrroojjttNPiD3/4Q/u9CQAAAM1w3gTQuZUkDWNrAIrKWWedFT/5yU+ie/fuWbd/5Stfia985StRUlIS5513Xtx0003p+z74wQ/GIYccEj/4wQ/itddei7333juWLl0aEyZMiBNPPDEGDBgQt9xyy3bP9aMf/Si+/OUvx8qVK6NXr14REXH//ffHxz72sXjzzTdj8ODBMXTo0Ljooovi0ksvjYiILVu2xN577x0TJ06Me+65J6qrq2P33XePBx98MKZMmZLe92c+85nYtGlT/OxnP2uPtwkAAChizpsA0DMFgPjIRz6S9Y/+iIjdd989fTnzH9+p66ny9IbOP//8OPnkk+Opp56KY445JmbOnBlTp06NiIjnn38+xo8fnz4hiIg4/PDDo66uLpYvXx7du3ePVatWxeTJk9P3d+nSJSZNmpQuWX/55Zdj06ZN8dGPfjTreWtqauLggw9u/YsHAABoAedNAMVNmAJA9OrVK0aNGtUm+zruuOPi73//e9x///3xwAMPxNFHHx0XXHBBXHfddW2y/9Q6wb/5zW9izz33zLqvpc0fAQAAWst5E0Bx0zMFgB167LHHtru+//77N7n9wIED48wzz4yf/OQnccMNN8TNN98cERH7779/PP3007Fx48b0to8++miUlpbGfvvtFxUVFbHHHnvE448/nr5/y5YtsWTJkvT1sWPHRnl5eaxYsSJGjRqV9TNs2LC2eskAAACt4rwJoHNTmQJAVFdXR2VlZdZtXbp0STdAvOuuu2LSpEnxoQ99KH7605/GE088ET/+8Y8b3dcVV1wREydOjAMOOCCqq6vjvvvuS59AzJo1K6688so488wz4+tf/3q89dZb8fnPfz5OP/30GDx4cEREzJkzJ6699toYPXp0jBkzJq6//vpYt25dev+77bZbXHLJJXHRRRdFXV1dfOhDH4r169fHo48+Gn369IkzzzyzHd4hAACg2DlvAihuwhQAYuHChbHHHntk3bbffvvFCy+8EBERV111Vdxxxx3xuc99LvbYY4/4+c9/HmPHjm10X926dYt58+bFa6+9Fj169Igjjjgi7rjjjoiI6NmzZ/zud7+LOXPmxKGHHho9e/aMk08+Oa6//vr04y+++OJYtWpVnHnmmVFaWhqf/vSn4xOf+ESsX78+vc3VV18dAwcOjPnz58crr7wSffv2jUMOOSS+8pWvtPVbAwAAEBHOmwCKXUmS6kwFAI0oKSmJX/3qVzFz5sx8DwUAAKBDct4E0PnpmQIAAAAAANAMYQoAAAAAAEAzLPMFAAAAAADQDJUpAAAAAAAAzRCmAAAAAAAANEOYAgAAAAAA0AxhCgAAAAAAQDOEKQAAAAAAAM0QpgAAAAAAADRDmAIAAAAAANAMYQoAAAAAAEAz/n9ryLvGCUV17AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2b - ii"
      ],
      "metadata": {
        "id": "3nX6_coEa1J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Refrence- Lab files (Week 7,8,9) provided on Blackboard by Patrick Mannion in the module CT5134 Agents, Multi-Agent Systems and Reinforcement Learning\n",
        "if __name__ == \"__main__\":\n",
        "    f2 = open(\"2b_ii.txt\", \"a\")\n",
        "    ag1 = Agent(alpha=0.5,gamma=0.9,epsilon=0.10)\n",
        "    reward_list2,eps_list2=ag1.q_learning(10000,decay=True)\n",
        "    ag1.show_values1(f2)\n",
        "    f2.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiYgrLHBNVHd",
        "outputId": "a57eb2f7-1c84-45a5-8860-49f753c19b3d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 0 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9753 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9754 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9755 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9756 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9757 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9758 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 2 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9759 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9760 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9761 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9762 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9763 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 0 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9764 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9765 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9766 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9767 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9768 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9769 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9770 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9771 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9772 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9773 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9774 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9775 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9776 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9777 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9778 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9779 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9780 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9781 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9782 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9783 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9784 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9785 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9786 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9787 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9788 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9789 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9790 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9791 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9792 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9793 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9794 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9795 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9796 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9797 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9798 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9799 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 2 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9800 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9801 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9802 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9803 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9804 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9805 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9806 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9807 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9808 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9809 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9810 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9811 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9812 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9813 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9814 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9815 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9816 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9817 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9818 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9819 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9820 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9821 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9822 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9823 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9824 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9825 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9826 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9827 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9828 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9829 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9830 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9831 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9832 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9833 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 3 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 4 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 5 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 6 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9834 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9835 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9836 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9837 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9838 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9839 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9840 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9841 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9842 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9843 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 1 reward -5 next_state (1, 0) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9844 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9845 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9846 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9847 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9848 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9849 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9850 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 2 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (2, 3) action 1 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9851 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9852 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9853 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9854 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9855 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9856 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9857 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9858 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9859 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9860 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9861 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9862 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9863 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9864 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9865 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9866 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9867 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9868 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9869 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9870 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9871 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9872 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9873 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9874 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9875 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9876 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9877 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9878 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9879 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9880 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9881 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9882 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9883 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9884 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "selecting random action\n",
            "step 6 state (1, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 7 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 8 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9885 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 0 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9886 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9887 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9888 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 1 reward -1 next_state (1, 2) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (1, 2) action 1 reward -1 next_state (2, 2) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (2, 3) action 1 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9889 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9890 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9891 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9892 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9893 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9894 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 2 reward -1 next_state (3, 3) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 9 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9895 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9896 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9897 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9898 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9899 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9900 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9901 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9902 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9903 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9904 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9905 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9906 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9907 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9908 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9909 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9910 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9911 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9912 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9913 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 1 reward -5 next_state (1, 3) old_q -5.0 max_q 0.0 new_q -5.0\n",
            "**** Beginning episode 9914 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9915 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9916 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "selecting random action\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9917 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 3 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9918 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9919 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9920 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9921 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9922 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9923 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9924 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9925 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9926 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9927 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "selecting random action\n",
            "step 5 state (0, 4) action 0 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9928 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9929 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9930 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9931 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9932 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9933 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9934 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9935 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9936 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9937 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9938 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9939 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9940 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9941 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9942 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9943 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9944 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9945 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9946 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9947 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9948 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9949 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9950 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9951 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9952 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9953 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9954 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9955 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9956 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9957 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9958 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9959 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9960 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9961 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9962 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "selecting random action\n",
            "step 3 state (0, 2) action 0 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9963 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9964 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9965 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9966 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9967 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9968 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9969 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9970 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "selecting random action\n",
            "step 4 state (0, 3) action 0 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9971 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9972 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9973 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9974 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9975 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9976 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9977 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9978 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9979 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9980 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9981 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9982 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9983 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9984 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 2 reward -1 next_state (0, 0) old_q -1.3906558000000007 max_q -0.4340620000000006 new_q -1.3906558000000007\n",
            "step 2 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 3 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 4 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 5 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 6 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 7 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 8 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 9 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9985 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9986 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9987 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9988 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9989 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9990 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9991 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9992 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9993 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9994 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9995 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9996 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9997 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9998 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "**** Beginning episode 9999 ****\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -0.4340620000000006 max_q 0.6288199999999993 new_q -0.4340620000000006\n",
            "step 2 state (0, 1) action 3 reward -1 next_state (0, 2) old_q 0.6288199999999993 max_q 1.8097999999999992 new_q 0.6288199999999993\n",
            "step 3 state (0, 2) action 3 reward -1 next_state (0, 3) old_q 1.8097999999999992 max_q 3.121999999999999 new_q 1.8097999999999992\n",
            "step 4 state (0, 3) action 3 reward -1 next_state (0, 4) old_q 3.121999999999999 max_q 4.579999999999998 new_q 3.121999999999999\n",
            "step 5 state (0, 4) action 1 reward -1 next_state (1, 4) old_q 4.579999999999998 max_q 6.199999999999999 new_q 4.579999999999998\n",
            "step 6 state (1, 4) action 1 reward -1 next_state (2, 4) old_q 6.199999999999999 max_q 8.0 new_q 6.199999999999999\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 8.0 max_q 10.0 new_q 8.0\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 10.0 max_q 0.0 new_q 10.0\n",
            "---------------------------------------------\n",
            "| -0.434 | 0.629  | 1.81   | 3.122  | 4.58   | \n",
            "---------------------------------------------\n",
            "| 0.0    | 1.81   | 3.122  | 0.0    | 6.2    | \n",
            "---------------------------------------------\n",
            "| 1.81   | 3.122  | 4.58   | 6.2    | 8.0    | \n",
            "---------------------------------------------\n",
            "| -0.334 | 0.0    | 6.2    | 8.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.855 | -1.883 | 0.0    | 10.0   | 0.0    | \n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot(reward_list2, eps_list2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "Y33vbjHK_BuH",
        "outputId": "9ee126db-864c-4d08-fab0-2740d914b2d4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2b - iii"
      ],
      "metadata": {
        "id": "99lPhhcylFm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Refrence- Lab files (Week 7,8,9) provided on Blackboard by Patrick Mannion in the module CT5134 Agents, Multi-Agent Systems and Reinforcement Learning\n",
        "if __name__ == \"__main__\":\n",
        "    f3 = open(\"2b_iii.txt\", \"a\")\n",
        "    ag2 = Agent(alpha=0.05,gamma=0.5,epsilon=0.05)\n",
        "    reward_list3,eps_list3=ag2.q_learning(10000,decay=True)\n",
        "    ag2.show_values1(f3)\n",
        "    f3.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcnKKDijbuLZ",
        "outputId": "96bff562-571b-49f6-d9fb-ac0cd3757d2a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9752 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "selecting random action\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9753 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9754 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9755 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9756 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9757 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9758 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9759 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9760 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9761 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9762 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9763 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9764 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9765 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9766 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9767 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9768 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9769 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9770 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9771 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9772 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9773 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9774 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9775 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9776 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9777 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "selecting random action\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9778 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9779 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9780 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9781 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "selecting random action\n",
            "step 2 state (0, 1) action 2 reward -1 next_state (0, 0) old_q -1.953161145703147 max_q -1.9062500000000044 new_q -1.9531593384179897\n",
            "step 3 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 4 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 5 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 6 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 7 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 8 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9782 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9783 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9784 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9785 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9786 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9787 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9788 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9789 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9790 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9791 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9792 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9793 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9794 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9795 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9796 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "selecting random action\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9797 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9798 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9799 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "selecting random action\n",
            "step 5 state (2, 2) action 0 reward -1 next_state (1, 2) old_q -1.6248709724082944 max_q -1.250000040375679 new_q -1.6248774247972717\n",
            "step 6 state (1, 2) action 1 reward -1 next_state (2, 2) old_q -1.250000040375679 max_q -0.5000000000000049 new_q -1.2500000383568952\n",
            "step 7 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 8 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9800 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9801 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9802 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9803 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9804 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9805 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9806 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9807 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9808 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9809 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9810 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9811 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9812 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9813 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9814 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "selecting random action\n",
            "step 4 state (2, 1) action 1 reward -5 next_state (3, 1) old_q -4.99945828389678 max_q 0.0 new_q -4.999485369701941\n",
            "**** Beginning episode 9815 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9816 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9817 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9818 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9819 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9820 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9821 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "selecting random action\n",
            "step 6 state (2, 3) action 1 reward -1 next_state (3, 3) old_q 0.9952723807881373 max_q 3.9999996723570024 new_q 0.9955087535576554\n",
            "step 7 state (3, 3) action 3 reward -1 next_state (3, 4) old_q 3.9999996723570024 max_q 9.999999999999982 new_q 3.999999688739152\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9822 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9823 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9824 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9825 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9826 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9827 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9828 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9829 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "selecting random action\n",
            "step 6 state (2, 3) action 2 reward -1 next_state (2, 2) old_q -1.2485840893767517 max_q -0.5000000000000049 new_q -1.2486548849079142\n",
            "step 7 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 8 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9830 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9831 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9832 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9833 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9834 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9835 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9836 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9837 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9838 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9839 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9840 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9841 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9842 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9843 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9844 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "selecting random action\n",
            "step 4 state (2, 1) action 2 reward -1 next_state (2, 0) old_q -1.791473408234911 max_q -1.6159126076220884 new_q -1.7922975530137177\n",
            "step 5 state (2, 0) action 3 reward -1 next_state (2, 1) old_q -1.6159126076220884 max_q -1.2500000000000044 new_q -1.616366977240984\n",
            "step 6 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 7 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 8 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9845 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9846 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9847 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9848 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9849 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9850 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9851 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9852 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9853 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9854 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9855 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9856 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9857 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9858 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9859 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9860 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9861 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9862 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "selecting random action\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9863 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9864 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9865 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9866 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9867 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.546 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.512 | -1.476 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "selecting random action\n",
            "step 4 state (2, 1) action 2 reward -1 next_state (2, 0) old_q -1.7922975530137177 max_q -1.6159386817935857 new_q -1.7930811424078714\n",
            "step 5 state (2, 0) action 1 reward -1 next_state (3, 0) old_q -1.6159386817935857 max_q -1.545880120700624 new_q -1.623788750721422\n",
            "step 6 state (3, 0) action 1 reward -1 next_state (4, 0) old_q -1.545880120700624 max_q -1.511560917421197 new_q -1.5563751376011228\n",
            "step 7 state (4, 0) action 3 reward -1 next_state (4, 1) old_q -1.511560917421197 max_q -1.4760547638193529 new_q -1.5228842406456211\n",
            "step 8 state (4, 1) action 1 reward -1 next_state (4, 1) old_q -1.4760547638193529 max_q -1.4760547638193529 new_q -1.489153394723869\n",
            "step 9 state (4, 1) action 2 reward -1 next_state (4, 0) old_q -1.4848422067757479 max_q -1.5118137052750147 new_q -1.4983954390688359\n",
            "step 10 state (4, 0) action 2 reward -1 next_state (4, 0) old_q -1.5118137052750147 max_q -1.5118137052750147 new_q -1.5240183626431394\n",
            "step 11 state (4, 0) action 1 reward -1 next_state (4, 0) old_q -1.5127896328316088 max_q -1.5127896328316088 new_q -1.5249698920108186\n",
            "step 12 state (4, 0) action 0 reward -1 next_state (3, 0) old_q -1.5200084504306965 max_q -1.547808383456348 new_q -1.5327032374955705\n",
            "step 13 state (3, 0) action 2 reward -1 next_state (3, 0) old_q -1.547808383456348 max_q -1.547808383456348 new_q -1.5591131738699393\n",
            "step 14 state (3, 0) action 0 reward -1 next_state (2, 0) old_q -1.5561199745475265 max_q -1.616366977240984 new_q -1.5687231502511747\n",
            "step 15 state (2, 0) action 3 reward -1 next_state (2, 1) old_q -1.616366977240984 max_q -1.2500000000000044 new_q -1.616798628378935\n",
            "step 16 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 17 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 18 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 19 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 20 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9868 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9869 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9870 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9871 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9872 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9873 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9874 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9875 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9876 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9877 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "selecting random action\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9878 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9879 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9880 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9881 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9882 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9883 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9884 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9885 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9886 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9887 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9888 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9889 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9890 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9891 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9892 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9893 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9894 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9895 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9896 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9897 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9898 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9899 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9900 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "selecting random action\n",
            "step 4 state (2, 1) action 1 reward -5 next_state (3, 1) old_q -4.999485369701941 max_q 0.0 new_q -4.999511101216844\n",
            "**** Beginning episode 9901 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9902 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9903 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9904 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9905 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9906 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9907 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9908 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9909 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9910 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9911 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9912 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9913 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9914 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9915 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9916 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9917 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9918 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9919 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9920 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9921 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9922 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9923 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9924 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9925 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9926 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9927 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9928 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9929 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9930 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "selecting random action\n",
            "step 4 state (2, 1) action 0 reward -1 next_state (1, 1) old_q -1.8119591723203403 max_q -1.6250000000000044 new_q -1.8119862137043234\n",
            "step 5 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 6 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 7 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 8 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 9 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 10 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9931 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9932 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9933 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9934 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9935 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9936 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9937 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9938 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9939 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9940 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9941 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9942 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9943 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9944 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9945 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9946 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9947 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9948 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9949 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9950 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9951 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9952 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "selecting random action\n",
            "step 6 state (2, 3) action 0 reward -5 next_state (1, 3) old_q -4.990422550966123 max_q 0.0 new_q -4.990901423417817\n",
            "**** Beginning episode 9953 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9954 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9955 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9956 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9957 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9958 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9959 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9960 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9961 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9962 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9963 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9964 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9965 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9966 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9967 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9968 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9969 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9970 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9971 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9972 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9973 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9974 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9975 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9976 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9977 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9978 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9979 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9980 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9981 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9982 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9983 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9984 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9985 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9986 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9987 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9988 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9989 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9990 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9991 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9992 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9993 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9994 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9995 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9996 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9997 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9998 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "**** Beginning episode 9999 ****\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n",
            "step 1 state (0, 0) action 3 reward -1 next_state (0, 1) old_q -1.9062500000000044 max_q -1.8125000000000044 new_q -1.9062500000000044\n",
            "step 2 state (0, 1) action 1 reward -1 next_state (1, 1) old_q -1.8125000000000044 max_q -1.6250000000000044 new_q -1.8125000000000044\n",
            "step 3 state (1, 1) action 1 reward -1 next_state (2, 1) old_q -1.6250000000000044 max_q -1.2500000000000044 new_q -1.6250000000000044\n",
            "step 4 state (2, 1) action 3 reward -1 next_state (2, 2) old_q -1.2500000000000044 max_q -0.5000000000000049 new_q -1.2500000000000044\n",
            "step 5 state (2, 2) action 3 reward -1 next_state (2, 3) old_q -0.5000000000000049 max_q 0.9999999999999922 new_q -0.5000000000000049\n",
            "step 6 state (2, 3) action 3 reward -1 next_state (2, 4) old_q 0.9999999999999922 max_q 3.9999999999999867 new_q 0.9999999999999922\n",
            "step 7 state (2, 4) action 1 reward -1 next_state (3, 4) old_q 3.9999999999999867 max_q 9.999999999999982 new_q 3.9999999999999867\n",
            "step 8 state (3, 4) action 1 reward 10 next_state (4, 4) old_q 9.999999999999982 max_q 0.0 new_q 9.999999999999982\n",
            "---------------------------------------------\n",
            "| -1.906 | -1.813 | -1.625 | -1.473 | -0.719 | \n",
            "---------------------------------------------\n",
            "| 0.0    | -1.625 | -1.25  | 0.0    | 0.999  | \n",
            "---------------------------------------------\n",
            "| -1.616 | -1.25  | -0.5   | 1.0    | 4.0    | \n",
            "---------------------------------------------\n",
            "| -1.556 | 0.0    | 0.996  | 4.0    | 10.0   | \n",
            "---------------------------------------------\n",
            "| -1.523 | -1.489 | 0.0    | 8.16   | 0.0    | \n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot(reward_list3, eps_list3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "nRN1EO22pEm7",
        "outputId": "8c8c957a-181c-4eed-e1e8-ee30b6423da0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crOXTsuB_EBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}